1
00:00:12,060 --> 00:00:14,280
welcome to lane space uh we are basically

2
00:00:14,280 --> 00:00:17,360
trying to provide the best optimal sort of

3
00:00:17,360 --> 00:00:17,760
podcast

4
00:00:17,760 --> 00:00:19,680
experience of europe's for people who are not

5
00:00:19,680 --> 00:00:22,160
here uh and congrats on your paper how's

6
00:00:22,160 --> 00:00:22,400
it feel

7
00:00:22,400 --> 00:00:25,120
yeah it was very exciting um yeah we

8
00:00:25,120 --> 00:00:27,180
had a poster yesterday and then today we'll

9
00:00:27,180 --> 00:00:27,900
have an oral talk

10
00:00:27,900 --> 00:00:30,060
were you just like mobbed oh yeah there

11
00:00:30,060 --> 00:00:31,100
was a lot of people it's like three

12
00:00:31,100 --> 00:00:31,860
hours straight of like

13
00:00:31,860 --> 00:00:33,480
you know like waves of people to like

14
00:00:33,480 --> 00:00:35,660
that where we were trying to stupid so

15
00:00:35,660 --> 00:00:36,540
i've never received the

16
00:00:36,540 --> 00:00:38,840
best paper did you just find out on

17
00:00:38,840 --> 00:00:40,860
the website like what uh oh i just

18
00:00:40,860 --> 00:00:42,940
like woke up one day and

19
00:00:42,940 --> 00:00:45,400
like checked my email and then ah they

20
00:00:45,400 --> 00:00:47,000
just thought yeah they was like oh like

21
00:00:47,000 --> 00:00:48,040
that's like i just saw

22
00:00:48,040 --> 00:00:50,580
you know you've like been awarded best paper

23
00:00:50,580 --> 00:00:53,140
but maybe you know from the reviews as

24
00:00:53,140 --> 00:00:53,500
well right

25
00:00:53,500 --> 00:00:55,700
so yeah we know from the reviews that

26
00:00:55,700 --> 00:00:58,240
we did well um but there's a difference

27
00:00:58,240 --> 00:00:59,020
between like doing well

28
00:00:59,020 --> 00:01:01,260
on the reviews and getting best paper so

29
00:01:01,260 --> 00:01:03,320
right part we didn't actually know yeah okay

30
00:01:03,320 --> 00:01:04,400
so i i skipped a

31
00:01:04,400 --> 00:01:05,720
little bit uh maybe we can go sort

32
00:01:05,720 --> 00:01:07,480
of um one by one and and sort

33
00:01:07,480 --> 00:01:09,460
of introduce um you know who you are

34
00:01:09,460 --> 00:01:10,800
and what you did on on on the

35
00:01:10,800 --> 00:01:14,040
team i'm kevin i was an undergrad from

36
00:01:14,040 --> 00:01:15,440
from princeton and i just graduated

37
00:01:15,440 --> 00:01:19,040
and yeah i guess i led the project

38
00:01:19,040 --> 00:01:21,300
like started the project and then i was

39
00:01:21,300 --> 00:01:21,920
very happy to collaborate

40
00:01:21,920 --> 00:01:24,040
with ishan and nicole and ben also um

41
00:01:24,040 --> 00:01:26,100
right and were you in like the same

42
00:01:26,100 --> 00:01:27,080
research group like how

43
00:01:27,080 --> 00:01:29,180
do you how that's your yeah uh social

44
00:01:29,180 --> 00:01:31,680
context so so yeah so we're all from

45
00:01:31,680 --> 00:01:33,800
princeton yeah um thanks

46
00:01:33,800 --> 00:01:35,840
to alan for booking you guys so this

47
00:01:35,840 --> 00:01:37,920
project actually started from like an iw seminar

48
00:01:37,920 --> 00:01:39,340
so uh like like

49
00:01:39,340 --> 00:01:41,860
independent work research seminar that ben was teaching

50
00:01:41,860 --> 00:01:44,100
and this was like actually like like one

51
00:01:44,100 --> 00:01:46,720
of my first experiences in like ml research

52
00:01:46,720 --> 00:01:48,520
um so it was really valuable to like

53
00:01:48,520 --> 00:01:49,160
get that experience

54
00:01:49,160 --> 00:01:51,700
and then ishan was also in that seminar

55
00:01:51,700 --> 00:01:53,780
and working on adjacent things so we collaborated

56
00:01:53,780 --> 00:01:56,240
um a lot during that seminar and then

57
00:01:56,240 --> 00:01:58,300
yeah the project turned out to have some

58
00:01:58,300 --> 00:01:59,020
pretty cool results

59
00:01:59,020 --> 00:02:00,540
and then later on also like the halt

60
00:02:00,540 --> 00:02:02,980
working on sort of similar things also um

61
00:02:02,980 --> 00:02:03,840
joined it on the project

62
00:02:03,840 --> 00:02:06,360
and became like a good collaboration yeah and

63
00:02:06,360 --> 00:02:07,860
um i i don't know if any of

64
00:02:07,860 --> 00:02:08,640
you guys want to want to

65
00:02:08,640 --> 00:02:10,500
chime in and on like other elements of

66
00:02:10,500 --> 00:02:14,200
coming into like deciding on this uh problem

67
00:02:14,200 --> 00:02:15,240
so it's like

68
00:02:15,240 --> 00:02:17,920
probably my lab works on deep reinforcement learning

69
00:02:17,920 --> 00:02:21,520
but historically deep meant like two or three

70
00:02:21,520 --> 00:02:21,820
or

71
00:02:21,820 --> 00:02:26,260
four layers not one thousand when kevin and

72
00:02:26,260 --> 00:02:27,580
sean mentioned they want to try really deep

73
00:02:27,580 --> 00:02:28,020
networks

74
00:02:28,020 --> 00:02:29,880
it's kind of skeptical it was going to

75
00:02:29,880 --> 00:02:31,700
work i've tried this before it doesn't work

76
00:02:31,700 --> 00:02:32,280
other papers have

77
00:02:32,280 --> 00:02:33,840
tried this before and it doesn't gonna work

78
00:02:33,840 --> 00:02:36,480
so i was very very skeptical starting out

79
00:02:36,480 --> 00:02:36,820
i don't know

80
00:02:36,820 --> 00:02:38,600
if i conveyed this at the time but

81
00:02:38,600 --> 00:02:41,360
that was my prior going in because but

82
00:02:41,360 --> 00:02:42,760
do you view your job as like

83
00:02:42,760 --> 00:02:45,260
screening or like hey guys this is probably

84
00:02:45,260 --> 00:02:46,360
isn't gonna work you should try a different

85
00:02:46,360 --> 00:02:46,840
idea you

86
00:02:46,840 --> 00:02:48,940
know like or should you be encouraging even

87
00:02:48,940 --> 00:02:53,100
if it's dumb it's selecting bets yeah and

88
00:02:53,100 --> 00:02:53,660
this was a

89
00:02:53,660 --> 00:02:55,380
bet i was willing to make what what

90
00:02:55,380 --> 00:02:57,980
made you willing to make a bet it

91
00:02:57,980 --> 00:03:00,120
seemed relatively low cost uh in

92
00:03:00,120 --> 00:03:03,540
that we mihal in particular had spent the

93
00:03:03,540 --> 00:03:05,420
past year developing infrastructure that made a lot

94
00:03:05,420 --> 00:03:05,700
easier

95
00:03:05,700 --> 00:03:08,760
to run some of these experiments and the

96
00:03:08,760 --> 00:03:11,080
precedent was deep renowers should do a whole

97
00:03:11,080 --> 00:03:11,960
lot better like

98
00:03:11,960 --> 00:03:14,380
that's what the deep learning revolution has been

99
00:03:14,380 --> 00:03:15,620
over the last day yeah i know why

100
00:03:15,620 --> 00:03:16,280
do we stop making

101
00:03:16,280 --> 00:03:18,960
them deeper and reinforcement learning was like this

102
00:03:18,960 --> 00:03:21,280
one anomaly where we continue to use these

103
00:03:21,280 --> 00:03:21,580
really

104
00:03:21,580 --> 00:03:23,840
shallow networks and that's particularly true in the

105
00:03:23,840 --> 00:03:25,520
settings that we were looking at where you're

106
00:03:25,520 --> 00:03:27,940
starting from scratch you're starting from nothing any

107
00:03:27,940 --> 00:03:29,660
other perspectives you guys want to chime in

108
00:03:29,660 --> 00:03:31,600
with i guess maybe i should just go

109
00:03:31,600 --> 00:03:34,040
over like an overview of our project yes

110
00:03:34,040 --> 00:03:36,220
okay sorry yes so the way that

111
00:03:36,220 --> 00:03:38,160
kind of view our project um is that

112
00:03:38,160 --> 00:03:39,780
if you look at the landscape of deep

113
00:03:39,780 --> 00:03:41,000
learning you know you have

114
00:03:41,000 --> 00:03:44,520
nlp like language vision and then rl and

115
00:03:44,520 --> 00:03:46,200
like as ben kind of alluded to you

116
00:03:46,200 --> 00:03:47,540
know like in in language

117
00:03:47,540 --> 00:03:49,760
in vision we've sort of converged to these

118
00:03:49,760 --> 00:03:51,720
like paradigms of scaling to massive networks right

119
00:03:51,720 --> 00:03:51,860
like

120
00:03:51,860 --> 00:03:53,440
hundreds of billions of parameters trillions of parameters

121
00:03:53,440 --> 00:03:56,320
and there's been you know a lot a

122
00:03:56,320 --> 00:03:56,620
lot

123
00:03:56,620 --> 00:03:58,100
gained from in deep learning from from that

124
00:03:58,100 --> 00:04:00,000
right and then but then it seems like

125
00:04:00,000 --> 00:04:00,960
in the third sort of

126
00:04:00,960 --> 00:04:03,360
branch of deep learning in deep rl that

127
00:04:03,360 --> 00:04:05,060
has not yet been the case um like

128
00:04:05,060 --> 00:04:06,040
i was very surprised like

129
00:04:06,040 --> 00:04:08,340
coming into some like you know ben's class

130
00:04:08,340 --> 00:04:09,620
and seminar when i was looking at the

131
00:04:09,620 --> 00:04:10,440
networks oh why

132
00:04:10,440 --> 00:04:11,680
were you just using like a simple like

133
00:04:11,680 --> 00:04:14,000
two-layer mlp for like these frontier sort

134
00:04:14,000 --> 00:04:14,980
of you know state

135
00:04:14,980 --> 00:04:17,180
of the rl algorithms um and so i

136
00:04:17,180 --> 00:04:20,340
was very curious like can we design rl

137
00:04:20,340 --> 00:04:21,760
algorithms can we sort of put

138
00:04:21,760 --> 00:04:23,820
together a recipe for rl that can allow

139
00:04:23,820 --> 00:04:27,020
it to scale in potentially you know analogous

140
00:04:27,020 --> 00:04:27,420
ways that

141
00:04:27,420 --> 00:04:29,720
language envisioned my skill and so what we

142
00:04:29,720 --> 00:04:32,360
did is that we know that traditional rl

143
00:04:32,360 --> 00:04:32,800
like let's say

144
00:04:32,800 --> 00:04:35,300
like value value-based rl doesn't really scale

145
00:04:35,300 --> 00:04:36,580
right this is pretty clear from the literature

146
00:04:36,580 --> 00:04:38,340
so we tried a different approach of rl

147
00:04:38,340 --> 00:04:41,500
um called self-supervised rl where instead of

148
00:04:41,500 --> 00:04:41,980
learning like a

149
00:04:41,980 --> 00:04:45,340
value function we're learning representations of states actions

150
00:04:45,340 --> 00:04:47,260
and future states such that the

151
00:04:47,260 --> 00:04:49,940
representations along the same trajectory are pushed together

152
00:04:49,940 --> 00:04:50,860
the representations along different

153
00:04:50,860 --> 00:04:52,520
trajectories are pushed apart and this is just

154
00:04:52,520 --> 00:04:55,520
like a different approach to uh rl that

155
00:04:55,520 --> 00:04:56,360
allows us to

156
00:04:56,360 --> 00:04:58,160
learn in a self-supervised manner so there's

157
00:04:58,160 --> 00:05:00,700
we can solve task reach goals without any

158
00:05:00,700 --> 00:05:01,060
human

159
00:05:01,060 --> 00:05:03,420
crafted reward single and so we know that

160
00:05:03,420 --> 00:05:05,220
self-supervised learning is scalable in these

161
00:05:05,220 --> 00:05:07,440
different areas and if deep learning so can

162
00:05:07,440 --> 00:05:10,980
self-supervised rl scale in similar ways when

163
00:05:10,980 --> 00:05:11,080
we

164
00:05:11,080 --> 00:05:12,680
first tried it it actually didn't work like

165
00:05:12,680 --> 00:05:15,280
we've made the networks deeper the performance like

166
00:05:15,280 --> 00:05:15,540
totally

167
00:05:15,540 --> 00:05:17,880
degraded but then we also but then i

168
00:05:17,880 --> 00:05:20,560
separately was like there's also some other work

169
00:05:20,560 --> 00:05:22,000
like um in in

170
00:05:22,000 --> 00:05:24,360
our literature like we tried like residual connections

171
00:05:24,360 --> 00:05:26,340
uh and it is other a few other

172
00:05:26,340 --> 00:05:28,040
architectural components that we had to put into

173
00:05:28,040 --> 00:05:30,000
the recipe and then all of a sudden

174
00:05:30,000 --> 00:05:30,880
like one day

175
00:05:30,880 --> 00:05:33,340
like i ran this experiment and there was

176
00:05:33,340 --> 00:05:34,880
like this one environment in which there was

177
00:05:34,880 --> 00:05:35,060
like

178
00:05:35,060 --> 00:05:37,260
like going from like like doubling the depth

179
00:05:37,260 --> 00:05:39,260
didn't really do anything but like doubling the

180
00:05:39,260 --> 00:05:39,720
depth again

181
00:05:40,160 --> 00:05:43,860
with these different components suddenly like skyrocketed performance

182
00:05:43,860 --> 00:05:44,980
in this one environment

183
00:05:44,980 --> 00:05:47,760
getting this to work was very non-trivial

184
00:05:47,760 --> 00:05:49,100
in the sense that like usually when we

185
00:05:49,100 --> 00:05:49,700
think about doing

186
00:05:49,700 --> 00:05:52,980
hyperparameter optimization we try changing a see if

187
00:05:52,980 --> 00:05:54,840
it makes it better try changing b see

188
00:05:54,840 --> 00:05:55,100
whether

189
00:05:55,100 --> 00:05:57,260
it makes it better and if we just

190
00:05:57,260 --> 00:05:59,520
made the depth bigger makes it worse we

191
00:05:59,520 --> 00:06:00,480
just had residual connections

192
00:06:00,480 --> 00:06:02,060
didn't make it better and it was really

193
00:06:02,060 --> 00:06:04,540
this combination of factors that kevin and sean

194
00:06:04,540 --> 00:06:04,980
figured

195
00:06:04,980 --> 00:06:07,440
out that really made this work and as

196
00:06:07,440 --> 00:06:08,920
a precursor to that we also try scaling

197
00:06:08,920 --> 00:06:09,820
along different dimensions

198
00:06:09,820 --> 00:06:12,660
so scaling the batch size uh scaling the

199
00:06:12,660 --> 00:06:13,900
the width of the network so the hidden

200
00:06:13,900 --> 00:06:15,140
layers in effect

201
00:06:15,800 --> 00:06:17,500
yeah pretty much kind of similar to just

202
00:06:17,500 --> 00:06:20,380
scaling depth naively yeah um and then once

203
00:06:20,380 --> 00:06:20,680
we started

204
00:06:20,680 --> 00:06:23,840
introducing residual connections layer norm these specific architectural

205
00:06:23,840 --> 00:06:25,400
choices that's when we saw

206
00:06:25,400 --> 00:06:28,420
these significant jumps in performance like these critical

207
00:06:28,420 --> 00:06:31,180
depths at which performance multiplies by a

208
00:06:31,180 --> 00:06:33,400
pretty huge factor and that's where we really

209
00:06:33,400 --> 00:06:36,240
noticed like unlocking some significant performance gains

210
00:06:36,900 --> 00:06:39,640
as opposed to scaling just along with which

211
00:06:39,640 --> 00:06:42,580
did yield some performance improvements um but when

212
00:06:42,580 --> 00:06:42,700
you

213
00:06:42,700 --> 00:06:44,220
look at the number of parameters that your

214
00:06:44,220 --> 00:06:46,740
network has as you grow with it's roughly

215
00:06:46,740 --> 00:06:47,660
a quadratic as

216
00:06:47,660 --> 00:06:49,460
opposed to something like growing depth so it's

217
00:06:49,460 --> 00:06:51,400
more in some sense it's more parameter efficient

218
00:06:51,400 --> 00:06:51,700
also

219
00:06:51,700 --> 00:06:54,040
more sample efficient from the experiments that we

220
00:06:54,040 --> 00:06:56,900
conducted nice um in some ways you're sort

221
00:06:56,900 --> 00:06:57,020
of

222
00:06:57,020 --> 00:06:59,480
replicating stuff that is seen in the wild

223
00:06:59,480 --> 00:07:02,140
but on on a very small model that

224
00:07:02,140 --> 00:07:03,660
you can study is that would

225
00:07:03,660 --> 00:07:04,720
you would you say that's yeah so i

226
00:07:04,720 --> 00:07:06,260
kind of add to what kevin said earlier

227
00:07:06,260 --> 00:07:07,760
we saw these huge performance

228
00:07:07,760 --> 00:07:11,100
improvements in language models image generation models by

229
00:07:11,100 --> 00:07:12,440
making them larger making them deeper

230
00:07:12,440 --> 00:07:15,000
which seems very intuitive yeah and so that's

231
00:07:15,000 --> 00:07:18,200
why our work we draw from like foundational

232
00:07:18,200 --> 00:07:18,700
research

233
00:07:18,700 --> 00:07:21,680
right like uh residual networks which employ residual

234
00:07:21,680 --> 00:07:24,740
connections to avoid uh vanishing gradients and

235
00:07:24,740 --> 00:07:25,800
that's something that we show in some of

236
00:07:25,800 --> 00:07:27,660
our ablations in our in our paper further

237
00:07:27,660 --> 00:07:28,000
down

238
00:07:28,000 --> 00:07:30,100
it's probably in the appendices where you did

239
00:07:30,100 --> 00:07:32,120
experiments without these residual connections

240
00:07:32,120 --> 00:07:34,420
and so it's sort of borrowing these concepts

241
00:07:34,420 --> 00:07:37,080
that have existed in other fields and applying

242
00:07:37,080 --> 00:07:37,260
them

243
00:07:37,260 --> 00:07:39,560
to this setting with uh rl and showing

244
00:07:39,560 --> 00:07:41,840
that it works before ben has to have

245
00:07:41,840 --> 00:07:43,040
to go i'll leave the sort

246
00:07:43,040 --> 00:07:45,400
of last word uh to him what additional

247
00:07:45,400 --> 00:07:47,360
work does this inspire that like that you

248
00:07:47,360 --> 00:07:49,560
want to push on next i think

249
00:07:49,560 --> 00:07:51,020
there's one thing i'd clarify about the paper

250
00:07:51,020 --> 00:07:53,640
and then i'll directly answer the question yes

251
00:07:53,640 --> 00:07:53,880
i think

252
00:07:53,880 --> 00:07:55,240
the thing i might clarify about the paper

253
00:07:55,240 --> 00:07:56,200
is i think a lot of people reading

254
00:07:56,200 --> 00:07:57,800
the title are like wow big networks

255
00:07:57,800 --> 00:07:59,880
they're great i'll take big networks you solved

256
00:07:59,880 --> 00:08:01,240
it now we can just go yeah just

257
00:08:01,240 --> 00:08:01,700
take big networks

258
00:08:01,700 --> 00:08:03,900
add them to ppo add them to sac

259
00:08:03,900 --> 00:08:05,880
add them to your favorite reinforcement learning algorithm

260
00:08:05,880 --> 00:08:06,560
but i think

261
00:08:06,560 --> 00:08:07,980
that's actually not the main conclusion i think

262
00:08:07,980 --> 00:08:10,280
the main conclusion is that using big networks

263
00:08:10,280 --> 00:08:11,060
not only

264
00:08:11,060 --> 00:08:14,280
requires these architectural tricks but also as kevin

265
00:08:14,280 --> 00:08:16,240
mentioned before it requires using a different

266
00:08:16,240 --> 00:08:20,100
objective this objective doesn't actually use rewards in

267
00:08:20,100 --> 00:08:22,280
it and so there's another word in the

268
00:08:22,280 --> 00:08:25,060
title reinforcement learning that also might be a

269
00:08:25,060 --> 00:08:27,460
little bit of a misnomer because we aren't

270
00:08:27,460 --> 00:08:30,300
directly trying to maximize rewards our code doesn't

271
00:08:30,300 --> 00:08:32,580
have a line of code saying maximize rewards

272
00:08:32,580 --> 00:08:35,400
here and so is at the end of

273
00:08:35,400 --> 00:08:37,120
the day this a reinforcement learning method i

274
00:08:37,120 --> 00:08:37,440
don't know

275
00:08:37,440 --> 00:08:39,460
it looks much more similar to the self

276
00:08:39,460 --> 00:08:42,080
-supervised methods in other areas of machine learning

277
00:08:42,080 --> 00:08:43,060
and so i

278
00:08:43,060 --> 00:08:45,440
think that the method the work really stands

279
00:08:45,440 --> 00:08:47,980
in some sort of interesting intersection of reinforcement

280
00:08:47,980 --> 00:08:52,660
learning and self-supervised learning research and we

281
00:08:52,660 --> 00:08:54,520
had this little figure on the bottom left

282
00:08:54,520 --> 00:08:55,160
of

283
00:08:55,160 --> 00:08:58,000
the poster which was the screenshot of a

284
00:08:58,000 --> 00:09:00,440
slide from young lakun talking about how to

285
00:09:00,440 --> 00:09:01,020
build intelligent

286
00:09:01,020 --> 00:09:03,040
systems and whether that's going to be done

287
00:09:03,040 --> 00:09:05,880
by unsupervised learning or supervised learning and

288
00:09:05,880 --> 00:09:07,940
reinforcement learning and i think what our paper

289
00:09:07,940 --> 00:09:10,560
really suggests is that the boundary between these

290
00:09:10,560 --> 00:09:12,460
things is really blurry and maybe the keys

291
00:09:12,460 --> 00:09:13,940
to building intelligent systems are going to be

292
00:09:13,940 --> 00:09:14,240
leveraging

293
00:09:14,240 --> 00:09:16,660
insights from all of them yeah the layer

294
00:09:16,660 --> 00:09:20,400
kick exactly uh well thank you for your

295
00:09:20,400 --> 00:09:21,500
time i know i know you have to

296
00:09:21,500 --> 00:09:21,660
go

297
00:09:21,660 --> 00:09:24,060
soon yeah thank you so much for coming

298
00:09:24,060 --> 00:09:26,780
i think that that insight of like blurring

299
00:09:26,780 --> 00:09:27,760
things is interesting

300
00:09:27,760 --> 00:09:29,320
i don't know if you like you were

301
00:09:29,320 --> 00:09:31,600
talking about so like uh the abstraction layer

302
00:09:31,600 --> 00:09:32,520
of representation

303
00:09:32,520 --> 00:09:34,980
learning i don't know if that triggers anything

304
00:09:34,980 --> 00:09:37,680
in terms of like the mix between self

305
00:09:37,680 --> 00:09:38,440
-supervised and

306
00:09:38,440 --> 00:09:40,620
reinforcement learning is that is there something fundamental

307
00:09:40,620 --> 00:09:42,740
that you've discovered or that we've

308
00:09:42,740 --> 00:09:44,900
that people don't understand when they when they

309
00:09:44,900 --> 00:09:46,700
read the paper yeah i think the best

310
00:09:46,700 --> 00:09:47,260
way that i would

311
00:09:47,260 --> 00:09:50,100
explain it is that we know that standard

312
00:09:50,100 --> 00:09:52,540
rl is not super scalable and so like

313
00:09:52,540 --> 00:09:53,680
why can this a different

314
00:09:53,680 --> 00:09:56,120
approach or different objective rl be scalable i

315
00:09:56,120 --> 00:09:58,220
think it's because we're fundamentally shifting the

316
00:09:58,780 --> 00:10:01,900
burden of learning from something like like q

317
00:10:01,900 --> 00:10:03,800
learning or like regressing to like td errors

318
00:10:03,800 --> 00:10:04,240
which

319
00:10:04,240 --> 00:10:05,780
we know is quite spurious and noisy and

320
00:10:05,780 --> 00:10:09,520
biased to fundamentally like a classification problem we're

321
00:10:09,520 --> 00:10:12,280
trying to classify whether future states is along

322
00:10:12,280 --> 00:10:13,980
the same trajectory or along a different trajectory

323
00:10:14,540 --> 00:10:16,740
and we do this with representation learning right

324
00:10:16,740 --> 00:10:19,620
and we know that classification cross-entry loss

325
00:10:19,620 --> 00:10:22,160
and representation learning is scalable in the deep

326
00:10:22,160 --> 00:10:23,760
learning literature right if we think about language

327
00:10:24,700 --> 00:10:26,920
and like some of the objectives there so

328
00:10:26,920 --> 00:10:30,060
in some sense we're kind of blurring the

329
00:10:30,060 --> 00:10:30,660
the lines we're

330
00:10:30,660 --> 00:10:32,560
doing reinforcement learning it's still an actor critic

331
00:10:32,560 --> 00:10:34,720
reinforcement learning algorithm it's like a goal

332
00:10:34,720 --> 00:10:37,760
condition reinforcement algorithm but the objective the burden

333
00:10:37,760 --> 00:10:39,940
of like learning of the of solving that

334
00:10:39,940 --> 00:10:40,160
rl

335
00:10:40,160 --> 00:10:42,680
task shifts to something that's more similar to

336
00:10:42,680 --> 00:10:45,280
objectives that you might see in language and

337
00:10:45,280 --> 00:10:46,560
vision that we know have scaled so much

338
00:10:46,560 --> 00:10:48,580
and so i think yeah i think that's

339
00:10:48,580 --> 00:10:49,500
like one of the fundamental

340
00:10:49,500 --> 00:10:52,280
insights that we've seen is that um it

341
00:10:52,280 --> 00:10:54,220
seems like by approaching rl in this different

342
00:10:54,220 --> 00:10:55,400
approach we're

343
00:10:55,400 --> 00:10:57,540
able to like get so much more out

344
00:10:57,540 --> 00:10:59,060
of we were able to scale our networks

345
00:10:59,060 --> 00:11:00,960
like significantly beyond what

346
00:11:00,960 --> 00:11:02,940
was like standard used in ara can i

347
00:11:02,940 --> 00:11:05,100
jump in i will just give a bit

348
00:11:05,100 --> 00:11:06,700
of more of context about the

349
00:11:06,700 --> 00:11:10,500
architecture because uh yeah we use another objective

350
00:11:10,500 --> 00:11:14,040
and the uh influences so the contrastive

351
00:11:14,040 --> 00:11:17,280
laws however the architecture is uh quite similar

352
00:11:17,280 --> 00:11:20,100
to the previous works uh of previous uh

353
00:11:20,100 --> 00:11:20,800
papers like

354
00:11:20,800 --> 00:11:24,320
bro or or uh simba simba v1 simba

355
00:11:24,320 --> 00:11:29,200
v2 uh simba v1 simba v2 so we

356
00:11:29,200 --> 00:11:31,000
we also tweaked a bit of this

357
00:11:31,000 --> 00:11:34,600
uh architecture however it's not that we like

358
00:11:34,600 --> 00:11:36,760
uh invented the wheel for the first time

359
00:11:36,760 --> 00:11:37,720
it's the

360
00:11:37,720 --> 00:11:40,400
merging between the architecture and the objective that

361
00:11:40,400 --> 00:11:43,520
makes the scale uh really uh like go

362
00:11:43,520 --> 00:11:44,460
up and

363
00:11:44,460 --> 00:11:46,700
and and performance follow the the scale i

364
00:11:46,700 --> 00:11:48,980
think that's something that we should uh probably

365
00:11:48,980 --> 00:11:50,000
mine deeper

366
00:11:50,000 --> 00:11:52,800
um do you think i guess like what

367
00:11:52,800 --> 00:11:55,080
domains what industry like you've applied it on

368
00:11:55,080 --> 00:11:56,000
multiple different

369
00:11:56,000 --> 00:11:58,120
uh types of networks that are or data

370
00:11:58,120 --> 00:12:01,060
sets is there a particular affinity that you

371
00:12:01,060 --> 00:12:01,820
think like has is

372
00:12:01,820 --> 00:12:04,100
like kind of low-hanging food yeah so

373
00:12:04,100 --> 00:12:05,180
actually if you look at a lot of

374
00:12:05,180 --> 00:12:07,080
our tasks they're particularly

375
00:12:07,080 --> 00:12:09,700
sort of like robotics tasks um so this

376
00:12:09,700 --> 00:12:12,420
is a person i'd be very curious about

377
00:12:12,420 --> 00:12:13,780
how a work like this could

378
00:12:13,780 --> 00:12:16,180
impact like the robotics field like my understanding

379
00:12:16,180 --> 00:12:17,760
of robotics is that a lot of robotics

380
00:12:17,760 --> 00:12:18,100
are now

381
00:12:18,100 --> 00:12:20,060
there's kind of multi a few different approaches

382
00:12:20,060 --> 00:12:22,220
like one approach is we want to train

383
00:12:22,220 --> 00:12:23,360
robots using

384
00:12:23,360 --> 00:12:24,940
imitation learning so we try to collect like

385
00:12:24,940 --> 00:12:26,560
an insane amount of data we have a

386
00:12:26,560 --> 00:12:27,300
ton of human

387
00:12:27,820 --> 00:12:29,320
supervision and we try to scale up this

388
00:12:29,320 --> 00:12:31,040
data and we're like learning with imitation learning

389
00:12:31,040 --> 00:12:33,840
like but on the other hand potentially like

390
00:12:33,840 --> 00:12:36,020
perhaps there's another approach which is like

391
00:12:36,020 --> 00:12:38,440
for example like goal condition reinforcement learning where

392
00:12:38,440 --> 00:12:39,900
we can actually train robotic

393
00:12:39,900 --> 00:12:42,240
agents and training rl agents to solve meaningful

394
00:12:42,240 --> 00:12:44,900
tasks with absolutely no human supervision no

395
00:12:44,900 --> 00:12:47,380
demonstration it's much more scalable yeah so yeah

396
00:12:47,380 --> 00:12:49,440
so this could serve as an alternate approach

397
00:12:49,440 --> 00:12:51,360
and perhaps instead of like scaling data like

398
00:12:51,360 --> 00:12:53,960
scaling manual like human supervision which

399
00:12:53,960 --> 00:12:56,140
which is you know not super scalable if

400
00:12:56,140 --> 00:12:58,040
there are ways to sort of make goal

401
00:12:58,040 --> 00:12:58,640
condition reinforcement

402
00:12:58,640 --> 00:13:01,000
learning scalable and like we can just scale

403
00:13:01,000 --> 00:13:02,720
the architecture or we can scale because you're

404
00:13:02,720 --> 00:13:04,520
focused on your objectives yeah right with with

405
00:13:04,520 --> 00:13:05,980
certain different objectives i think that could be

406
00:13:05,980 --> 00:13:07,580
very exciting and see how to see how

407
00:13:07,580 --> 00:13:09,360
that can affect a field like robotics for

408
00:13:09,360 --> 00:13:11,260
example yeah uh double

409
00:13:11,260 --> 00:13:13,020
click on on just one one thing on

410
00:13:13,020 --> 00:13:14,780
the efficiency which you guys are talking about

411
00:13:14,780 --> 00:13:15,780
i would expect

412
00:13:16,320 --> 00:13:18,640
the very deep the deeper it is there

413
00:13:18,640 --> 00:13:21,320
should be quadratically worse i am not familiar

414
00:13:21,320 --> 00:13:21,740
with like

415
00:13:21,740 --> 00:13:23,760
the the pre-existing literature i'm just like

416
00:13:23,760 --> 00:13:27,620
sort of working out intuitions but um basically

417
00:13:27,620 --> 00:13:28,420
uh what

418
00:13:28,420 --> 00:13:30,680
are the trade-offs that you've found that

419
00:13:30,680 --> 00:13:32,080
i think you might want to warn people

420
00:13:32,080 --> 00:13:33,640
about because because you

421
00:13:33,640 --> 00:13:35,380
you are the guy who mentioned efficiency so

422
00:13:35,380 --> 00:13:37,200
yeah sure sure yeah so i was referring

423
00:13:37,200 --> 00:13:37,680
to like one of the

424
00:13:37,680 --> 00:13:39,320
figures on our poster also in our paper

425
00:13:39,320 --> 00:13:41,300
where we compare like the number of parameters

426
00:13:41,300 --> 00:13:42,480
that models have

427
00:13:42,480 --> 00:13:44,220
as we see along the axis of depth

428
00:13:44,220 --> 00:13:46,080
and as we scale along the axis of

429
00:13:46,080 --> 00:13:47,480
width yeah from our baseline

430
00:13:47,480 --> 00:13:49,700
architecture the most baseline one would be like

431
00:13:49,700 --> 00:13:52,360
a width of 256 like the hidden layers

432
00:13:52,360 --> 00:13:53,320
of 256 neurons

433
00:13:53,320 --> 00:13:55,180
and then the depth is for four layers

434
00:13:55,180 --> 00:13:58,320
or hidden layers um and so the point

435
00:13:58,320 --> 00:13:59,040
i was making there is

436
00:13:59,040 --> 00:14:00,760
that when you scale along depth your the

437
00:14:00,760 --> 00:14:02,500
number of parameters that your model has is

438
00:14:02,500 --> 00:14:02,820
going to grow

439
00:14:02,820 --> 00:14:06,480
roughly linearly uh whereas with with you're making

440
00:14:06,480 --> 00:14:09,480
your network outputs wider and then the input

441
00:14:09,480 --> 00:14:10,080
to the net

442
00:14:10,080 --> 00:14:12,640
next network is also growing as well and

443
00:14:12,640 --> 00:14:14,220
so the the number of parameters your network's

444
00:14:14,220 --> 00:14:14,480
then going

445
00:14:14,480 --> 00:14:17,360
to have grows approximately quadratically and so one

446
00:14:17,360 --> 00:14:18,640
of the experiments we did was sort of

447
00:14:18,640 --> 00:14:19,160
examining

448
00:14:19,680 --> 00:14:21,280
as we grow the number of parameters in

449
00:14:21,280 --> 00:14:23,020
our model by scaling along these two different

450
00:14:23,020 --> 00:14:23,520
choices

451
00:14:23,980 --> 00:14:26,640
which one for the same like approximate number

452
00:14:26,640 --> 00:14:28,960
of parameters yields a better performance and the

453
00:14:28,960 --> 00:14:29,140
depth

454
00:14:29,140 --> 00:14:30,460
curve kind of goes like this it jumps

455
00:14:30,460 --> 00:14:32,900
up pretty fast that's like present throughout our

456
00:14:32,900 --> 00:14:33,900
paper for with

457
00:14:33,900 --> 00:14:35,180
it grows a little bit more slowly and

458
00:14:35,180 --> 00:14:36,740
so that the kind of takeaway from that

459
00:14:36,740 --> 00:14:38,220
is that if you

460
00:14:38,220 --> 00:14:40,900
are a bit more resource constrained scaling along

461
00:14:40,900 --> 00:14:42,180
depth might be better because there's fewer

462
00:14:42,180 --> 00:14:44,620
parameters with a smaller model to a smaller

463
00:14:44,620 --> 00:14:47,380
number tool learnable parameters width is expensive

464
00:14:47,380 --> 00:14:49,940
width is expensive exactly and in general of

465
00:14:49,940 --> 00:14:51,340
course like more parameters is also going to

466
00:14:51,340 --> 00:14:51,600
be more

467
00:14:51,600 --> 00:14:54,540
expensive so that's just like another consideration to

468
00:14:54,540 --> 00:14:56,740
think about when using these networks and suppose

469
00:14:56,740 --> 00:14:59,280
yeah any other sort of rules of thumbs

470
00:14:59,280 --> 00:15:00,720
like that that i can extract that this

471
00:15:00,720 --> 00:15:01,400
is just the most basic

472
00:15:01,400 --> 00:15:03,640
one that i could think of yeah uh

473
00:15:03,640 --> 00:15:05,680
i don't know if there's any others yeah

474
00:15:05,680 --> 00:15:07,140
i guess like your original

475
00:15:07,140 --> 00:15:09,240
question of like the trade-offs um like

476
00:15:09,240 --> 00:15:10,260
one of the trade-offs one of the

477
00:15:10,260 --> 00:15:11,180
limitations that we say is

478
00:15:11,180 --> 00:15:12,620
like obviously if you make the networks bigger

479
00:15:12,620 --> 00:15:15,140
the it will take longer to run right

480
00:15:15,140 --> 00:15:17,100
so if you like

481
00:15:17,100 --> 00:15:18,700
double the depth at some level of depth

482
00:15:18,700 --> 00:15:20,580
you you it might take like twice as

483
00:15:20,580 --> 00:15:21,820
much to like take make a

484
00:15:21,820 --> 00:15:24,420
forward pass through the network right however this

485
00:15:24,420 --> 00:15:26,520
is not so like within our paper like

486
00:15:26,520 --> 00:15:27,800
for most

487
00:15:27,800 --> 00:15:30,320
environments um we are able to like saturate

488
00:15:30,320 --> 00:15:32,580
like get to like almost perfect performance within

489
00:15:32,580 --> 00:15:33,480
just you know

490
00:15:33,480 --> 00:15:34,580
we don't even need to get to like

491
00:15:34,580 --> 00:15:36,500
a thousand layers like maybe just 64 layers

492
00:15:36,500 --> 00:15:37,540
for example is

493
00:15:37,540 --> 00:15:40,840
sufficient um and in this regime like like

494
00:15:40,840 --> 00:15:42,500
the latency of the network is not necessarily

495
00:15:42,500 --> 00:15:42,740
actually

496
00:15:42,740 --> 00:15:45,640
even uh not necessarily like a significant bottleneck

497
00:15:45,640 --> 00:15:47,000
like you can imagine there's a lot of

498
00:15:47,000 --> 00:15:47,500
tasks in

499
00:15:47,500 --> 00:15:50,120
which especially in rl that like collecting data

500
00:15:50,120 --> 00:15:52,460
might be the bottleneck right and making four

501
00:15:52,460 --> 00:15:52,700
passes

502
00:15:52,700 --> 00:15:54,480
through our network may not be the bottleneck

503
00:15:54,480 --> 00:15:56,940
and so in our environment we in our

504
00:15:56,940 --> 00:15:57,340
research we

505
00:15:57,340 --> 00:16:00,560
specifically use the jacks gcrl environment which is

506
00:16:00,560 --> 00:16:03,040
a jack space gpu accelerator environment so we

507
00:16:03,040 --> 00:16:03,180
can

508
00:16:03,180 --> 00:16:06,500
collect like thousands of like environment trajectories like

509
00:16:06,500 --> 00:16:07,760
in parallel at the same time

510
00:16:07,760 --> 00:16:09,980
so that we're able to like make uh

511
00:16:09,980 --> 00:16:12,460
like oh this is built in right this

512
00:16:12,460 --> 00:16:13,760
is built in so that we can

513
00:16:13,760 --> 00:16:15,900
collect you know like like a thousand trajectories

514
00:16:15,900 --> 00:16:17,880
at the same time along all these environments

515
00:16:17,880 --> 00:16:18,140
and

516
00:16:18,140 --> 00:16:20,520
so um makes that make sure that like

517
00:16:20,520 --> 00:16:22,600
we have enough data to like saturate the

518
00:16:22,600 --> 00:16:23,340
learning from wow

519
00:16:24,000 --> 00:16:26,400
yeah that's like work they've been called okay

520
00:16:26,400 --> 00:16:27,840
and you i don't know if you want

521
00:16:27,840 --> 00:16:29,280
to explore expand upon

522
00:16:29,280 --> 00:16:32,440
that on the jargon zcrl uh maybe and

523
00:16:32,440 --> 00:16:33,820
you know most people are familiar with pytor

524
00:16:33,820 --> 00:16:34,340
is maybe less

525
00:16:34,340 --> 00:16:36,040
familiar with jacks uh with jacks i think

526
00:16:36,040 --> 00:16:39,560
jacks is getting the uh the traction especially

527
00:16:39,560 --> 00:16:40,540
in rl field

528
00:16:40,540 --> 00:16:43,920
because the in for online reinforcement learning getting

529
00:16:43,920 --> 00:16:46,760
as much data as you can is is

530
00:16:46,760 --> 00:16:47,080
the most

531
00:16:47,080 --> 00:16:48,880
important there's got to be a pytorch equivalence

532
00:16:48,880 --> 00:16:52,140
but anyway any tips for other people also

533
00:16:52,140 --> 00:16:52,540
exploring

534
00:16:52,540 --> 00:16:54,920
this kind of uh rollout yeah yeah so

535
00:16:54,920 --> 00:16:57,640
i think i can also recommend like uh

536
00:16:57,640 --> 00:16:58,880
for for gold conditioned

537
00:16:58,880 --> 00:17:01,420
rl i'm recommending jacks this year but there

538
00:17:01,420 --> 00:17:04,560
are also like multi-agent jacks uh implementation

539
00:17:04,560 --> 00:17:05,240
and

540
00:17:05,240 --> 00:17:07,660
other so going back to our paper if

541
00:17:07,660 --> 00:17:09,540
you look at the plots we only see

542
00:17:09,540 --> 00:17:11,580
this like huge performance

543
00:17:11,580 --> 00:17:15,840
increase when we cross like 50 uh millions

544
00:17:15,840 --> 00:17:19,660
of uh transitions uh gap so so i

545
00:17:19,660 --> 00:17:21,100
think the data is crucial

546
00:17:21,100 --> 00:17:23,540
like here yeah i guess even to build

547
00:17:23,540 --> 00:17:25,880
on that like i like drawing analogies to

548
00:17:25,880 --> 00:17:27,240
like successes in other

549
00:17:27,240 --> 00:17:29,980
areas of deep learning like for example in

550
00:17:29,980 --> 00:17:32,580
large language models the reason why we're able

551
00:17:32,580 --> 00:17:32,920
to scale

552
00:17:32,920 --> 00:17:34,680
to such large networks is that we found

553
00:17:34,680 --> 00:17:36,440
a paradigm in which we can leverage the

554
00:17:36,440 --> 00:17:37,220
entire internet scale

555
00:17:37,220 --> 00:17:40,020
of data is alert right and so data

556
00:17:40,020 --> 00:17:41,920
in rl traditionally has been hard to come

557
00:17:41,920 --> 00:17:44,440
by um but now with these like

558
00:17:44,440 --> 00:17:46,780
gpu accelerated environments we can collect hundreds of

559
00:17:46,780 --> 00:17:48,260
millions of times of the data within just

560
00:17:48,260 --> 00:17:48,500
a few

561
00:17:48,500 --> 00:17:50,700
hours and so i think that this serves

562
00:17:50,700 --> 00:17:52,660
as like a really good test bed for

563
00:17:52,660 --> 00:17:53,540
us to be able to also

564
00:17:53,540 --> 00:17:56,080
find ways to scale up um like network

565
00:17:56,080 --> 00:17:58,160
capacity and get similar kind of games i

566
00:17:58,160 --> 00:17:58,960
think i asked are you

567
00:17:58,960 --> 00:18:01,720
saying that you have a difference you would

568
00:18:01,720 --> 00:18:05,340
do pre-training differently in llms like what's

569
00:18:05,340 --> 00:18:05,900
the what's

570
00:18:05,900 --> 00:18:09,260
the difference uh objective now um yeah i

571
00:18:09,260 --> 00:18:12,540
mean very simply very simply the the paradigm

572
00:18:12,540 --> 00:18:12,840
that you're

573
00:18:12,840 --> 00:18:15,420
referencing is next word or next token right

574
00:18:15,420 --> 00:18:19,580
it's very robust how do you change that

575
00:18:19,580 --> 00:18:20,460
oh i'm not

576
00:18:20,460 --> 00:18:22,040
saying that we're changing i want to leverage

577
00:18:22,040 --> 00:18:24,840
insights from that to apply to all i

578
00:18:24,840 --> 00:18:25,660
feel like

579
00:18:25,660 --> 00:18:26,800
you should go the other way you think

580
00:18:26,800 --> 00:18:28,560
you should go the other way yeah maybe

581
00:18:28,560 --> 00:18:29,980
i mean i would be a very

582
00:18:29,980 --> 00:18:32,260
interesting research direction too but actually yeah even

583
00:18:32,260 --> 00:18:33,500
on that point like one of the things

584
00:18:33,500 --> 00:18:33,700
i was

585
00:18:33,700 --> 00:18:35,520
thinking about is that the way that our

586
00:18:35,520 --> 00:18:37,880
rl objective works is in some set it's

587
00:18:37,880 --> 00:18:38,620
not exactly next

588
00:18:39,120 --> 00:18:41,640
word prediction but it's kind of like next

589
00:18:41,640 --> 00:18:43,800
state prediction right you imagine you're at some

590
00:18:43,800 --> 00:18:44,040
current

591
00:18:44,040 --> 00:18:46,500
state and you're at some current action and

592
00:18:46,500 --> 00:18:47,580
we want to predict whether or not this

593
00:18:47,580 --> 00:18:48,060
future state

594
00:18:48,060 --> 00:18:50,400
this this certain state is a future state

595
00:18:50,400 --> 00:18:51,840
along the same trajectory or a different trajectory

596
00:18:52,240 --> 00:18:54,500
and so in some sense we are actually

597
00:18:54,500 --> 00:18:56,900
doing some sort of like implicit world model

598
00:18:56,900 --> 00:18:58,680
is it like uh you

599
00:18:58,680 --> 00:18:59,920
know like i don't know if that's a

600
00:18:59,920 --> 00:19:01,460
bad word these things or is or like

601
00:19:01,460 --> 00:19:02,760
in language you you do a cross

602
00:19:02,760 --> 00:19:04,160
entry loss to classify the next token right

603
00:19:04,160 --> 00:19:05,940
and here we're just doing a binary classification

604
00:19:05,940 --> 00:19:06,460
of like

605
00:19:06,460 --> 00:19:08,140
whether or not some next state is some

606
00:19:08,140 --> 00:19:10,420
yeah yeah it's a classification yeah yeah yeah

607
00:19:10,420 --> 00:19:11,340
and so i do

608
00:19:11,340 --> 00:19:13,600
see that there are some like sort of

609
00:19:13,600 --> 00:19:15,820
parallels here that perhaps we should dig into

610
00:19:15,820 --> 00:19:16,380
deeper and

611
00:19:16,380 --> 00:19:18,940
see like what is the core to of

612
00:19:18,940 --> 00:19:21,100
what enables deep learning to scale and then

613
00:19:21,100 --> 00:19:22,440
how can we like leverage

614
00:19:22,440 --> 00:19:24,680
that how can we distill those like insights

615
00:19:24,680 --> 00:19:26,560
and then apply those across like all different

616
00:19:26,560 --> 00:19:26,800
fields

617
00:19:26,800 --> 00:19:29,720
whether it's language or reinforcement learning yeah uh

618
00:19:29,720 --> 00:19:31,040
did you did you get my my meaning

619
00:19:31,040 --> 00:19:31,360
about the

620
00:19:31,360 --> 00:19:33,940
world model stuff yeah yeah actually and i

621
00:19:33,940 --> 00:19:35,600
i heard i think i might have heard

622
00:19:35,600 --> 00:19:36,400
professor eisenbach

623
00:19:36,400 --> 00:19:37,820
yesterday talking about this at a poster and

624
00:19:37,820 --> 00:19:39,640
he's explaining to a couple of people that

625
00:19:39,640 --> 00:19:40,300
because

626
00:19:40,300 --> 00:19:42,520
this is like doing representation learning and trying

627
00:19:42,520 --> 00:19:43,900
to learn these meaningful representations

628
00:19:44,360 --> 00:19:45,900
for a given state of action been for

629
00:19:45,900 --> 00:19:47,800
a given goal in some sense you can

630
00:19:47,800 --> 00:19:48,700
think of it almost like

631
00:19:48,700 --> 00:19:50,800
learning a model of environment learning a model

632
00:19:50,800 --> 00:19:52,580
of the world but without having to do

633
00:19:52,580 --> 00:19:53,220
any sort of like

634
00:19:53,220 --> 00:19:56,180
next frame prediction or stuff like that that's

635
00:19:56,180 --> 00:19:58,060
a little bit more high dimensional and complex

636
00:19:58,060 --> 00:20:00,120
yeah yeah i would think like

637
00:20:00,120 --> 00:20:02,160
um the the angle that i'm trying to

638
00:20:02,160 --> 00:20:05,640
think about and push is instead of learn

639
00:20:05,640 --> 00:20:06,860
the next world they're

640
00:20:06,860 --> 00:20:09,120
basically like generate a number of candidates possible

641
00:20:09,120 --> 00:20:11,560
worlds and classify them uh to your point

642
00:20:11,560 --> 00:20:14,020
uh which is exactly how i do things

643
00:20:14,020 --> 00:20:15,680
let's say i'm playing poker and i'm trying

644
00:20:15,680 --> 00:20:16,740
to classify what hands you

645
00:20:16,740 --> 00:20:18,820
have well there's a range of hands based

646
00:20:18,820 --> 00:20:20,820
on what you're you're doing and the more

647
00:20:20,820 --> 00:20:21,680
information i get

648
00:20:21,680 --> 00:20:24,240
the more i resolve to oh i know

649
00:20:24,240 --> 00:20:25,560
exactly what hand you have based on what

650
00:20:25,560 --> 00:20:27,580
you're showing you know or

651
00:20:27,580 --> 00:20:29,940
you're buffing but that's a different thing but

652
00:20:29,940 --> 00:20:31,200
you know what i mean like i i

653
00:20:31,200 --> 00:20:32,240
feel like that is

654
00:20:32,240 --> 00:20:35,360
the ultimate sort of angle of representation which

655
00:20:35,360 --> 00:20:37,760
is a world but i don't know if

656
00:20:37,760 --> 00:20:38,800
that is too vague

657
00:20:38,800 --> 00:20:41,440
compared to the more concrete types of world

658
00:20:41,440 --> 00:20:43,720
models that let's say the video gen people

659
00:20:43,720 --> 00:20:44,040
are doing

660
00:20:44,800 --> 00:20:46,900
and then i guess one other thing like

661
00:20:46,900 --> 00:20:49,060
i i'm also exploring i you mentioned like

662
00:20:49,060 --> 00:20:50,440
the deep models

663
00:20:50,440 --> 00:20:53,560
being slower or more expensive yeah that is

664
00:20:53,560 --> 00:20:55,140
a trend in the inference world of making

665
00:20:55,140 --> 00:20:55,780
models shallower

666
00:20:55,780 --> 00:20:58,880
right and i wonder if this like short

667
00:20:58,880 --> 00:21:00,800
catchphrase i was thinking about like deep teacher

668
00:21:02,620 --> 00:21:06,940
shallow student would be a good deployment paradigm

669
00:21:06,940 --> 00:21:10,060
yeah like you push the frontier capabilities with

670
00:21:10,060 --> 00:21:10,180
the

671
00:21:10,180 --> 00:21:12,280
with death and then you distill it back

672
00:21:12,280 --> 00:21:14,320
yeah actually this is a good point like

673
00:21:14,320 --> 00:21:14,880
if you go out to our

674
00:21:14,880 --> 00:21:17,040
website like this is one of the future

675
00:21:17,040 --> 00:21:18,420
directions that we list at the very bottom

676
00:21:18,420 --> 00:21:21,160
oh okay yeah uh we

677
00:21:21,160 --> 00:21:23,000
we we would love to see if we

678
00:21:23,000 --> 00:21:25,240
could get similar performance like we push the

679
00:21:25,240 --> 00:21:26,180
you know like we do

680
00:21:26,180 --> 00:21:28,380
achieve state-of-the-art performance on uh

681
00:21:28,380 --> 00:21:30,300
gold condition rl in jackson crl by a

682
00:21:30,300 --> 00:21:30,980
significant amount

683
00:21:30,980 --> 00:21:32,520
and so it's very exciting to see the

684
00:21:32,520 --> 00:21:34,880
like the the sort of frontier of the

685
00:21:34,880 --> 00:21:36,700
ability to train rl agents uh

686
00:21:36,700 --> 00:21:39,220
sort of pushed um and if we can

687
00:21:39,220 --> 00:21:40,980
do that in a way that also sort

688
00:21:40,980 --> 00:21:42,560
of is just as efficient as a standard

689
00:21:42,560 --> 00:21:44,740
uh you know networks that would be very

690
00:21:44,740 --> 00:21:47,080
cool so you know like yeah because training

691
00:21:47,080 --> 00:21:48,220
uh doesn't have

692
00:21:48,220 --> 00:21:49,900
to be the same thing that you deploy

693
00:21:49,900 --> 00:21:51,460
at inference right you know what i mean

694
00:21:51,460 --> 00:21:53,400
like yeah so yeah so

695
00:21:53,400 --> 00:21:54,720
if there's ways to like distill down to

696
00:21:54,720 --> 00:21:56,140
a smaller model or prune the model and

697
00:21:56,140 --> 00:21:57,280
maybe not and still

698
00:21:57,280 --> 00:21:59,740
retain performance that's a very interesting research structure

699
00:21:59,740 --> 00:22:00,740
that we choose but let's talk about

700
00:22:00,740 --> 00:22:03,260
other uh future directions what what else is

701
00:22:03,260 --> 00:22:07,460
your personal passions yeah so uh currently i'm

702
00:22:07,460 --> 00:22:07,860
pursuing

703
00:22:07,860 --> 00:22:11,080
the direction of uh stitching in reinforcement learning

704
00:22:11,080 --> 00:22:13,480
so we are trying to generalize

705
00:22:14,400 --> 00:22:18,200
reinforcement learning from shorter sub behaviors so that

706
00:22:18,200 --> 00:22:21,240
they are stitched merged during the test time

707
00:22:21,240 --> 00:22:23,860
and uh yeah i think this is one

708
00:22:23,860 --> 00:22:26,100
of my uh last papers that i will

709
00:22:26,100 --> 00:22:28,360
tackle during the phd personally i

710
00:22:28,360 --> 00:22:30,760
would i'm very curious of like can we

711
00:22:30,760 --> 00:22:33,500
like what's the like real like can we

712
00:22:33,500 --> 00:22:35,240
push i'm i'm curious about

713
00:22:35,240 --> 00:22:37,480
like advancing the frontier as much as possible

714
00:22:37,480 --> 00:22:38,860
um so if you actually look at our

715
00:22:38,860 --> 00:22:39,940
paper we focus on

716
00:22:39,940 --> 00:22:42,140
scaling depth but we notice that we see

717
00:22:42,140 --> 00:22:44,100
that scaling width actually also improves performance

718
00:22:44,100 --> 00:22:46,360
and we also find that actually by scaling

719
00:22:46,360 --> 00:22:48,620
depth we actually unlock the ability to scale

720
00:22:48,620 --> 00:22:49,100
along batch

721
00:22:49,100 --> 00:22:51,280
size as well um so this is uh

722
00:22:51,280 --> 00:22:53,420
one of yeah so okay it's like a

723
00:22:53,420 --> 00:22:55,760
collinear like yeah right so like

724
00:22:55,760 --> 00:22:57,440
okay i guess for context like in traditional

725
00:22:57,440 --> 00:22:59,700
rl like value-based rl scaling batch size

726
00:22:59,700 --> 00:23:00,180
is not super

727
00:23:00,180 --> 00:23:02,840
effective but there's we also can see there's

728
00:23:02,840 --> 00:23:04,100
also other work in other areas of deep

729
00:23:04,100 --> 00:23:04,520
learning that

730
00:23:04,520 --> 00:23:06,820
show that scaling batch size is only most

731
00:23:06,820 --> 00:23:08,620
effective when there's like a large enough

732
00:23:08,620 --> 00:23:10,800
network capacity to take advantage of the scaled

733
00:23:10,800 --> 00:23:13,380
batch size and we actually find that you

734
00:23:13,380 --> 00:23:13,480
know

735
00:23:13,480 --> 00:23:15,420
perhaps so one hypothesis and i'd be like

736
00:23:15,420 --> 00:23:17,700
perhaps the reason why scale batches isn't that

737
00:23:17,700 --> 00:23:18,220
effective

738
00:23:18,220 --> 00:23:20,140
in traditional rls because like we've been using

739
00:23:20,140 --> 00:23:21,540
these tiny networks that haven't been able to

740
00:23:21,540 --> 00:23:23,620
capture that and one of our experiments is

741
00:23:23,620 --> 00:23:26,840
that like because we are enabled successful training

742
00:23:26,840 --> 00:23:26,960
of

743
00:23:26,960 --> 00:23:29,140
deep network we actually were able to this

744
00:23:29,140 --> 00:23:30,760
is a great test bed for you know

745
00:23:30,760 --> 00:23:31,620
like testing this

746
00:23:31,620 --> 00:23:33,800
hypothesis and we find that indeed as we

747
00:23:33,800 --> 00:23:36,260
scale to network capacity we also unlock this

748
00:23:36,260 --> 00:23:36,480
different

749
00:23:36,480 --> 00:23:38,960
dimension of scaling batch size and so all

750
00:23:38,960 --> 00:23:40,740
that to say is that i'm very curious

751
00:23:40,740 --> 00:23:42,700
for someone like

752
00:23:42,700 --> 00:23:45,180
with enough compute to like take some of

753
00:23:45,180 --> 00:23:47,480
these environments scale up batch uh scale up

754
00:23:47,480 --> 00:23:48,040
depth to

755
00:23:48,040 --> 00:23:50,400
the maximum capability also scale along with also

756
00:23:50,400 --> 00:23:52,960
scale along batch size and let's like basically

757
00:23:52,960 --> 00:23:53,300
like

758
00:23:53,300 --> 00:23:55,080
in the same way that in language we're

759
00:23:55,080 --> 00:23:57,420
scaling on so many different axes can we

760
00:23:57,420 --> 00:23:57,900
unlock different

761
00:23:57,900 --> 00:24:00,020
dimensions of scaling as well and what capabilities

762
00:24:00,020 --> 00:24:01,620
and how far can we push the frontier

763
00:24:01,620 --> 00:24:02,440
of training

764
00:24:02,440 --> 00:24:04,320
these rl agents from doing them before we

765
00:24:04,320 --> 00:24:05,880
pass it sean uh when you say enough

766
00:24:05,880 --> 00:24:07,060
compute what kind

767
00:24:07,060 --> 00:24:09,140
of compute budget did you have how does

768
00:24:09,140 --> 00:24:11,020
it how i just want to see what

769
00:24:11,020 --> 00:24:11,960
you guys got good question

770
00:24:11,960 --> 00:24:13,320
so we we wanted to make sure that

771
00:24:13,320 --> 00:24:15,380
this is we we wanted to make it

772
00:24:15,380 --> 00:24:16,720
such that like uh you know it's

773
00:24:16,720 --> 00:24:19,120
quite accessible so i can the nice thing

774
00:24:19,120 --> 00:24:20,800
is that all of our experiments even a

775
00:24:20,800 --> 00:24:21,200
thousand layer

776
00:24:21,200 --> 00:24:23,340
networks can be run on one single 80

777
00:24:23,340 --> 00:24:27,160
gigabyte h100 gpu um so that's those dollars

778
00:24:27,160 --> 00:24:28,340
yeah right right

779
00:24:28,340 --> 00:24:29,960
right so everything can be run on one

780
00:24:29,960 --> 00:24:32,100
gpu um but in theory if we had

781
00:24:32,100 --> 00:24:33,080
you know like a distributed

782
00:24:33,080 --> 00:24:35,500
training setup and like can just like blast

783
00:24:35,500 --> 00:24:37,260
compute through this and really wanted to push

784
00:24:37,260 --> 00:24:37,680
the frontier

785
00:24:37,680 --> 00:24:39,600
it'd be very interesting to see how things

786
00:24:39,600 --> 00:24:42,160
go yeah cool and i've actively been trying

787
00:24:42,160 --> 00:24:42,480
to learn as

788
00:24:42,480 --> 00:24:43,880
much as i can about vision language action

789
00:24:43,880 --> 00:24:47,060
models uh role models at europe's and going

790
00:24:47,060 --> 00:24:47,440
to a lot of

791
00:24:47,440 --> 00:24:50,200
machine language action models vision language vision language

792
00:24:50,200 --> 00:24:53,200
yeah um and yeah curious about

793
00:24:53,200 --> 00:24:56,300
applications of representation for these yeah exactly for

794
00:24:56,300 --> 00:24:58,040
robotics um actively trying to explore

795
00:24:58,040 --> 00:25:00,300
more in that area so just reading a

796
00:25:00,300 --> 00:25:01,820
lot of literature talking to as many people

797
00:25:01,820 --> 00:25:04,100
yeah we just released our

798
00:25:04,100 --> 00:25:07,480
episode with uh general intuition oh okay um

799
00:25:07,480 --> 00:25:09,480
awesome where if you know a bit about

800
00:25:09,480 --> 00:25:10,180
their history they

801
00:25:10,180 --> 00:25:13,160
started as a gaming clipping company and uh

802
00:25:13,160 --> 00:25:15,060
they basically have a vision language action model

803
00:25:15,060 --> 00:25:16,120
yeah which

804
00:25:16,120 --> 00:25:17,680
um i i saw i saw i saw

805
00:25:17,680 --> 00:25:20,120
a preview it was very impressive i'm not

806
00:25:20,120 --> 00:25:22,480
sure exactly how transferable it is to

807
00:25:22,480 --> 00:25:25,340
embodied use cases but it doesn't have to

808
00:25:25,340 --> 00:25:29,340
like screen is fine you know like yeah

809
00:25:29,340 --> 00:25:30,300
i i don't know if you

810
00:25:30,300 --> 00:25:32,140
have any takes on yeah that's an exciting

811
00:25:32,140 --> 00:25:35,140
research direction definitely yeah i i think um

812
00:25:35,140 --> 00:25:37,580
the the the the

813
00:25:37,580 --> 00:25:39,740
the concepts of actions as as something that

814
00:25:39,740 --> 00:25:44,940
you are outputting is actually not that popular

815
00:25:44,940 --> 00:25:45,640
in industry

816
00:25:45,640 --> 00:25:49,580
right right only because text has completely dominated

817
00:25:49,580 --> 00:25:51,880
the last three years and tool calling

818
00:25:51,880 --> 00:25:54,320
and which is a just another form of

819
00:25:54,320 --> 00:25:56,860
structured text uh and i i feel like

820
00:25:56,860 --> 00:25:59,100
the uh action research is is

821
00:25:59,100 --> 00:26:00,920
kind of like i don't know how i

822
00:26:00,920 --> 00:26:02,280
don't know what needs to happen in order

823
00:26:02,280 --> 00:26:05,000
to unlock the next phase in

824
00:26:05,000 --> 00:26:06,420
that i don't know if you i think

825
00:26:06,420 --> 00:26:08,860
anything interesting out here shut it out yeah

826
00:26:08,860 --> 00:26:09,940
there's a lot of cool work

827
00:26:09,940 --> 00:26:13,560
on like leveraging pre-trained vlms and you

828
00:26:13,560 --> 00:26:14,920
freeze it and then you apply it oh

829
00:26:14,920 --> 00:26:15,720
yeah and then you're

830
00:26:15,720 --> 00:26:17,260
recording on top of that like sort of

831
00:26:17,260 --> 00:26:20,960
experts to output actions um also like systems

832
00:26:20,960 --> 00:26:22,400
for doing like

833
00:26:22,400 --> 00:26:25,280
hierarchical planning maybe outputting some higher level plan

834
00:26:25,280 --> 00:26:26,860
that and this is like a larger network

835
00:26:26,860 --> 00:26:29,260
that takes a long time to a little

836
00:26:29,260 --> 00:26:30,840
longer to do inference and so it outputs

837
00:26:30,840 --> 00:26:31,980
its plans with less

838
00:26:31,980 --> 00:26:34,080
frequency like some sort of chunk and then

839
00:26:34,080 --> 00:26:36,080
from there there's like some sort of uh

840
00:26:36,080 --> 00:26:37,060
second system that

841
00:26:37,060 --> 00:26:38,360
operates a bit more fast i think there's

842
00:26:38,360 --> 00:26:40,080
quite a bit of interesting research in that

843
00:26:40,080 --> 00:26:40,660
direction so

844
00:26:40,660 --> 00:26:43,860
that's what i'm looking forward to cool final

845
00:26:43,860 --> 00:26:46,560
question uh hardest question you were asked at

846
00:26:46,560 --> 00:26:48,980
the poster session or just favorite encounter anyone

847
00:26:48,980 --> 00:26:51,540
famous that you met so i actually haven't

848
00:26:51,540 --> 00:26:51,900
gotten a

849
00:26:51,900 --> 00:26:53,320
chance to go to the conference that much

850
00:26:53,320 --> 00:26:55,100
i'm actually working full-time now so oh

851
00:26:55,100 --> 00:26:57,580
damn yeah uh so

852
00:26:57,580 --> 00:26:59,620
so far i i actually literally just got

853
00:26:59,620 --> 00:27:02,040
my badge like a few moments before session

854
00:27:02,040 --> 00:27:03,280
so i guess i wouldn't

855
00:27:03,280 --> 00:27:05,100
be the best to answer that question no

856
00:27:05,100 --> 00:27:06,840
no like you see you like people ask

857
00:27:06,840 --> 00:27:08,000
you stuff right oh that

858
00:27:08,000 --> 00:27:10,360
might i might close people asking you or

859
00:27:10,360 --> 00:27:13,020
meeting you and like you know just just

860
00:27:13,020 --> 00:27:13,760
give a vibe of

861
00:27:13,760 --> 00:27:16,220
like what people are saying and yeah people

862
00:27:16,220 --> 00:27:18,920
were very i think it's sort of like

863
00:27:18,920 --> 00:27:20,180
a very eye-opening

864
00:27:20,180 --> 00:27:21,740
i think that the general question is that

865
00:27:21,740 --> 00:27:23,660
people thought is a very eye-opening paper

866
00:27:23,660 --> 00:27:24,220
because like

867
00:27:24,220 --> 00:27:26,820
the objective is quite simple it's quite elegant

868
00:27:26,820 --> 00:27:29,180
and for us to be able to like

869
00:27:29,180 --> 00:27:30,880
you know like i don't

870
00:27:30,880 --> 00:27:32,540
want to say overturn but like sort of

871
00:27:32,540 --> 00:27:35,160
challenge the conventional wisdom that like rl is

872
00:27:35,160 --> 00:27:35,540
not super

873
00:27:35,540 --> 00:27:38,280
scalable and push it to such limits like

874
00:27:38,280 --> 00:27:41,140
a thousand layers d and see continuing improve

875
00:27:41,140 --> 00:27:41,880
performance i

876
00:27:41,880 --> 00:27:44,280
think the general impression that i've gotten is

877
00:27:44,280 --> 00:27:47,320
that you know this this could be like

878
00:27:47,320 --> 00:27:47,600
a really

879
00:27:47,600 --> 00:27:49,780
cool like if we can sort of build

880
00:27:49,780 --> 00:27:52,180
along this direction and that like we can

881
00:27:52,180 --> 00:27:52,800
really scale along

882
00:27:52,800 --> 00:27:54,520
all these different dimensions and push the frontier

883
00:27:54,520 --> 00:27:56,560
of the ability for rl i'm very curious

884
00:27:56,560 --> 00:27:57,060
to see how that

885
00:27:57,060 --> 00:27:58,600
goes all right well thank you so much

886
00:27:58,600 --> 00:28:00,620
for dropping by uh congrats on the paper

887
00:28:00,620 --> 00:28:00,840
again

888
00:28:01,380 --> 00:28:03,060
and uh good luck in your future work

889
00:28:03,060 --> 00:28:04,360
thank you thanks for having us yeah

