1
00:00:00,000 --> 00:00:02,720
다리오 아모데이는 앤스로픽의 CEO입니다. 그는

2
00:00:02,720 --> 00:00:04,640
또한 제 생각에는 가장 목소리를 크게 내는

3
00:00:04,640 --> 00:00:07,240
AI의 좋은 점과 나쁜 점, 그리고

4
00:00:07,240 --> 00:00:10,000
AI가 가져올 수 있는 추악한 면과 파괴에 대해 진실을 말하는 사람 중 한 명입니다.

5
00:00:10,380 --> 00:00:11,700
우리는 즉흥적으로 기회를 얻어

6
00:00:11,700 --> 00:00:14,620
다리오와 이야기할 수 있었습니다. 그의 메모가 공개돼 있었습니다.

7
00:00:14,720 --> 00:00:15,900
우리는 조금 더

8
00:00:15,900 --> 00:00:18,380
깊이 이야기해 보고 싶다고 말했습니다. 다리오가 말하는 방식은,

9
00:00:18,500 --> 00:00:21,940
다섯 단어로 말하면 인류는 깨어나야 합니다.

10
00:00:24,280 --> 00:00:26,100
작년에 그가 그런 경고를 했을 때,

11
00:00:26,100 --> 00:00:28,620
화이트칼라 일자리의 50%가

12
00:00:28,620 --> 00:00:30,940
AI 때문에 몇 년 안에 쓸모없어질 수 있다고 말했을 때,

13
00:00:31,280 --> 00:00:33,720
그는 전국적인 대화를 촉발했습니다. 이번 메모에서,

14
00:00:33,880 --> 00:00:35,340
그는 다른 경로를 택하고 있습니다. 그는

15
00:00:35,340 --> 00:00:37,720
말하려고 합니다. 이것은 일자리만의 문제가 아니라고요. 이것은

16
00:00:37,720 --> 00:00:40,020
국가 안보일 수도 있습니다. 이것은 여러분의

17
00:00:40,020 --> 00:00:42,020
삶의 방식일 수도 있습니다. 그리고 그것은

18
00:00:42,020 --> 00:00:43,720
도발적으로 쓰이도록 작성됐습니다. 그것은 도발적입니다.

19
00:00:43,720 --> 00:00:47,540
권위주의 정부들과 함께 그가 가장 크게 경고한 것 중 하나는

20
00:00:47,540 --> 00:00:51,360
AI 기업들이었습니다. 그는 AI 기업의 수장으로서 이런 말을 하기가 어색하다고 했습니다.

21
00:00:51,360 --> 00:00:52,680
그가 말했습니다. 저는 이런 말을 하는 것이

22
00:00:52,680 --> 00:00:55,640
AI 기업의 대표로서 어색하지만, 모든

23
00:00:55,640 --> 00:00:58,400
사용자 수를 보십시오. 그리고

24
00:00:58,400 --> 00:01:00,460
그들이 가진 모든 데이터센터를 보십시오. 그리고

25
00:01:00,460 --> 00:01:03,900
그들이 가진 모든 권력을 보십시오. 그리고 만약

26
00:01:03,900 --> 00:01:08,340
그들이 이 거대한 소비자 사용자 기반을 세뇌한다면

27
00:01:08,340 --> 00:01:09,800
어떻게 되겠습니까? 그것은 제게 새로웠습니다, 짐.

28
00:01:09,800 --> 00:01:11,640
사람들이 응답해야 할 경종입니다.

29
00:01:11,640 --> 00:01:13,880
사람들은 답해야 합니다. 사람들은 들어야 합니다. 여러분은

30
00:01:13,880 --> 00:01:15,700
회의적일 수도 있습니다. 두려울 수도 있습니다. 여러분은

31
00:01:15,700 --> 00:01:19,160
열광적일 수도 있습니다. 다리오가 말하는 것은

32
00:01:19,160 --> 00:01:23,020
중요합니다. 그리고 덧붙이자면, 그것은 도발적으로 종합하지만,

33
00:01:23,020 --> 00:01:25,340
제 생각에는 정확하게도, 우리가

34
00:01:25,340 --> 00:01:28,920
다른 사람들과의 대화 또 대화에서 듣는 것을 종합합니다. 이것은

35
00:01:28,920 --> 00:01:31,080
그가 그저 히스테릭해하는 것이 아닙니다. 저는 이것이

36
00:01:31,080 --> 00:01:33,580
그가 그저 기술을 과장하는 것만은 아니라고 생각합니다. 제가 보기에는

37
00:01:33,580 --> 00:01:36,220
그것은 그가 우리가 거듭해서 듣는 우려를 반영하기 때문입니다.

38
00:01:36,220 --> 00:01:38,080
적어도 비공개

39
00:01:38,080 --> 00:01:39,000
대화에서,

40
00:01:39,000 --> 00:01:41,360
이 기술들을 만들고 있는 사람들과

41
00:01:41,360 --> 00:01:43,180
이를 사용하는 사람들로부터요. 그리고 우리는

42
00:01:43,180 --> 00:01:45,900
메모를 넘어가고 싶었습니다. 우리는 특히

43
00:01:45,900 --> 00:01:49,300
만약 가장 직설적인 형태로 전달된다면 어떤 메시지를

44
00:01:49,300 --> 00:01:52,340
다리오가 전달하고 싶어 하는지에 대해 이야기하고 싶었습니다.

45
00:01:52,340 --> 00:01:54,620
의회 의원들과

46
00:01:54,620 --> 00:01:55,460
연방 정부 구성원들에게요?

47
00:02:05,000 --> 00:02:08,640
지금 의회가 했으면 하는 세 가지는 무엇입니까?

48
00:02:08,640 --> 00:02:11,940
그리고 또, 정말로 무엇이 벌어지고 있는지에 능통하고

49
00:02:11,940 --> 00:02:14,360
완전히 솔직하게 말한다면, 그들이 유권자들에게

50
00:02:14,360 --> 00:02:16,680
무엇을 말하길 바라십니까?

51
00:02:16,680 --> 00:02:19,220
상황을 정확히 이해한 채로 말입니다.

52
00:02:19,220 --> 00:02:22,020
네. 그러면 지금 해야 할 일 세 가지를 말씀드리겠습니다.

53
00:02:22,020 --> 00:02:27,200
첫째는 가능한 한 강력한 투명성 관련 입법입니다.

54
00:02:27,200 --> 00:02:30,180
어떤 테스트를 했습니까?

55
00:02:30,340 --> 00:02:32,100
당신들의 모델과 관련해서는 무엇을 보고 있습니까?

56
00:02:32,100 --> 00:02:34,660
기업들은 이런 것들을 연구할 역량이

57
00:02:34,660 --> 00:02:36,600
있고, 실제로 자주 합니다. 그래서

58
00:02:36,600 --> 00:02:38,860
그들이 이런 위험을 연구하도록 요구할 뿐 아니라

59
00:02:38,860 --> 00:02:41,640
그 위험을 대중에게 보여주도록 요구하는 것입니다.

60
00:02:42,060 --> 00:02:43,280
제품에 라벨을 붙이도록 하는 것입니다.

61
00:02:43,880 --> 00:02:45,880
그것은 소비자에게 정말로 도움이 된다고 생각합니다.

62
00:02:46,080 --> 00:02:48,920
또한 이는 기업들이 서로에게서 배우게 해 준다는 점에서도 도움이 됩니다.

63
00:02:48,920 --> 00:02:51,160
각 기업이

64
00:02:51,160 --> 00:02:53,260
각자 이런 것들을 연구하고

65
00:02:53,260 --> 00:02:55,100
경쟁 때문에 자신들이 발견한 것을

66
00:02:55,100 --> 00:02:58,700
다른 이들에게 보여주길 두려워한다면,

67
00:02:58,700 --> 00:03:00,520
우리는 과학 공동체로서

68
00:03:00,520 --> 00:03:02,200
이런 것들을 배울 수 없습니다.

69
00:03:02,200 --> 00:03:06,080
둘째는, 제가 여러 번 말해 왔지만,

70
00:03:06,080 --> 00:03:08,580
미국 내 기업들 사이에서도

71
00:03:08,580 --> 00:03:12,840
이 미친 상업적 경쟁을 다루는 것만으로도 충분히 어렵습니다. 하지만 이론적으로는

72
00:03:12,840 --> 00:03:15,480
방금 제가 설명한 것 같은 법을 통과시켜

73
00:03:15,480 --> 00:03:17,180
기업들을 견제하는 데 도움이 되게 할 수 있습니다.

74
00:03:17,180 --> 00:03:20,280
하지만 권위주의적 적대국이 밖에서

75
00:03:20,280 --> 00:03:23,400
우리만큼이나 빠르게

76
00:03:23,400 --> 00:03:27,320
기술을 만들고 있다면, 그런 일은 거의 불가능합니다.

77
00:03:27,320 --> 00:03:30,220
그렇지 않습니까? 이것은 끔찍한 딜레마를 만듭니다. 그리고

78
00:03:30,220 --> 00:03:31,660
저는 우리가 공급망을

79
00:03:31,660 --> 00:03:34,400
차단해야 한다고 생각합니다. 우리는 칩에서 그들보다 수년 앞서 있습니다.

80
00:03:34,400 --> 00:03:36,900
우리는 정말로 할 수 있습니다. 우리는 정말로 공급망을

81
00:03:36,900 --> 00:03:37,980
차단할 수 있습니다.

82
00:03:37,980 --> 00:03:39,980
그리고 그것은 우리에게 시간과

83
00:03:39,980 --> 00:03:42,960
완충을 제공해 이런 위험들을 제대로 다룰 수 있게 해 줍니다. 그리고

84
00:03:42,960 --> 00:03:45,360
셋째로, 저는 우리가 이 기술의 혜택

85
00:03:45,360 --> 00:03:51,940
분배에 대해 생각해야 한다고 봅니다.

86
00:03:51,940 --> 00:03:56,300
저는 AI가 엄청난 경제 성장을 만들어내는 세상을 만들 것이라고 봅니다.

87
00:03:56,300 --> 00:03:59,800
암을 치료할 수 있습니다.

88
00:03:59,800 --> 00:04:03,480
더 저렴한 에너지를 개발할 수 있습니다.

89
00:04:03,660 --> 00:04:06,580
엄청나게 새로운 재료를 개발할 수도 있습니다. 그리고 그런 것들이

90
00:04:06,580 --> 00:04:10,120
경제를 엄청나게 성장시킬 것입니다. 하지만 바로

91
00:04:10,120 --> 00:04:13,400
AI가, 그러니까,

92
00:04:13,500 --> 00:04:17,020
현재 많은 화이트칼라 근로자들이 하는 일들을 수행하기 때문에,

93
00:04:17,140 --> 00:04:19,300
노동에서 자본으로 부가 이전되면서

94
00:04:19,300 --> 00:04:21,580
이 부가 어느 정도 집중될 것입니다.

95
00:04:21,940 --> 00:04:23,380
그래서 우리는 이전에는 본 적이 거의 없는

96
00:04:23,380 --> 00:04:26,140
이상한 세상을 맞게 될 것입니다. 즉,

97
00:04:26,140 --> 00:04:28,640
엄청난 부가 있지만, 분배가

98
00:04:28,640 --> 00:04:31,160
문제인 세상입니다. 그것은 다른 세상입니다. 저는

99
00:04:31,160 --> 00:04:32,900
그것이 이념적인 문제라고 생각하지 않지만,

100
00:04:32,900 --> 00:04:34,940
우리는 그 세상에 적응해야

101
00:04:34,940 --> 00:04:35,280
한다고 생각합니다.

102
00:04:35,840 --> 00:04:37,700
지금 그것에 어떻게 적응하시겠습니까?

103
00:04:37,840 --> 00:04:39,680
분명히 우리에게는 그런 현실이 없습니다. 그것은

104
00:04:39,680 --> 00:04:43,320
오늘날의 현실이 아닙니다. 그래서, 의회는 어떻게

105
00:04:43,320 --> 00:04:45,040
국가가 그에 대비하도록 해서 우리가

106
00:04:45,040 --> 00:04:47,180
잠자다가 소급적으로 대응하지 않도록 하겠습니까?

107
00:04:47,180 --> 00:04:49,340
아마도 가장 명백한 것은,

108
00:04:49,420 --> 00:04:51,480
더 강력한 조세 정책에 대해

109
00:04:51,480 --> 00:04:54,980
생각해야 한다는 것입니다. 그리고,

110
00:04:55,080 --> 00:04:57,560
저는 이것이 과거의

111
00:04:57,560 --> 00:04:59,660
조세 정책이라고 생각하지 않습니다. 이것은

112
00:04:59,660 --> 00:05:02,400
조만장자가 있는 세상을 위한 것입니다. 일론 머스크로

113
00:05:02,400 --> 00:05:03,300
거의 그 수준에 와 있습니다.

114
00:05:03,300 --> 00:05:06,860
그리고 저는 AI의 효과와

115
00:05:06,860 --> 00:05:08,500
AI 기업들의 효과가

116
00:05:08,500 --> 00:05:09,780
그것을 더 극단적으로 만들 것이라고 생각합니다. 그리고,

117
00:05:09,820 --> 00:05:11,700
저는 그것의 혜택을 받는

118
00:05:11,700 --> 00:05:12,920
사람 중 한 명으로서 말하는 것입니다, 그렇지요?

119
00:05:13,660 --> 00:05:17,540
이 문제에 대해 잘 설계된 답을 찾지 못하면

120
00:05:17,540 --> 00:05:20,820
잘못 설계된 답을 얻게 될 수도

121
00:05:20,820 --> 00:05:23,340
있습니다. 그렇지요? 우리는

122
00:05:23,340 --> 00:05:25,900
매우 공격적이고 잘못 설계된 답을 얻게 될 수도 있습니다.

123
00:05:26,060 --> 00:05:27,520
그래서 제 부탁은 이렇습니다.

124
00:05:27,980 --> 00:05:30,180
부의 분배에 편향이 생길 것입니다.

125
00:05:30,180 --> 00:05:32,700
경제적으로 문해력 있고

126
00:05:32,700 --> 00:05:35,760
경제적으로 합리적인 방식으로 이를 다루는 방법은 무엇입니까?

127
00:05:35,760 --> 00:05:38,700
그래서 우리가 이런 미친

128
00:05:38,700 --> 00:05:39,200
즉흥적 반응을 피할 수 있도록요?

129
00:05:39,200 --> 00:05:42,900
다리오, 이것들은 매우 무거운 과제입니다. 제가

130
00:05:42,900 --> 00:05:45,000
대화하는 의회 의원들은 이 문제에 대해

131
00:05:45,000 --> 00:05:48,900
얘기하는 것조차 두려워합니다. 그들의 유권자들은

132
00:05:48,900 --> 00:05:51,060
일자리를 걱정하거나 전기요금에 화가 나 있거나

133
00:05:51,060 --> 00:05:54,180
불쾌하다고 생각하거나

134
00:05:54,180 --> 00:05:55,220
불안해합니다.

135
00:05:55,220 --> 00:05:59,940
펜실베이니아 애비뉴 양끝의 정책 결정자들에게

136
00:05:59,940 --> 00:06:03,840
이 문제들에 대해 이야기할 수 있고 반드시 이야기해야 한다고

137
00:06:03,840 --> 00:06:05,260
파고들어야 한다고 어떻게 설득하시겠습니까?

138
00:06:05,260 --> 00:06:07,880
하루아침에 되지는 않을 것입니다.

139
00:06:07,880 --> 00:06:09,740
하지만 제가 말씀드리고 싶은 것은

140
00:06:09,740 --> 00:06:12,100
AI의 효과를 보면서,

141
00:06:12,200 --> 00:06:15,720
대중이 AI가 이 모든 놀라운 것들을,

142
00:06:15,720 --> 00:06:18,440
이 모든 의료의 경이를,

143
00:06:18,440 --> 00:06:21,060
이 모든 풍요를 가져다준다는 것을 이해할 것으로 기대합니다.

144
00:06:21,120 --> 00:06:23,800
결국에는 모든 것을 할

145
00:06:23,800 --> 00:06:25,920
저렴한 로봇을 갖게 될 것입니다. 하지만 이런 문제들이

146
00:06:25,920 --> 00:06:28,600
등장할 것입니다. 사람들은 '내 일자리는 어디에 있나요?'라고 물을 것입니다.

147
00:06:28,600 --> 00:06:31,200
사람들은 '왜 저 사람은

148
00:06:31,200 --> 00:06:33,760
조만장자이고 제 임금은 내려갔나요?

149
00:06:33,760 --> 00:06:37,300
제 기술이 쓸모없어졌기 때문입니다'라고 물을 것입니다. 사람들은 이런

150
00:06:37,300 --> 00:06:40,880
질문을 합니다. 그리고 저는 미리 앞서 나가서

151
00:06:40,880 --> 00:06:42,440
지금부터 생각하기 시작하는 것이

152
00:06:42,440 --> 00:06:43,040
더 낫다고 생각합니다.

153
00:06:43,120 --> 00:06:44,100
그리고 덧붙이자면, 이것이 당파적

154
00:06:44,100 --> 00:06:46,020
문제가 될 것이라고 생각하지 않습니다. 지금도 당파적이지

155
00:06:46,020 --> 00:06:49,640
않습니다. 제가 대화한

156
00:06:49,640 --> 00:06:53,420
정치적 스펙트럼의 양 극단에 있는 사람들도,

157
00:06:53,560 --> 00:06:56,320
그들이 하는 말이 얼마나 비슷한지

158
00:06:56,320 --> 00:06:56,780
놀랍습니다.

159
00:06:56,780 --> 00:06:59,000
동료 미래 조만장자들 중

160
00:06:59,000 --> 00:07:04,700
이것을 지지하고, 논의하거나,

161
00:07:04,700 --> 00:07:06,340
아니면 그들이 싸울 것이라고 생각하십니까?

162
00:07:07,280 --> 00:07:10,100
다른 사람이 무엇을 할지는

163
00:07:10,100 --> 00:07:11,740
말할 수 없습니다. 그렇지요?

164
00:07:11,820 --> 00:07:13,080
저는...

165
00:07:13,080 --> 00:07:13,140
하지만 당신은 그들을 아시잖아요...

166
00:07:13,140 --> 00:07:17,660
미래의 동료 조만장자들을 아시잖아요. 당신의

167
00:07:17,660 --> 00:07:20,300
생각에 그들을 동참시키려면 무엇을

168
00:07:20,300 --> 00:07:21,640
하실 수 있습니까? 말씀드리자면,

169
00:07:21,640 --> 00:07:22,580
그들 중 많은 수가 지금은 거기에 있지 않습니다.

170
00:07:23,000 --> 00:07:25,560
네, 네. 많은 분들이 지금은 거기에

171
00:07:25,560 --> 00:07:26,700
있지 않다는 데 동의합니다. 제 말은,

172
00:07:26,700 --> 00:07:29,160
다양한 견해가 있습니다. 그리고 다시 말하지만, 저는

173
00:07:29,160 --> 00:07:31,000
다른 누구도 대변할 수 없습니다. 하지만 저는

174
00:07:31,000 --> 00:07:32,780
전에 말한 것을 다시 말씀드리겠습니다.

175
00:07:33,420 --> 00:07:36,820
그냥 돌아다니면서 '좋아요,

176
00:07:37,360 --> 00:07:40,500
우리가 이 모든 풍요를

177
00:07:40,500 --> 00:07:43,040
창출할 거예요. 그 많은 부분이

178
00:07:43,040 --> 00:07:45,540
우리에게 갈 거예요. 그리고

179
00:07:45,540 --> 00:07:47,760
우리가 조만장자가 될 거예요. 그리고 아무도

180
00:07:47,760 --> 00:07:49,480
그것에 대해 불평하지 않을 거예요'라고 말할 수는 없습니다.

181
00:07:49,480 --> 00:07:50,840
아무도 어떤 시도도 하지 않을 것이다,

182
00:07:51,260 --> 00:07:53,940
그렇지요? 만약 당신의 답이 그냥

183
00:07:53,940 --> 00:07:56,580
'알 바 아니고, 우리가 할 수 있거나 해야 할

184
00:07:56,580 --> 00:07:58,960
일은 없다'라면, 그것은

185
00:07:58,960 --> 00:08:00,040
많은 불만을 만들 것입니다.

186
00:08:00,040 --> 00:08:02,420
이미 그렇습니다. 우리는 이미 그

187
00:08:02,420 --> 00:08:04,460
시작을 보고 있습니다. 그리고 그것은 더 나빠질

188
00:08:04,460 --> 00:08:06,260
것입니다. 그래서 제 견해는 이것이 옳은 일이기 때문에

189
00:08:06,260 --> 00:08:07,480
해야 한다는 것입니다.

190
00:08:07,480 --> 00:08:07,900
옳은 일이니까요.

191
00:08:08,000 --> 00:08:09,240
하지만 다른 사람들과 이야기한다면,

192
00:08:09,400 --> 00:08:10,740
그것이 그들에게 설득력이 없다면, 그리고 저는

193
00:08:10,740 --> 00:08:12,220
설득력이 있기를 바라지만, 그것이 그들에게 설득력이

194
00:08:12,220 --> 00:08:14,900
없다면, 저는 '보세요,

195
00:08:14,900 --> 00:08:16,780
이것을 올바른 방식으로 하지 않으면

196
00:08:16,780 --> 00:08:19,480
폭도들이 당신에게

197
00:08:19,480 --> 00:08:19,680
몰려올 것입니다'라고 말할 것입니다.

198
00:08:19,680 --> 00:08:20,760
잘못된 방식으로 하지 않고 올바른 방식으로 하지 않으면,

199
00:08:20,760 --> 00:08:22,080
매우 잘못된 방식으로

200
00:08:22,080 --> 00:08:23,420
일어나게 될 것입니다.

201
00:08:23,920 --> 00:08:25,980
의회 의원들은 AI의 상태와 향후 1년간

202
00:08:25,980 --> 00:08:28,320
우리가 어디로 향하고 있는지에 대해

203
00:08:28,320 --> 00:08:29,640
유권자들에게 무엇을 말해야 합니까?

204
00:08:30,140 --> 00:08:32,940
AI에서 흥미로운 상황이 있습니다.

205
00:08:32,940 --> 00:08:36,780
사람들이 그것에 대해 걱정하고 있습니다.

206
00:08:36,980 --> 00:08:39,460
대체로 그 우려는

207
00:08:39,460 --> 00:08:42,580
정당합니다. 하지만 그것이 모두

208
00:08:42,580 --> 00:08:43,780
정확히 겨냥되어 있는지는 모르겠습니다.

209
00:08:43,780 --> 00:08:46,320
예를 들어 AI의 물 사용량 같은

210
00:08:46,320 --> 00:08:49,260
위험이 있는데,

211
00:08:49,260 --> 00:08:51,420
살펴보면 AI는 실제로

212
00:08:51,420 --> 00:08:52,340
그렇게 많은 물을 사용하지 않습니다.

213
00:08:52,480 --> 00:08:53,940
AI에는 많은 문제가 있지만 그것은

214
00:08:53,940 --> 00:08:55,940
그중 하나가 아닙니다. 그리고 물론,

215
00:08:56,000 --> 00:08:57,700
사람들은 전기요금을 걱정하는데, 저는 그것이

216
00:08:57,700 --> 00:09:00,220
이해할 만하고 어느 정도는 꽤

217
00:09:00,220 --> 00:09:00,700
핵심을 잘 찌른다고 생각합니다.

218
00:09:00,740 --> 00:09:03,800
하지만, 아시다시피 장기적으로 보면 저는

219
00:09:03,800 --> 00:09:06,560
전기요금 문제가 아닙니다. 핵심은

220
00:09:06,560 --> 00:09:09,200
막대한 풍요로움과 사람들이 그

221
00:09:09,200 --> 00:09:09,620
풍요로움에서 자신의 몫을 가져가느냐입니다.

222
00:09:09,620 --> 00:09:13,840
전기요금은 어쩌면 아주 작은

223
00:09:13,840 --> 00:09:17,260
부분에 불과합니다. 그래서 저는

224
00:09:17,260 --> 00:09:21,460
유권자들이 걱정하긴 하지만, 그들에게

225
00:09:21,460 --> 00:09:24,300
상황이 어디로 가고 있는지 교육하고,

226
00:09:24,620 --> 00:09:26,040
함께 따라오도록 돕는 것이 도움이 된다고 말씀드리겠습니다.

227
00:09:26,040 --> 00:09:28,220
왜냐하면, 다시 말씀드리지만 저는 늘 같은 말을 합니다.

228
00:09:28,360 --> 00:09:30,640
리더십을 발휘하지 않고, '이것이

229
00:09:30,640 --> 00:09:34,300
앞으로의 방향입니다'라고 말하지 않으면, 우리는

230
00:09:34,300 --> 00:09:37,340
해결책을 열심히 찾고 있습니다,

231
00:09:37,340 --> 00:09:38,560
비록 아직 모든

232
00:09:38,560 --> 00:09:41,180
답을 갖고 있지는 않더라도, '우리가 여러분을 지켜드리겠습니다,

233
00:09:41,180 --> 00:09:42,660
해결책을 찾으려고 노력하고 있습니다'라고 말해야 합니다.

234
00:09:42,740 --> 00:09:45,440
그렇게 하는 편이

235
00:09:45,440 --> 00:09:48,280
'걱정할 게 없습니다'라고 말하거나

236
00:09:48,280 --> 00:09:50,180
그저 '우리는 이런

237
00:09:50,180 --> 00:09:53,080
아주 제한적인 문제들만 보고 있다'고 하는 것보다 훨씬 나을 것입니다.

238
00:09:53,080 --> 00:09:55,960
그리고 워싱턴에서는 트럼프 대통령과

239
00:09:55,960 --> 00:09:58,500
데이비드 색스 등 여러 사람들이

240
00:09:58,500 --> 00:10:01,040
AI에 대해 간섭하지 않으려 하고 미국이

241
00:10:01,040 --> 00:10:02,420
중국과의 경쟁에서 이기길 원하기 때문에 그렇다는 전제가 있습니다.

242
00:10:02,720 --> 00:10:05,160
의회는 개입할 의지가 없어 보입니다.

243
00:10:05,720 --> 00:10:08,980
3년을 기다리며 아무것도 하지 않을 때의 위험을

244
00:10:08,980 --> 00:10:10,720
설명해 주시겠습니까? 솔직히 말해

245
00:10:10,720 --> 00:10:12,780
지금으로서는 그것이 가장 가능성 높은 시나리오처럼 보입니다.

246
00:10:12,780 --> 00:10:15,080
네. 그러니까, 제 생각에는,

247
00:10:15,200 --> 00:10:16,500
만약 우리가 3년을

248
00:10:16,500 --> 00:10:20,140
기다리면, 이 기술은 기하급수적으로 발전하고 있지 않습니까?

249
00:10:20,300 --> 00:10:22,640
3년 전인 2023년에는 모델이

250
00:10:22,640 --> 00:10:24,280
아마도 똑똑한 고등

251
00:10:24,280 --> 00:10:24,840
학교 학생 정도의 수준이었습니다.

252
00:10:25,280 --> 00:10:28,180
지금은 앤트로픽에서 모델이

253
00:10:28,180 --> 00:10:30,200
엔지니어를 위해 모든 코드를 작성하고,

254
00:10:30,200 --> 00:10:32,480
엔지니어는 그것을 조금 편집하는 정도인 경우도 있습니다.

255
00:10:32,620 --> 00:10:35,820
하지만 우리는 이제 중간

256
00:10:35,820 --> 00:10:37,920
에서 고급 전문 수준에 아주 가까워졌습니다. 그렇지요.

257
00:10:37,920 --> 00:10:39,540
그런 변화가 고작 3년 만에 일어났습니다.

258
00:10:39,700 --> 00:10:42,780
앞으로 1년, 3년을 더 기다리면 저는

259
00:10:42,780 --> 00:10:44,200
제가 에세이에서

260
00:10:44,200 --> 00:10:46,820
'데이터센터 속의 천재들로 이루어진 우리나라'라고 부르는 것을

261
00:10:46,820 --> 00:10:48,480
아마 3년도 안 되어 보게 될 것이라고 생각합니다.

262
00:10:49,020 --> 00:10:51,860
그러니, 3년은 이

263
00:10:51,860 --> 00:10:53,060
분야에서는 영원에 가깝습니다.

264
00:10:53,220 --> 00:10:56,140
그래서 우리는 반드시

265
00:10:56,140 --> 00:10:57,000
그 전에 행동해야 한다고 생각합니다.

266
00:10:57,000 --> 00:10:59,840
제가 정말 희망을 갖는 지점 하나는

267
00:10:59,840 --> 00:11:02,660
이런 문제들이 나타나기 시작하면

268
00:11:03,220 --> 00:11:05,400
다시 말해, 그것이 당파적 이슈가 되지는 않을 것이라는 점입니다. 그렇지요?

269
00:11:05,580 --> 00:11:08,060
예컨대 처음에는, 어떤

270
00:11:08,060 --> 00:11:10,300
정당이나 한쪽 진영이

271
00:11:10,300 --> 00:11:12,320
반규제 이념을 갖는 데서 출발할 수도 있습니다.

272
00:11:12,340 --> 00:11:14,720
하지만 이런 문제들이 현실이 되면,

273
00:11:14,960 --> 00:11:16,960
모두에게서 대응을 요구하는 목소리가 나오게 될 것입니다.

274
00:11:16,960 --> 00:11:19,700
그리고 메모에서 여러

275
00:11:19,700 --> 00:11:22,480
위험을 정리하셨지요. 생물테러이든, 권위주의

276
00:11:22,480 --> 00:11:26,760
정권이 전복적

277
00:11:26,760 --> 00:11:27,360
행동을 할 수 있는 도구를 너무 많이 갖게 되는 것이든요.

278
00:11:27,560 --> 00:11:28,000
예를 들면 어떤 것들입니까?

279
00:11:28,760 --> 00:11:30,180
그리고 얼마나 걱정하고 계신가요?

280
00:11:30,260 --> 00:11:31,980
말씀하실 정도면 분명

281
00:11:31,980 --> 00:11:34,140
걱정이 크셔서 문제를 제기하시는 것인데,

282
00:11:34,240 --> 00:11:36,740
하지만 대표님께서는

283
00:11:36,740 --> 00:11:38,920
그런 결과가, 특히 우리가 아무것도 하지 않으면

284
00:11:38,920 --> 00:11:40,000
향후 3년 동안 얼마나 일어날 가능성이 있다고 보십니까?

285
00:11:40,140 --> 00:11:41,600
1% 정도입니까, 아니면

286
00:11:41,680 --> 00:11:42,840
아니요, 아니요, 3년 동안

287
00:11:42,840 --> 00:11:44,660
아무것도 하지 않으면 정말 큰일 날 수 있다는 수준입니까?

288
00:11:45,700 --> 00:11:47,400
네, 그것은 언제나 판단하기 어렵습니다.

289
00:11:47,540 --> 00:11:48,640
제가 에세이에서 말한 것 중 하나는

290
00:11:48,640 --> 00:11:52,220
우리는 정말로 모른다는 점입니다.

291
00:11:52,360 --> 00:11:52,420
그렇지요?

292
00:11:52,500 --> 00:11:53,780
나중에 돌아보며 이렇게 말할 수도 있습니다.

293
00:11:53,880 --> 00:11:55,540
'하하, AI 주도 생물테러라니.'

294
00:11:55,660 --> 00:11:58,220
그때는, 그러니까,

295
00:11:58,220 --> 00:11:59,440
그럴듯하게 들리긴 했지만,

296
00:11:59,600 --> 00:12:04,140
결국 실제로는 전혀

297
00:12:04,140 --> 00:12:04,480
일어나지 않았다고요.

298
00:12:04,620 --> 00:12:06,200
그리고 그것은 매우 예측 불가능합니다.

299
00:12:06,480 --> 00:12:07,500
제 표현으로 말하면,

300
00:12:07,500 --> 00:12:11,260
우리는 운영 방식에서

301
00:12:11,260 --> 00:12:14,040
그런 위험들에 대해 편집증적일 정도로 경계하는 자세를 취하고 있습니다.

302
00:12:14,040 --> 00:12:16,700
우리는 잘못될 수 있는

303
00:12:16,700 --> 00:12:18,000
모든 일은 실제로 잘못된다고 항상 가정합니다.

304
00:12:18,160 --> 00:12:19,680
그래야 신뢰할 수 있는 것을 만들 수 있습니다.

305
00:12:19,920 --> 00:12:20,000
그렇지요?

306
00:12:20,080 --> 00:12:21,780
로켓을 만든다고 해서,

307
00:12:22,120 --> 00:12:23,560
'아, 이 부품은 분명 잘 작동하겠지'라고

308
00:12:23,560 --> 00:12:23,780
생각하지는 않습니다.

309
00:12:23,880 --> 00:12:25,420
'이것이 인장

310
00:12:25,420 --> 00:12:25,840
하중을 견디겠지'라고도요.

311
00:12:26,040 --> 00:12:27,460
대신 '아니, 나는

312
00:12:27,460 --> 00:12:29,280
이것과 저것과 또 다른 것들에 대해

313
00:12:29,280 --> 00:12:30,060
시나리오 분석을 하겠다'고 합니다.

314
00:12:30,240 --> 00:12:32,520
그러니까, 저는 어떤 것도

315
00:12:32,520 --> 00:12:33,120
당연하게 여기지 않겠습니다.

316
00:12:33,120 --> 00:12:36,660
그리고, 만약 정부가 나서서

317
00:12:36,660 --> 00:12:39,100
적절한 조치를 취한다면 저는

318
00:12:39,100 --> 00:12:41,440
성공 가능성이 크게 올라간다고 생각합니다.

319
00:12:41,660 --> 00:12:43,240
그렇지 않더라도 우리는 가능한

320
00:12:43,240 --> 00:12:43,980
최선을 다하겠습니다.

321
00:12:44,140 --> 00:12:46,080
하지만 정책결정자들이

322
00:12:46,080 --> 00:12:49,600
운전대를 놓고

323
00:12:49,600 --> 00:12:50,260
잠들어 있지 않다면 많은 일이 더 쉬워질 것입니다.

324
00:12:50,440 --> 00:12:52,040
이제 마무리해야 할 것 같습니다, 다리오님.

325
00:12:52,120 --> 00:12:53,920
시간 내주셔서 감사합니다.

326
00:12:54,540 --> 00:12:55,560
메모가 정말 흥미롭습니다.

327
00:12:55,800 --> 00:12:56,660
선언문도 훌륭합니다.

328
00:12:56,840 --> 00:12:57,300
정말 감사드립니다.

329
00:12:57,300 --> 00:12:58,140
대화해 주셔서 감사합니다.

330
00:12:58,140 --> 00:13:00,080
메모는 얼마나 오래 작업하셨나요?

331
00:13:01,000 --> 00:13:02,560
제가 직접 썼습니다.

332
00:13:02,720 --> 00:13:05,260
첫 초안을 72시간 만에 썼습니다.

333
00:13:05,260 --> 00:13:06,200
겨울 휴가 동안입니다.

334
00:13:07,120 --> 00:13:09,900
솔직히 제 겨울 휴가는

335
00:13:09,900 --> 00:13:11,780
일주일은 그냥 멍하니

336
00:13:11,780 --> 00:13:12,640
비디오게임을 하며 보내고,

337
00:13:12,800 --> 00:13:14,040
그리고 휴가 마지막 3일에는

338
00:13:14,040 --> 00:13:15,240
속으로 '아, 이제는' 하고

339
00:13:15,300 --> 00:13:17,040
뭔가 제대로

340
00:13:17,040 --> 00:13:17,520
해봐야겠다고 생각했습니다.

341
00:13:17,700 --> 00:13:19,120
그래서, 그러니까,

342
00:13:19,580 --> 00:13:22,640
거의 잠도 자지 않고 72시간 동안 썼습니다.

343
00:13:22,980 --> 00:13:24,420
그중 얼마나 많은 부분을 클로드가 했나요?

344
00:13:26,100 --> 00:13:28,460
클로드는 그 어떤 부분도 쓰지 않았습니다.

345
00:13:28,680 --> 00:13:31,140
다만 클로드가 제가

346
00:13:31,140 --> 00:13:32,800
상당한 조사를 하는 데 도움이 되었고,

347
00:13:32,980 --> 00:13:33,900
클로드가 피드백도 주었습니다.

348
00:13:34,080 --> 00:13:35,240
저는 제가 작가였고

349
00:13:35,240 --> 00:13:37,380
클로드는 제 편집자이자

350
00:13:37,380 --> 00:13:38,520
리서치 어시스턴트였다고 말씀드리겠습니다.

351
00:13:39,160 --> 00:13:39,940
마이크를 내려놓겠습니다.

352
00:13:40,140 --> 00:13:40,740
시간 내주셔서 감사합니다.
