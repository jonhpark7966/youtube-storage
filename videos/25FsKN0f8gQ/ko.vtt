WEBVTT

00:00:12.060 --> 00:00:14.280
레인 스페이스에 오신 것을 환영합니다, 저희는 기본적으로

00:00:14.280 --> 00:00:17.360
최대한 최적의 최고의

00:00:17.360 --> 00:00:17.760
팟캐스트

00:00:17.760 --> 00:00:18.720
뉴립스 현장을 위한, 참석하지 못한 분들을 위한

00:00:18.720 --> 00:00:19.680
시청자분들께

00:00:19.680 --> 00:00:22.160
그리고 논문 수상을 축하드립니다, 기분이

00:00:22.160 --> 00:00:22.400
어떠신가요?

00:00:22.400 --> 00:00:25.120
네, 정말 흥분되었습니다, 저희는

00:00:25.120 --> 00:00:26.150
어제 포스터 발표를 했고 오늘은

00:00:26.150 --> 00:00:27.180
또

00:00:27.180 --> 00:00:27.900
구두 발표가 있습니다.

00:00:27.900 --> 00:00:30.060
사람들에게 둘러싸이셨나요? 네, 정말로

00:00:30.060 --> 00:00:31.100
사람이 많았고 거의 세

00:00:31.100 --> 00:00:31.860
시간 내내 계속

00:00:31.860 --> 00:00:33.480
파도처럼 사람들이 찾아와서

00:00:33.480 --> 00:00:35.660
저희가 설명하려고

00:00:35.660 --> 00:00:36.540
그렇게 했습니다, 저는

00:00:36.540 --> 00:00:38.840
베스트 페이퍼는 어떻게 알게 되셨나요, 웹사이트에서

00:00:38.840 --> 00:00:40.860
보신 건가요? 저는 그냥

00:00:40.860 --> 00:00:42.940
어느 날 일어나서

00:00:42.940 --> 00:00:45.400
이메일을 확인했는데, 거기에

00:00:45.400 --> 00:00:47.000
그냥 '아, 그랬구나' 하고

00:00:47.000 --> 00:00:48.040
보게 됐습니다.

00:00:48.040 --> 00:00:49.310
‘베스트

00:00:49.310 --> 00:00:50.580
페이퍼를 수상하셨습니다’라고

00:00:50.580 --> 00:00:53.140
하지만 리뷰에서도

00:00:53.140 --> 00:00:53.500
짐작은 하셨겠죠?

00:00:53.500 --> 00:00:55.700
네, 리뷰를 보면

00:00:55.700 --> 00:00:58.240
잘 됐다는 건 알았지만, 그래도

00:00:58.240 --> 00:00:59.020
그냥 좋은 평가를 받은 것과

00:00:59.020 --> 00:01:01.260
베스트 페이퍼를 받는 것은 다르니

00:01:01.260 --> 00:01:02.290
사실은 몰랐습니다, 네.

00:01:02.290 --> 00:01:03.320
알겠습니다.

00:01:03.320 --> 00:01:04.400
그러면 제가 조금

00:01:04.400 --> 00:01:05.720
건너뛰었는데, 이제

00:01:05.720 --> 00:01:07.480
한 분씩

00:01:07.480 --> 00:01:09.460
자기소개를 해 주시고

00:01:09.460 --> 00:01:10.800
팀에서 무엇을 하셨는지

00:01:10.800 --> 00:01:14.040
말씀해 주시죠, 저는 케빈이고 프린스턴에서 학부를

00:01:14.040 --> 00:01:15.440
다녔고 방금 졸업했습니다.

00:01:15.440 --> 00:01:19.040
네, 제가 프로젝트를 주도했고

00:01:19.040 --> 00:01:21.300
처음 시작했으며, 그리고

00:01:21.300 --> 00:01:21.920
이샨과 니콜, 그리고 벤과 함께 협업하게 되어

00:01:21.920 --> 00:01:24.040
매우 기뻤습니다.

00:01:24.040 --> 00:01:26.100
네, 같은 연구 그룹이었나요, 어떻게

00:01:26.100 --> 00:01:27.080
연결되신 건가요?

00:01:27.080 --> 00:01:29.180
그게 어떤 맥락인가요?

00:01:29.180 --> 00:01:31.680
네, 그래서 저희는 모두

00:01:31.680 --> 00:01:33.800
프린스턴에서 왔습니다, 네.

00:01:33.800 --> 00:01:35.840
그리고 여러분을 섭외해 준 앨런에게 감사드리고요, 이 프로젝트는

00:01:35.840 --> 00:01:36.880
사실 IW

00:01:36.880 --> 00:01:37.920
세미나에서 시작했습니다.

00:01:37.920 --> 00:01:39.340
즉

00:01:39.340 --> 00:01:40.600
벤이 가르치던 독립 연구 세미나였고

00:01:40.600 --> 00:01:41.860
그에서

00:01:41.860 --> 00:01:44.100
이것이 사실 제

00:01:44.100 --> 00:01:45.410
머신러닝 연구의 첫

00:01:45.410 --> 00:01:46.720
경험 중 하나였습니다.

00:01:46.720 --> 00:01:48.520
그래서 그 경험을

00:01:48.520 --> 00:01:49.160
얻는 것이 정말 소중했고

00:01:49.160 --> 00:01:51.700
이샨도 그 세미나에 있었고

00:01:51.700 --> 00:01:52.740
비슷한 주제를 하고 있어서 저희가

00:01:52.740 --> 00:01:53.780
협업했습니다.

00:01:53.780 --> 00:01:56.240
세미나 동안 많이 협업했고, 그리고

00:01:56.240 --> 00:01:58.300
결과적으로 프로젝트에서

00:01:58.300 --> 00:01:59.020
꽤 흥미로운 결과가 나왔습니다.

00:01:59.020 --> 00:02:00.540
그리고 이후에는

00:02:00.540 --> 00:02:02.980
비슷한 일을 하던 분들도

00:02:02.980 --> 00:02:03.840
프로젝트에 합류했고

00:02:03.840 --> 00:02:05.100
좋은 협업이 되었습니다, 네.

00:02:05.100 --> 00:02:06.360
그리고

00:02:06.360 --> 00:02:07.860
혹시 여러분 중에

00:02:07.860 --> 00:02:08.640
이 주제를 선택하게 된

00:02:08.640 --> 00:02:10.500
다른 배경에 대해

00:02:10.500 --> 00:02:12.350
덧붙이고 싶은 분이

00:02:12.350 --> 00:02:14.200
계신가요?

00:02:14.200 --> 00:02:15.240
그러면

00:02:15.240 --> 00:02:16.764
저희 연구실은 딥

00:02:16.764 --> 00:02:17.920
강화학습을 합니다만

00:02:17.920 --> 00:02:19.720
역사적으로 ‘딥’은 두세

00:02:19.720 --> 00:02:21.520
개

00:02:21.520 --> 00:02:21.820
또는

00:02:21.820 --> 00:02:24.040
네 개 층 정도를 의미했고, 케빈이

00:02:24.040 --> 00:02:26.260
그리고

00:02:26.260 --> 00:02:27.060
이샨이 정말 깊은

00:02:27.060 --> 00:02:27.580
네트워크를

00:02:27.580 --> 00:02:28.020
해 보자고 했을 때

00:02:28.020 --> 00:02:29.880
저는 잘 될지 꽤 회의적이었습니다.

00:02:29.880 --> 00:02:30.790
저도 전에 해 봤는데 잘 안 됐고

00:02:30.790 --> 00:02:31.700
요.

00:02:31.700 --> 00:02:32.280
다른 논문들도

00:02:32.280 --> 00:02:33.080
이전에 시도했지만 잘 될

00:02:33.080 --> 00:02:33.840
리 없었죠.

00:02:33.840 --> 00:02:36.480
그래서 시작할 때는 정말 매우 회의적이었습니다.

00:02:36.480 --> 00:02:36.820
제가

00:02:36.820 --> 00:02:38.600
당시에 이걸 전달했는지는 모르겠지만

00:02:38.600 --> 00:02:41.360
들어갈 때의 제 선입견은 그랬습니다, 그런데

00:02:41.360 --> 00:02:42.760
본인의 일을, 예를 들면

00:02:42.760 --> 00:02:44.010
심사하는 일로 보시나요, 아니면 '여러분, 이건'

00:02:44.010 --> 00:02:45.260
아마도

00:02:45.260 --> 00:02:46.060
잘 안 될 것 같으니

00:02:46.060 --> 00:02:46.360
다른

00:02:46.360 --> 00:02:46.840
아이디어를

00:02:46.840 --> 00:02:47.890
시도해 보라고 하시는 건가요, 아니면 계속 격려하시는 건가요

00:02:47.890 --> 00:02:48.940
심지어

00:02:48.940 --> 00:02:53.100
멍청해 보여도요; 결국은 베팅을 고르는 겁니다, 네, 그리고

00:02:53.100 --> 00:02:53.660
이건

00:02:53.660 --> 00:02:55.380
제가 기꺼이 해볼 만한 베팅이었습니다; 뭐가

00:02:55.380 --> 00:02:57.980
그 베팅을 하게 했나요? 저는

00:02:57.980 --> 00:03:00.120
비용이 비교적 낮아 보였다고 생각했습니다, 음,

00:03:00.120 --> 00:03:03.540
특히 미할이 지난

00:03:03.540 --> 00:03:04.480
1년 동안 인프라를 개발해 왔고

00:03:04.480 --> 00:03:05.420
그게 훨씬

00:03:05.420 --> 00:03:05.700
더 쉽게

00:03:05.700 --> 00:03:08.760
이런 실험들을 돌릴 수 있게 해줬습니다, 그리고

00:03:08.760 --> 00:03:09.920
전례로는 깊은 신경망이

00:03:09.920 --> 00:03:11.080
훨씬

00:03:11.080 --> 00:03:11.960
더 잘해야 한다는 믿음이 있었죠, 그러니까

00:03:11.960 --> 00:03:13.170
그게 딥러닝 혁명이

00:03:13.170 --> 00:03:14.380
지금까지

00:03:14.380 --> 00:03:15.620
지난 10년 동안 해온 일이잖아요, 네, 그런데 왜

00:03:15.620 --> 00:03:16.280
우리는 더 깊게 만들기를 멈췄을까요, 그리고 강화학습은

00:03:16.280 --> 00:03:17.620
이렇게

00:03:17.620 --> 00:03:18.960
특이하게도

00:03:18.960 --> 00:03:21.280
계속

00:03:21.280 --> 00:03:21.580
정말

00:03:21.580 --> 00:03:22.710
얕은 네트워크를 써 왔고, 이는 특히

00:03:22.710 --> 00:03:23.840
우리가

00:03:23.840 --> 00:03:24.680
보고 있던

00:03:24.680 --> 00:03:25.520
설정에서 더 그랬는데, 거기서는

00:03:25.520 --> 00:03:26.730
처음부터

00:03:26.730 --> 00:03:27.940
아무것도 없는 상태에서 시작하잖아요; 혹시

00:03:27.940 --> 00:03:28.800
다른 관점이 있으면 여러분도

00:03:28.800 --> 00:03:29.660
끼어들어서

00:03:29.660 --> 00:03:31.600
말씀해 주실래요? 아니면 제가 그냥

00:03:31.600 --> 00:03:34.040
저희 프로젝트 개요를 말씀드릴까요? 네,

00:03:34.040 --> 00:03:36.220
알겠습니다, 죄송합니다, 네, 그러면

00:03:36.220 --> 00:03:38.160
저희가 프로젝트를 보는 방식은, 음, 이렇습니다

00:03:38.160 --> 00:03:39.780
만약 딥러닝의

00:03:39.780 --> 00:03:41.000
전반적인 지형을 보면, 아시다시피

00:03:41.000 --> 00:03:44.520
NLP, 언어, 비전, 그리고 RL이 있고

00:03:44.520 --> 00:03:46.200
그리고 벤이

00:03:46.200 --> 00:03:47.540
암시했듯이, 아시다시피 언어에서는

00:03:47.540 --> 00:03:49.760
비전에서는, 우리는

00:03:49.760 --> 00:03:50.740
거대한 네트워크로 스케일링하는

00:03:50.740 --> 00:03:51.720
패러다임에 수렴해 왔습니다, 그렇죠,

00:03:51.720 --> 00:03:51.860
그러니까

00:03:51.860 --> 00:03:52.660
수천억 개의 파라미터,

00:03:52.660 --> 00:03:53.440
수조 개의 파라미터,

00:03:53.440 --> 00:03:56.320
그리고 그로 인해, 아시다시피, 많은 것이

00:03:56.320 --> 00:03:56.620
정말

00:03:56.620 --> 00:03:57.420
딥러닝에서

00:03:57.420 --> 00:03:58.100
얻어졌습니다, 그로부터,

00:03:58.100 --> 00:04:00.000
그렇죠, 그런데 그러고 나서는

00:04:00.000 --> 00:04:00.960
딥러닝의 세 번째

00:04:00.960 --> 00:04:03.360
가지인 딥 RL에서는

00:04:03.360 --> 00:04:05.060
아직 그런 일이 일어나지 않았습니다, 음, 예를 들면

00:04:05.060 --> 00:04:06.040
저는 정말 놀랐는데, 예를 들면

00:04:06.040 --> 00:04:08.340
제가 벤의 수업

00:04:08.340 --> 00:04:09.620
과 세미나에 들어갔을 때 네트워크를 보며, 어 왜

00:04:09.620 --> 00:04:10.440
그냥

00:04:10.440 --> 00:04:11.680
단순한, 그러니까

00:04:11.680 --> 00:04:14.000
이런 최전선 수준의

00:04:14.000 --> 00:04:14.980
강화학습 알고리즘에서도

00:04:14.980 --> 00:04:17.180
그냥

00:04:17.180 --> 00:04:20.340
2층 MLP 같은 것만 쓰고 계시지 않나 싶었고, 그래서 저는

00:04:20.340 --> 00:04:21.760
정말 궁금했습니다; 우리가 RL 알고리즘을 설계할 수 있을지,

00:04:21.760 --> 00:04:23.820
우리가

00:04:23.820 --> 00:04:25.420
RL이 스케일하도록 하는 레시피를

00:04:25.420 --> 00:04:27.020
모아서

00:04:27.020 --> 00:04:27.420
어쩌면, 아시다시피

00:04:27.420 --> 00:04:28.570
유사한

00:04:28.570 --> 00:04:29.720
방식으로, 언어와 비전이 스케일하는 방식처럼 만들 수 있을지, 그래서 우리가

00:04:29.720 --> 00:04:32.360
한

00:04:32.360 --> 00:04:32.800
일은, 전통적인 RL은

00:04:32.800 --> 00:04:34.050
예를 들어

00:04:34.050 --> 00:04:35.300
가치 기반 RL은 사실

00:04:35.300 --> 00:04:36.100
스케일하지 않는다는 게

00:04:36.100 --> 00:04:36.580
문헌에서

00:04:36.580 --> 00:04:38.340
꽤 분명하다는 점이었죠,

00:04:38.340 --> 00:04:39.920
그래서 우리는 RL의 다른 접근을

00:04:39.920 --> 00:04:41.500
시도했는데, 이를 자기지도 RL이라고 부르며, 여기서는 대신

00:04:41.500 --> 00:04:41.980
가치 함수를

00:04:41.980 --> 00:04:43.552
학습하는 게 아니라

00:04:43.552 --> 00:04:45.340
상태와 행동,

00:04:45.340 --> 00:04:47.260
그리고 미래 상태의 표현을 학습해서, 그

00:04:47.260 --> 00:04:49.091
같은 궤적을 따라 나온 표현들은

00:04:49.091 --> 00:04:49.940
서로 가깝게 밀어 넣고,

00:04:49.940 --> 00:04:50.860
서로 다른

00:04:50.860 --> 00:04:51.690
궤적의 표현들은 멀어지게 밀어냅니다, 그리고 이것은

00:04:51.690 --> 00:04:52.520
그냥

00:04:52.520 --> 00:04:55.520
RL에 대한 또 다른 접근이라

00:04:55.520 --> 00:04:56.360
저희가

00:04:56.360 --> 00:04:57.260
자기지도 방식으로 학습할 수 있게 해줍니다, 그래서

00:04:57.260 --> 00:04:58.160
즉

00:04:58.160 --> 00:05:00.700
사람이

00:05:00.700 --> 00:05:01.060
직접

00:05:01.060 --> 00:05:02.240
만든 보상 신호 없이도 과제를 풀고 목표에 도달할 수 있습니다, 그래서

00:05:02.240 --> 00:05:03.420
우리는

00:05:03.420 --> 00:05:04.320
자기지도 학습이

00:05:04.320 --> 00:05:05.220
이런

00:05:05.220 --> 00:05:06.330
여러 영역에서 스케일된다는 걸 알고 있고, 딥러닝에서도 그렇듯

00:05:06.330 --> 00:05:07.440
과연

00:05:07.440 --> 00:05:09.210
자기지도 RL도 비슷한 방식으로 스케일할 수 있겠습니까? 그런데

00:05:09.210 --> 00:05:10.980
우리가

00:05:10.980 --> 00:05:11.080
처음 시도했을 때는 사실 잘 안 됐습니다,

00:05:11.080 --> 00:05:11.880
그러니까

00:05:11.880 --> 00:05:12.680
그러니까

00:05:12.680 --> 00:05:14.448
저희가 네트워크를 더 깊게 만들수록

00:05:14.448 --> 00:05:15.280
성능이

00:05:15.280 --> 00:05:15.540
완전히

00:05:15.540 --> 00:05:17.880
악화됐는데, 또 한편으로는, 그리고 저는

00:05:17.880 --> 00:05:19.220
별도로 생각하기에, 또 어떤

00:05:19.220 --> 00:05:20.560
다른 연구도

00:05:20.560 --> 00:05:22.000
음, 그러니까

00:05:22.000 --> 00:05:23.180
관련 문헌에서 저희가 잔차

00:05:23.180 --> 00:05:24.360
연결을

00:05:24.360 --> 00:05:26.340
그리고 몇 가지 다른

00:05:26.340 --> 00:05:27.190
아키텍처 구성 요소도

00:05:27.190 --> 00:05:28.040
함께 넣어

00:05:28.040 --> 00:05:30.000
레시피를 구성했더니, 그러자 갑자기

00:05:30.000 --> 00:05:30.880
어느 날

00:05:30.880 --> 00:05:33.340
제가 이 실험을 돌렸는데, 거기에는

00:05:33.340 --> 00:05:34.140
특정한 한 환경이 있었고 그 환경에서는

00:05:34.140 --> 00:05:34.880
정말

00:05:34.880 --> 00:05:35.060
그러니까

00:05:35.060 --> 00:05:36.160
깊이를 두 배로 늘리는 것으로

00:05:36.160 --> 00:05:37.260
깊이를

00:05:37.260 --> 00:05:38.260
사실 별 변화가 없었는데

00:05:38.260 --> 00:05:39.260
그다음에 다시

00:05:39.260 --> 00:05:39.720
깊이를 또 두 배로 늘리니

00:05:40.160 --> 00:05:42.336
이런 다른 구성 요소들과 함께하니까 갑자기

00:05:42.336 --> 00:05:43.860
성능이 급상승했고

00:05:43.860 --> 00:05:44.980
이 한 환경에서

00:05:44.980 --> 00:05:47.760
이걸 동작하게 만드는 것이 매우 쉽지 않았습니다

00:05:47.760 --> 00:05:49.100
보통 저희가

00:05:49.100 --> 00:05:49.700
한다고 생각하는

00:05:49.700 --> 00:05:51.887
하이퍼파라미터 최적화는

00:05:51.887 --> 00:05:52.980
A를 바꿔 보고

00:05:52.980 --> 00:05:54.840
좋아지면, B를 바꿔 보면서

00:05:54.840 --> 00:05:55.100
과연

00:05:55.100 --> 00:05:57.260
더 좋아지는지 보는 방식인데, 그냥

00:05:57.260 --> 00:05:59.520
깊이만 더 키우면 더 나빠지면 저희는

00:05:59.520 --> 00:06:00.480
잔차 연결만 넣어 보기도 했지만

00:06:00.480 --> 00:06:02.060
좋아지지 않았고, 정말로

00:06:02.060 --> 00:06:03.300
이 요인들의 조합을 Kevin과

00:06:03.300 --> 00:06:04.540
Sean이

00:06:04.540 --> 00:06:04.980
알아냈고

00:06:04.980 --> 00:06:07.440
그게 정말로 이걸 되게 했습니다, 그리고

00:06:07.440 --> 00:06:08.920
그 전에 저희는 스케일링도 시도했는데

00:06:08.920 --> 00:06:09.820
다른 차원들로

00:06:09.820 --> 00:06:12.660
배치 크기를 키우고, 또

00:06:12.660 --> 00:06:13.900
네트워크의 폭, 즉 은닉

00:06:13.900 --> 00:06:15.140
레이어 크기도 키웠습니다

00:06:15.800 --> 00:06:17.500
네, 거의 그냥

00:06:17.500 --> 00:06:18.940
깊이를 순진하게 스케일링하는 것과 비슷했고, 음, 그리고

00:06:18.940 --> 00:06:20.380
일단

00:06:20.380 --> 00:06:20.680
저희가 시작해서

00:06:20.680 --> 00:06:22.371
잔차 연결과 레이어

00:06:22.371 --> 00:06:23.840
정규화 같은 특정한 아키텍처

00:06:23.840 --> 00:06:25.400
선택들을 도입하자 그때

00:06:25.400 --> 00:06:27.413
이런 큰 성능 점프가

00:06:27.413 --> 00:06:28.420
즉 이런 임계

00:06:28.420 --> 00:06:29.800
깊이에서 성능이

00:06:29.800 --> 00:06:31.180
정말

00:06:31.180 --> 00:06:32.290
매우 큰 비율로 곱해지듯 늘었고, 그 지점에서 저희는

00:06:32.290 --> 00:06:33.400
진짜로

00:06:33.400 --> 00:06:35.378
의미 있는

00:06:35.378 --> 00:06:36.240
성능 향상이 열리는 것을

00:06:36.900 --> 00:06:38.270
그냥 스케일링하는 것과는 달리

00:06:38.270 --> 00:06:39.640
그것은

00:06:39.640 --> 00:06:41.110
어느 정도 성능 향상을 주기는 했지만, 음,

00:06:41.110 --> 00:06:42.580
하지만

00:06:42.580 --> 00:06:42.700
여러분이

00:06:42.700 --> 00:06:44.220
네트워크의 파라미터 수를

00:06:44.220 --> 00:06:46.740
성장시키면서 보면, 대략

00:06:46.740 --> 00:06:47.660
이차적으로 증가하는데

00:06:47.660 --> 00:06:48.560
깊이를 늘리는 것과는 달리,

00:06:48.560 --> 00:06:49.460
그래서

00:06:49.460 --> 00:06:50.430
어떤 의미에서는 파라미터

00:06:50.430 --> 00:06:51.400
효율이 더 높고

00:06:51.400 --> 00:06:51.700
또한

00:06:51.700 --> 00:06:52.870
저희 실험에서는 샘플 효율도 더 높았습니다

00:06:52.870 --> 00:06:54.040
저희가

00:06:54.040 --> 00:06:56.900
수행했습니다, 좋습니다, 음, 어떤 면에서는 여러분이

00:06:56.900 --> 00:06:57.020
일종의

00:06:57.020 --> 00:06:59.480
현실에서 관찰되는 현상을

00:06:59.480 --> 00:07:02.140
하지만 아주 작은 모델에서

00:07:02.140 --> 00:07:03.660
연구할 수 있게 재현하신 것인데

00:07:03.660 --> 00:07:04.720
그렇다고 보십니까? 네, 그래서 저는

00:07:04.720 --> 00:07:06.260
Kevin이 앞서 말한 것에 덧붙이자면

00:07:06.260 --> 00:07:07.760
저희는 이런 엄청난 성능

00:07:07.760 --> 00:07:09.928
향상을 언어 모델과 이미지

00:07:09.928 --> 00:07:11.100
생성 모델에서

00:07:11.100 --> 00:07:12.440
더 크게 만들고 더 깊게 만듦으로써

00:07:12.440 --> 00:07:13.720
얻었는데, 그건 매우 직관적으로 보입니다, 그리고 그래서

00:07:13.720 --> 00:07:15.000
그게

00:07:15.000 --> 00:07:17.286
저희 연구가

00:07:17.286 --> 00:07:18.200
기초

00:07:18.200 --> 00:07:18.700
연구에서

00:07:18.700 --> 00:07:20.820
예컨대 잔차 네트워크 같은 것에서 영감을 받는 이유입니다, 즉

00:07:20.820 --> 00:07:21.680
잔차

00:07:21.680 --> 00:07:23.875
연결을 사용해 소실

00:07:23.875 --> 00:07:24.740
그래디언트를 막고

00:07:24.740 --> 00:07:25.800
그건 저희가

00:07:25.800 --> 00:07:27.660
논문에서 더 아래쪽에 있는 어블레이션으로 보여드리는데

00:07:27.660 --> 00:07:28.000
아래에

00:07:28.000 --> 00:07:29.050
아마 부록에 있을 텐데, 거기에서

00:07:29.050 --> 00:07:30.100
저희가

00:07:30.100 --> 00:07:31.110
이 잔차

00:07:31.110 --> 00:07:32.120
연결 없이 한 실험을

00:07:32.120 --> 00:07:33.270
그래서 이런 개념들을

00:07:33.270 --> 00:07:34.420
가져와

00:07:34.420 --> 00:07:35.750
다른 분야에 이미 존재하던 것을

00:07:35.750 --> 00:07:37.080
적용해서

00:07:37.080 --> 00:07:37.260
그것을

00:07:37.260 --> 00:07:39.560
RL이라는 이 설정에 가져와서, 동작한다는 것을

00:07:39.560 --> 00:07:41.840
보인 것입니다, Ben이 가기 전에

00:07:41.840 --> 00:07:43.040
가셔야 하니, 마지막 발언은

00:07:43.040 --> 00:07:45.400
그분께 맡기겠습니다.

00:07:45.400 --> 00:07:47.360
추가로 어떤 작업이

00:07:47.360 --> 00:07:49.560
이로 인해 떠오르는지, 또 어떤 걸

00:07:49.560 --> 00:07:50.360
다음으로 밀고 가고 싶으신지요. 저는

00:07:50.360 --> 00:07:51.020
논문에 대해 한 가지는 정리하고

00:07:51.020 --> 00:07:52.330
가고 싶습니다.

00:07:52.330 --> 00:07:53.640
네.

00:07:53.640 --> 00:07:53.880
제가 보기엔

00:07:53.880 --> 00:07:55.240
논문에서 제가 정리하고 싶은 부분은

00:07:55.240 --> 00:07:56.200
제목을 읽는 많은 분들이

00:07:56.200 --> 00:07:57.800
“와, 큰 네트워크라니”

00:07:57.800 --> 00:07:58.840
“좋네, 나도 큰 네트워크를 쓰면 되겠네, 당신들이”

00:07:58.840 --> 00:07:59.880
“해결했구나”라고

00:07:59.880 --> 00:08:01.240
“이제 그냥, 그래, 그냥”

00:08:01.240 --> 00:08:01.700
“큰 네트워크를 가져다가”

00:08:01.700 --> 00:08:03.900
“PPO에 붙이고, SAC에 붙이고”

00:08:03.900 --> 00:08:04.890
“좋아하는 강화학습”

00:08:04.890 --> 00:08:05.880
“알고리즘에 붙이면 되지”라고 하시는데,

00:08:05.880 --> 00:08:06.560
하지만 저는

00:08:06.560 --> 00:08:07.360
그게 핵심 결론은 아니라고

00:08:07.360 --> 00:08:07.980
생각합니다.

00:08:07.980 --> 00:08:09.130
핵심 결론은 큰

00:08:09.130 --> 00:08:10.280
네트워크를 쓰려면

00:08:10.280 --> 00:08:11.060
단지

00:08:11.060 --> 00:08:13.475
이런 구조적 기법들이 필요합니다만

00:08:13.475 --> 00:08:14.280
케빈이

00:08:14.280 --> 00:08:15.260
앞서 말씀하셨듯이, 또

00:08:15.260 --> 00:08:16.240
다른

00:08:16.240 --> 00:08:19.117
목표 함수를 사용해야 합니다. 이 목표 함수는 실제로

00:08:19.117 --> 00:08:20.100
보상을

00:08:20.100 --> 00:08:22.280
포함하지 않습니다. 그래서 제목에 또 들어 있는

00:08:22.280 --> 00:08:23.670
‘강화학습’이라는 단어도

00:08:23.670 --> 00:08:25.060
다소

00:08:25.060 --> 00:08:27.460
오해의 소지가 있는데, 저희는

00:08:27.460 --> 00:08:28.880
보상을 직접 최대화하려는 것이 아닙니다. 저희

00:08:28.880 --> 00:08:30.300
코드에는

00:08:30.300 --> 00:08:31.440
‘보상을 최대화하라’라는

00:08:31.440 --> 00:08:32.580
코드 한 줄도

00:08:32.580 --> 00:08:35.400
없습니다. 그래서 결국

00:08:35.400 --> 00:08:36.260
이것이 강화학습

00:08:36.260 --> 00:08:37.120
방법이

00:08:37.120 --> 00:08:37.440
맞는지는

00:08:37.440 --> 00:08:39.460
잘 모르겠습니다. 오히려 다른 분야의 자기

00:08:39.460 --> 00:08:40.770
지도 방법들과

00:08:40.770 --> 00:08:42.080
더 비슷해 보입니다.

00:08:42.080 --> 00:08:43.060
그래서 저는

00:08:43.060 --> 00:08:44.250
이 방법과 연구가 실제로

00:08:44.250 --> 00:08:45.440
서 있는 곳은

00:08:45.440 --> 00:08:46.710
흥미로운 교차점, 즉

00:08:46.710 --> 00:08:47.980
강화

00:08:47.980 --> 00:08:51.310
학습과 자기지도학습

00:08:51.310 --> 00:08:52.660
연구의 접점이라고 생각하며, 저희는

00:08:52.660 --> 00:08:54.520
포스터 왼쪽 아래에 작은 그림을

00:08:54.520 --> 00:08:55.160
넣었는데,

00:08:55.160 --> 00:08:58.000
그것은 어떤 슬라이드의 스크린샷이었고

00:08:58.000 --> 00:08:59.220
얀 르쿤의 슬라이드로, 어떻게

00:08:59.220 --> 00:09:00.440
어떻게

00:09:00.440 --> 00:09:01.020
지능적인

00:09:01.020 --> 00:09:02.030
시스템을 만들고, 그것이

00:09:02.030 --> 00:09:03.040
이뤄질지,

00:09:03.040 --> 00:09:04.460
비지도학습인지, 지도

00:09:04.460 --> 00:09:05.880
학습인지,

00:09:05.880 --> 00:09:06.910
강화학습인지에 관한 내용이었으며, 저는

00:09:06.910 --> 00:09:07.940
저희 논문이

00:09:07.940 --> 00:09:09.250
정말로 시사하는 바는 그 경계가

00:09:09.250 --> 00:09:10.560
이들

00:09:10.560 --> 00:09:12.460
사이가 매우 흐릿하며, 어쩌면 핵심은

00:09:12.460 --> 00:09:13.260
지능적인 시스템을 만드는 데 있어

00:09:13.260 --> 00:09:13.940
결국

00:09:13.940 --> 00:09:14.240
활용하는 것, 즉

00:09:14.240 --> 00:09:16.660
세 분야 모두에서 얻은 통찰을 활용하는 것이라고 생각합니다. 네, 레이어

00:09:16.660 --> 00:09:20.400
킥, 맞습니다. 시간 내주셔서

00:09:20.400 --> 00:09:21.500
감사합니다. 곧

00:09:21.500 --> 00:09:21.660
가셔야

00:09:21.660 --> 00:09:24.060
한다는 것을 알고 있습니다. 와주셔서 정말 감사합니다.

00:09:24.060 --> 00:09:26.780
그 ‘경계를 흐리게 한다’는 통찰이

00:09:26.780 --> 00:09:27.760
흥미롭다고 생각합니다.

00:09:27.760 --> 00:09:29.320
아까 말씀하신 내용이

00:09:29.320 --> 00:09:30.460
그러니까, 어, 추상화

00:09:30.460 --> 00:09:31.600
계층,

00:09:31.600 --> 00:09:32.520
표현

00:09:32.520 --> 00:09:33.750
학습에 관한 것이었는데, 그게 어떤 생각을

00:09:33.750 --> 00:09:34.980
떠올리게 하는 것이 있는지요.

00:09:34.980 --> 00:09:37.680
자기

00:09:37.680 --> 00:09:38.440
지도와

00:09:38.440 --> 00:09:39.530
강화학습의 결합과 관련해,

00:09:39.530 --> 00:09:40.620
근본적으로

00:09:40.620 --> 00:09:42.740
발견하신 것이 있는지, 또는 저희가

00:09:42.740 --> 00:09:43.820
사람들이 이해하지 못하는 부분이

00:09:43.820 --> 00:09:44.900
논문을

00:09:44.900 --> 00:09:46.700
읽을 때 있는지요. 네, 제가 가장

00:09:46.700 --> 00:09:47.260
잘

00:09:47.260 --> 00:09:50.100
설명하자면, 표준

00:09:50.100 --> 00:09:52.540
강화학습은 확장성이 좋지 않다는 것을 알고 있습니다. 그래서

00:09:52.540 --> 00:09:53.680
왜 이런 다른

00:09:53.680 --> 00:09:54.900
접근이나 다른 목표를 가진 강화학습은

00:09:54.900 --> 00:09:56.120
확장 가능하냐는 질문인데,

00:09:56.120 --> 00:09:57.170
제 생각에는 그 이유가 저희가 근본적으로

00:09:57.170 --> 00:09:58.220
학습의

00:09:58.780 --> 00:10:00.340
부담을 예를 들어

00:10:00.340 --> 00:10:01.900
Q

00:10:01.900 --> 00:10:02.850
러닝이나 TD

00:10:02.850 --> 00:10:03.800
오차에 대한 회귀 같은 것에서

00:10:03.800 --> 00:10:04.240
이는

00:10:04.240 --> 00:10:05.780
저희가 알다시피 상당히 허구적이고 잡음이 많고

00:10:05.780 --> 00:10:07.714
편향된 것들인데, 이를 근본적으로

00:10:07.714 --> 00:10:09.520
분류 문제로 바꾸는 것입니다. 저희는

00:10:09.520 --> 00:10:10.900
미래 상태가

00:10:10.900 --> 00:10:12.280
같은

00:10:12.280 --> 00:10:13.130
궤적을 따르는지, 아니면 다른

00:10:13.130 --> 00:10:13.980
궤적을 따르는지를

00:10:14.540 --> 00:10:15.640
분류하려고 합니다. 그리고 이를 표현

00:10:15.640 --> 00:10:16.740
학습으로 수행합니다.

00:10:16.740 --> 00:10:18.640
또한 분류

00:10:18.640 --> 00:10:19.620
교차 엔트로피 손실과

00:10:19.620 --> 00:10:20.890
표현 학습은

00:10:20.890 --> 00:10:22.160
딥

00:10:22.160 --> 00:10:22.960
러닝 문헌에서 확장 가능하다는 것을 알고 있습니다. 예를 들어

00:10:22.960 --> 00:10:23.760
언어

00:10:24.700 --> 00:10:26.920
모델의 일부 목표 함수들을 생각해 보면요. 그래서

00:10:26.920 --> 00:10:30.060
어떤 의미에서는 저희가 그

00:10:30.060 --> 00:10:30.660
경계를 흐리고 있고,

00:10:30.660 --> 00:10:31.610
강화학습을 하고 있기는 합니다. 여전히

00:10:31.610 --> 00:10:32.560
액터-크리틱

00:10:32.560 --> 00:10:33.640
강화학습 알고리즘이며,

00:10:33.640 --> 00:10:34.720
목표

00:10:34.720 --> 00:10:36.763
조건부 강화학습 알고리즘입니다. 하지만

00:10:36.763 --> 00:10:37.760
목표 함수, 즉 부담은

00:10:37.760 --> 00:10:39.940
그 RL 과제를 학습하고 해결하는

00:10:39.940 --> 00:10:40.160
부분에서

00:10:40.160 --> 00:10:41.420
더

00:10:41.420 --> 00:10:42.680
유사한 어떤 것으로 옮겨가며,

00:10:42.680 --> 00:10:43.980
언어에서 볼 법한 목표 함수들로

00:10:43.980 --> 00:10:45.280
그리고

00:10:45.280 --> 00:10:46.080
비전에서 볼 법한 목표 함수들로 옮겨갑니다. 이것들이 크게 스케일해 왔다는

00:10:46.080 --> 00:10:46.560
것을 알고 있습니다.

00:10:46.560 --> 00:10:48.580
그래서 제 생각에는, 네, 이것이

00:10:48.580 --> 00:10:49.500
근본적인

00:10:49.500 --> 00:10:52.280
통찰 중 하나입니다. 또한

00:10:52.280 --> 00:10:53.250
RL에 이런

00:10:53.250 --> 00:10:54.220
다른 방식으로 접근하면 저희가

00:10:54.220 --> 00:10:55.400
훨씬 더 많은 것을 끌어낼 수 있고

00:10:55.400 --> 00:10:57.540
네트워크를 크게 스케일할 수 있으며

00:10:57.540 --> 00:10:59.060
기존

00:10:59.060 --> 00:11:00.960
RL에서 표준적으로 쓰이던 수준을 훨씬 넘어설 수 있었습니다. 잠깐

00:11:00.960 --> 00:11:02.940
끼어들어도 되겠습니까, 제가 조금

00:11:02.940 --> 00:11:05.100
더

00:11:05.100 --> 00:11:06.700
아키텍처에 대한 맥락을 말씀드리겠습니다. 왜냐하면 저희는

00:11:06.700 --> 00:11:09.258
다른 목표 함수를

00:11:09.258 --> 00:11:10.500
사용하고 있고, 또 영향 받은 것들이 있어 대조

00:11:10.500 --> 00:11:14.040
손실 같은 것이 있습니다. 다만 아키텍처는 꽤

00:11:14.040 --> 00:11:15.660
유사합니다,

00:11:15.660 --> 00:11:17.280
이전 연구들, 이전

00:11:17.280 --> 00:11:20.100
논문들, 예를 들어

00:11:20.100 --> 00:11:20.800
BRO라든지, 혹은 SIMBA, SIMBA V1, SIMBA

00:11:20.800 --> 00:11:24.320
V2, SIMBA V1, SIMBA V2 같은 것들입니다. 그래서

00:11:24.320 --> 00:11:29.200
저희도 이

00:11:29.200 --> 00:11:31.000
아키텍처를 조금 수정했습니다. 하지만

00:11:31.000 --> 00:11:32.800
저희가

00:11:32.800 --> 00:11:34.600
처음부터 바퀴를 새로 발명한 것은 아닙니다.

00:11:34.600 --> 00:11:36.760
핵심은

00:11:36.760 --> 00:11:37.720
아키텍처와

00:11:37.720 --> 00:11:39.060
목표 함수의

00:11:39.060 --> 00:11:40.400
결합이 스케일이 정말로

00:11:40.400 --> 00:11:43.520
올라가게 만들고,

00:11:43.520 --> 00:11:44.460
성능도 그 스케일을 따라가게 만든다고

00:11:44.460 --> 00:11:46.700
생각합니다. 이 부분은 저희가

00:11:46.700 --> 00:11:47.840
아마도

00:11:47.840 --> 00:11:48.980
더 깊이 파고들어야 할 것 같습니다.

00:11:48.980 --> 00:11:50.000
음, 그러면

00:11:50.000 --> 00:11:52.800
어떤 도메인, 어떤 산업에 적용하셨는지

00:11:52.800 --> 00:11:53.940
여쭙고 싶습니다.

00:11:53.940 --> 00:11:55.080
여러

00:11:55.080 --> 00:11:56.000
다양한 네트워크 유형이나 데이터

00:11:56.000 --> 00:11:58.120
셋에 적용해 보셨을 텐데, 특별히 잘 맞는 것이

00:11:58.120 --> 00:11:59.590
있다고

00:11:59.590 --> 00:12:01.060
생각하시는지,

00:12:01.060 --> 00:12:01.820
즉 비교적 쉽게 성과가 나는 영역이 있는지요. 네, 그래서

00:12:01.820 --> 00:12:04.100
사실

00:12:04.100 --> 00:12:05.180
저희 과제들을 많이 보면 특히

00:12:05.180 --> 00:12:07.080
로보틱스 과제들입니다. 그래서 저는

00:12:07.080 --> 00:12:09.700
이런 작업이

00:12:09.700 --> 00:12:12.420
이런 연구가 로보틱스 분야에 어떤 영향을 줄지 매우 궁금합니다.

00:12:12.420 --> 00:12:13.780
이런 연구가 어떻게

00:12:13.780 --> 00:12:14.980
로보틱스 분야에 어떤 영향을

00:12:14.980 --> 00:12:16.180
줄 수 있는지에 대해 제

00:12:16.180 --> 00:12:17.760
로보틱스에 대한 이해로는 많은 로보틱스가

00:12:17.760 --> 00:12:18.100
요즘은

00:12:18.100 --> 00:12:19.080
서로 다른 몇 가지

00:12:19.080 --> 00:12:20.060
접근법이 있습니다.

00:12:20.060 --> 00:12:22.220
한 가지 접근은 로봇을 훈련할 때

00:12:22.220 --> 00:12:23.360
로봇을

00:12:23.360 --> 00:12:24.160
모방 학습으로 훈련하자는 것이고, 그래서

00:12:24.160 --> 00:12:24.940
즉

00:12:24.940 --> 00:12:26.560
엄청난 양의 데이터를 모으려 하며,

00:12:26.560 --> 00:12:27.300
사람의

00:12:27.820 --> 00:12:29.320
감독을 많이 투입하고 이 데이터를 스케일업하려 합니다.

00:12:29.320 --> 00:12:30.180
그리고 그 데이터로

00:12:30.180 --> 00:12:31.040
모방 학습을 하는 것입니다.

00:12:31.040 --> 00:12:32.440
하지만 다른 한편으로는 잠재적으로

00:12:32.440 --> 00:12:33.840
또

00:12:33.840 --> 00:12:34.930
다른 접근도 있을 수 있는데, 이는

00:12:34.930 --> 00:12:36.020
예를 들면

00:12:36.020 --> 00:12:37.292
목표 조건부

00:12:37.292 --> 00:12:38.440
강화학습처럼

00:12:38.440 --> 00:12:39.900
실제로 로봇

00:12:39.900 --> 00:12:41.070
에이전트를 훈련시키고 RL 에이전트를 훈련해

00:12:41.070 --> 00:12:42.240
의미 있는

00:12:42.240 --> 00:12:43.570
과업을 사람의 감독 없이 전적으로 수행하게 하는 것입니다.

00:12:43.570 --> 00:12:44.900
사람의 감독도

00:12:44.900 --> 00:12:46.140
시범 데이터도 없이 훨씬 더 확장 가능하다는 점입니다, 네.

00:12:46.140 --> 00:12:47.380
그래서요.

00:12:47.380 --> 00:12:48.410
그래서 이것이 대안적인

00:12:48.410 --> 00:12:49.440
접근법이 될 수 있습니다.

00:12:49.440 --> 00:12:50.400
그리고 어쩌면 데이터를 스케일업하는 대신

00:12:50.400 --> 00:12:51.360
즉

00:12:51.360 --> 00:12:52.660
수작업으로 사람의 감독을 늘리는 것을

00:12:52.660 --> 00:12:53.960
말하는데,

00:12:53.960 --> 00:12:56.140
그런 방식은 아시다시피 그다지 확장 가능하지 않으니,

00:12:56.140 --> 00:12:58.040
목표 조건부

00:12:58.040 --> 00:12:58.640
강화

00:12:58.640 --> 00:12:59.820
학습을 확장 가능하게 만들 수 있다면, 그냥

00:12:59.820 --> 00:13:01.000
확장하면 됩니다.

00:13:01.000 --> 00:13:01.860
아키텍처를 확장할 수도 있고, 또 다른 방식으로 확장할 수도 있는데,

00:13:01.860 --> 00:13:02.720
왜냐하면 여러분이

00:13:02.720 --> 00:13:03.620
목표에 집중하고 있기 때문입니다, 네, 그렇죠, 그리고

00:13:03.620 --> 00:13:04.520
또

00:13:04.520 --> 00:13:05.320
서로 다른 특정 목적들이 있다면, 그것이

00:13:05.320 --> 00:13:05.980
상당히

00:13:05.980 --> 00:13:07.580
매우 흥미로울 수 있고, 또 어떻게

00:13:07.580 --> 00:13:09.360
그것이 로보틱스 같은 분야에 어떤 영향을 줄 수 있는지

00:13:09.360 --> 00:13:11.260
예를 들면 말입니다, 네. 어, 효율성에 대해

00:13:11.260 --> 00:13:13.020
한 가지만 더 짚어보겠습니다.

00:13:13.020 --> 00:13:13.900
아까 말씀하신 효율성에 대해서요.

00:13:13.900 --> 00:13:14.780
말입니다.

00:13:14.780 --> 00:13:15.780
저는 이렇게 예상했습니다.

00:13:16.320 --> 00:13:18.640
네트워크가 매우 깊고 더 깊어질수록

00:13:18.640 --> 00:13:19.980
제곱으로 더 나빠져야 할 것 같은데, 저는

00:13:19.980 --> 00:13:21.320
잘 알지 못합니다.

00:13:21.320 --> 00:13:21.740
예를 들어

00:13:21.740 --> 00:13:22.750
기존 문헌에는 익숙하지 않고, 저는 그저

00:13:22.750 --> 00:13:23.760
그냥

00:13:23.760 --> 00:13:25.690
직관을 정리해보는 중인데요, 음

00:13:25.690 --> 00:13:27.620
기본적으로

00:13:27.620 --> 00:13:28.420
어떤

00:13:28.420 --> 00:13:30.680
트레이드오프를 발견하셨는지,

00:13:30.680 --> 00:13:32.080
사람들에게 경고하고 싶은 점이 있다면 무엇인지

00:13:32.080 --> 00:13:33.640
말씀해 주실 수 있습니까, 왜냐하면

00:13:33.640 --> 00:13:34.510
효율성을 언급하신 분이 바로 여러분이니까요.

00:13:34.510 --> 00:13:35.380
그래서

00:13:35.380 --> 00:13:37.200
네, 물론입니다. 제가 말한 것은

00:13:37.200 --> 00:13:37.680
저희 포스터에 있는

00:13:37.680 --> 00:13:39.320
그림 중 하나이자 논문에도 있는

00:13:39.320 --> 00:13:40.310
비교 그래프이며, 모델의

00:13:40.310 --> 00:13:41.300
파라미터

00:13:41.300 --> 00:13:42.480
개수를

00:13:42.480 --> 00:13:44.220
깊이 축으로 확장할 때와

00:13:44.220 --> 00:13:46.080
너비 축으로 확장할 때를

00:13:46.080 --> 00:13:47.480
비교한 것입니다. 저희 기준

00:13:47.480 --> 00:13:48.590
아키텍처에서 가장 기본은

00:13:48.590 --> 00:13:49.700
예를 들면

00:13:49.700 --> 00:13:52.360
너비 256, 즉 은닉층

00:13:52.360 --> 00:13:53.320
뉴런이 256개인 설정입니다.

00:13:53.320 --> 00:13:55.180
그리고 깊이는 4개 층,

00:13:55.180 --> 00:13:58.320
즉 은닉층 4개 정도입니다. 그래서

00:13:58.320 --> 00:13:59.040
제가 거기서 강조한 점은

00:13:59.040 --> 00:14:00.760
깊이 방향으로 확장하면

00:14:00.760 --> 00:14:01.630
모델의 파라미터 수가

00:14:01.630 --> 00:14:02.500
대체로

00:14:02.500 --> 00:14:02.820
증가하는데,

00:14:02.820 --> 00:14:05.528
대략 선형적으로 증가합니다. 반면에

00:14:05.528 --> 00:14:06.480
여러분이

00:14:06.480 --> 00:14:07.980
네트워크의 출력 폭을 더 넓히면,

00:14:07.980 --> 00:14:09.480
입력

00:14:09.480 --> 00:14:10.080
또한 다음

00:14:10.080 --> 00:14:12.640
네트워크로 들어가는 값도 커지게 되고,

00:14:12.640 --> 00:14:13.440
그래서 파라미터 수는

00:14:13.440 --> 00:14:14.220
네트워크가

00:14:14.220 --> 00:14:14.480
결국

00:14:14.480 --> 00:14:15.920
대략 제곱에 가깝게 증가합니다.

00:14:15.920 --> 00:14:17.360
그래서 저희가 한

00:14:17.360 --> 00:14:18.640
실험 중 하나는

00:14:18.640 --> 00:14:19.160
다음과 같은 점을 살펴본 것입니다.

00:14:19.680 --> 00:14:21.280
이 두 가지 방향으로 확장하면서

00:14:21.280 --> 00:14:22.150
모델의 파라미터 수를 늘릴 때

00:14:22.150 --> 00:14:23.020
서로 다른

00:14:23.020 --> 00:14:23.520
선택지에서

00:14:23.980 --> 00:14:25.310
대략 비슷한 파라미터 수라면

00:14:25.310 --> 00:14:26.640
어느

00:14:26.640 --> 00:14:27.800
쪽이 더 좋은 성능을 내는지입니다.

00:14:27.800 --> 00:14:28.960
그리고

00:14:28.960 --> 00:14:29.140
깊이

00:14:29.140 --> 00:14:30.460
곡선은 이런 식으로 꽤 빠르게

00:14:30.460 --> 00:14:31.680
확 치고 올라가며, 이는

00:14:31.680 --> 00:14:32.900
저희

00:14:32.900 --> 00:14:33.900
논문 전반에서

00:14:33.900 --> 00:14:35.180
일관되게 나타납니다. 반대로 너비는

00:14:35.180 --> 00:14:36.740
조금 더 천천히 증가합니다. 그래서

00:14:36.740 --> 00:14:38.220
그로부터 얻는 결론은

00:14:38.220 --> 00:14:39.560
자원이 더 제한되어 있다면

00:14:39.560 --> 00:14:40.900
확장은

00:14:40.900 --> 00:14:41.700
깊이 방향이 더 나을 수 있다는 점입니다, 왜냐하면

00:14:41.700 --> 00:14:42.180
파라미터가

00:14:42.180 --> 00:14:43.400
더 적기 때문이고, 더 작은 모델에서

00:14:43.400 --> 00:14:44.620
더 적은

00:14:44.620 --> 00:14:46.000
학습 가능한 파라미터로도 가능하기 때문입니다. 너비는

00:14:46.000 --> 00:14:47.380
비쌉니다.

00:14:47.380 --> 00:14:48.660
너비는 비쌉니다, 맞습니다. 그리고 일반적으로

00:14:48.660 --> 00:14:49.940
물론

00:14:49.940 --> 00:14:50.740
파라미터가 더 많으면

00:14:50.740 --> 00:14:51.340
더

00:14:51.340 --> 00:14:51.600
비싸집니다.

00:14:51.600 --> 00:14:53.652
그래서 이것도 또 하나의

00:14:53.652 --> 00:14:54.540
고려사항으로

00:14:54.540 --> 00:14:55.640
이런 네트워크를 사용할 때 생각해볼 만합니다.

00:14:55.640 --> 00:14:56.740
가정해보겠습니다.

00:14:56.740 --> 00:14:59.280
네. 이런 종류의 다른 경험칙도

00:14:59.280 --> 00:15:00.720
제가 더 뽑아낼 수 있습니까? 이것은

00:15:00.720 --> 00:15:01.400
제가 생각할 수 있는 가장 기본적인

00:15:01.400 --> 00:15:03.640
것이었습니다. 네, 어,

00:15:03.640 --> 00:15:05.680
다른 게 있을지는 잘 모르겠습니다. 네,

00:15:05.680 --> 00:15:07.140
처음 질문하신

00:15:07.140 --> 00:15:09.240
트레이드오프에 대해 말하자면, 음,

00:15:09.240 --> 00:15:10.260
트레이드오프 중 하나이자

00:15:10.260 --> 00:15:11.180
저희가 언급한 제한사항은

00:15:11.180 --> 00:15:11.980
당연히 네트워크를 더 크게 만들면

00:15:11.980 --> 00:15:12.620
더 커질수록

00:15:12.620 --> 00:15:15.140
실행 시간이 더 오래 걸린다는 점입니다.

00:15:15.140 --> 00:15:17.100
그래서 만약

00:15:17.100 --> 00:15:18.700
어느 깊이에서 깊이를 두 배로 늘리면

00:15:18.700 --> 00:15:20.580
어쩌면 대략 두 배

00:15:20.580 --> 00:15:21.820
정도의 시간이

00:15:21.820 --> 00:15:23.120
순전파에 더 걸릴 수 있습니다.

00:15:23.120 --> 00:15:24.420
하지만

00:15:24.420 --> 00:15:26.520
저희 논문에서처럼

00:15:26.520 --> 00:15:27.800
대부분

00:15:27.800 --> 00:15:29.060
환경에서는 저희가

00:15:29.060 --> 00:15:30.320
학습을 포화시켜

00:15:30.320 --> 00:15:31.750
거의 완벽한

00:15:31.750 --> 00:15:32.580
성능까지

00:15:32.580 --> 00:15:33.480
도달하는 데

00:15:33.480 --> 00:15:34.580
꼭

00:15:34.580 --> 00:15:35.540
1000층까지 갈 필요는 없고, 예를 들어 64층

00:15:35.540 --> 00:15:36.500
정도면

00:15:36.500 --> 00:15:37.540
충분할 수 있습니다.

00:15:37.540 --> 00:15:40.840
그리고 이런 구간에서는

00:15:40.840 --> 00:15:41.670
네트워크의 지연 시간이

00:15:41.670 --> 00:15:42.500
반드시

00:15:42.500 --> 00:15:42.740
사실

00:15:42.740 --> 00:15:44.190
그렇게 큰

00:15:44.190 --> 00:15:45.640
병목이 되지는 않습니다.

00:15:45.640 --> 00:15:47.000
여러분도 상상하실 수 있듯이

00:15:47.000 --> 00:15:47.500
많은 과제에서

00:15:47.500 --> 00:15:49.284
특히 RL에서는

00:15:49.284 --> 00:15:50.120
데이터 수집이

00:15:50.120 --> 00:15:51.290
병목일 수 있고, 또

00:15:51.290 --> 00:15:52.460
네 번

00:15:52.460 --> 00:15:52.700
패스를

00:15:52.700 --> 00:15:53.590
저희 네트워크에 통과시키는 것이

00:15:53.590 --> 00:15:54.480
병목이 아닐 수도 있습니다.

00:15:54.480 --> 00:15:56.940
그래서 저희 연구에서는

00:15:56.940 --> 00:15:57.340
구체적으로

00:15:57.340 --> 00:15:59.297
JAX GCRL

00:15:59.297 --> 00:16:00.560
환경을 사용했는데,

00:16:00.560 --> 00:16:01.800
이는 JAX 기반 GPU 가속 환경입니다.

00:16:01.800 --> 00:16:03.040
그래서

00:16:03.040 --> 00:16:03.180
저희는

00:16:03.180 --> 00:16:05.543
환경

00:16:05.543 --> 00:16:06.500
트래젝터리를 수천 개

00:16:06.500 --> 00:16:07.760
동시에 병렬로 수집할 수 있습니다.

00:16:07.760 --> 00:16:09.980
그렇게 하면, 음,

00:16:09.980 --> 00:16:12.460
아, 이게 기본으로 내장돼 있습니다, 이게

00:16:12.460 --> 00:16:13.760
내장돼 있어서 저희가

00:16:13.760 --> 00:16:14.830
그러니까 약 천 개 정도의

00:16:14.830 --> 00:16:15.900
트래젝터리(궤적)를

00:16:15.900 --> 00:16:16.890
이 모든 환경에서 동시에

00:16:16.890 --> 00:16:17.880
수집할 수 있도록

00:16:17.880 --> 00:16:18.140
그리고

00:16:18.140 --> 00:16:20.520
그래서 음, 이렇게 하면

00:16:20.520 --> 00:16:22.600
학습이 충분히 이뤄질 만큼의 데이터를

00:16:22.600 --> 00:16:23.340
충분히 확보하게 해 줍니다, 와.

00:16:24.000 --> 00:16:25.200
네, 그게 그쪽에서 말하던 방식이라고 들었습니다.

00:16:25.200 --> 00:16:26.400
알겠습니다.

00:16:26.400 --> 00:16:27.840
그리고 혹시 원하시면

00:16:27.840 --> 00:16:29.280
조금 더 탐색해서 설명해 주실 수도

00:16:29.280 --> 00:16:32.440
있을 것 같습니다, 전문용어로는 ZCRL 같은 것 말입니다, 그리고

00:16:32.440 --> 00:16:33.240
아시다시피 대부분은

00:16:33.240 --> 00:16:33.820
파이토치(PyTorch)는

00:16:33.820 --> 00:16:34.340
익숙하지만 아마

00:16:34.340 --> 00:16:36.040
JAX는 덜 익숙하실 텐데, JAX는

00:16:36.040 --> 00:16:37.800
요즘 점점 주목을 받고

00:16:37.800 --> 00:16:39.560
특히

00:16:39.560 --> 00:16:40.540
강화학습 분야에서 그렇습니다.

00:16:40.540 --> 00:16:42.937
왜냐하면 온라인 강화

00:16:42.937 --> 00:16:43.920
학습에서는

00:16:43.920 --> 00:16:46.760
가능한 한 많은 데이터를 모으는 것이

00:16:46.760 --> 00:16:47.080
가장

00:16:47.080 --> 00:16:47.980
중요하기 때문입니다. 물론 파이토치

00:16:47.980 --> 00:16:48.880
대응물도 있겠지만,

00:16:48.880 --> 00:16:52.140
어쨌든, 이런 것도

00:16:52.140 --> 00:16:52.540
탐색하는

00:16:52.540 --> 00:16:54.920
롤아웃을 시도하는 다른 분들께 팁이 있습니까? 네네, 그래서

00:16:54.920 --> 00:16:57.640
저도 추천드리자면

00:16:57.640 --> 00:16:58.880
목표 조건

00:16:58.880 --> 00:17:00.150
강화학습에는 올해 JAX를 추천드리는데,

00:17:00.150 --> 00:17:01.420
또

00:17:01.420 --> 00:17:02.990
멀티에이전트용 JAX

00:17:02.990 --> 00:17:04.560
구현들도 있고

00:17:04.560 --> 00:17:05.240
그리고

00:17:05.240 --> 00:17:07.660
다른 것들도 있습니다. 다시 저희 논문으로 돌아가 보면,

00:17:07.660 --> 00:17:09.540
플롯을 보면 저희는

00:17:09.540 --> 00:17:11.580
이런 큰 성능

00:17:11.580 --> 00:17:15.840
향상이 약 5천만

00:17:15.840 --> 00:17:19.660
트랜지션을 넘길 때 나타나는 것만 봤습니다. 그래서 저는

00:17:19.660 --> 00:17:21.100
데이터가 핵심이라고 생각합니다.

00:17:21.100 --> 00:17:23.540
여기서도 그렇고, 그리고 덧붙이자면

00:17:23.540 --> 00:17:25.880
저는 다른

00:17:25.880 --> 00:17:27.240
딥러닝

00:17:27.240 --> 00:17:29.980
영역의 성공 사례들과 비유를 드는 걸 좋아하는데, 예를 들어

00:17:29.980 --> 00:17:31.280
대규모 언어 모델에서 저희가

00:17:31.280 --> 00:17:32.580
그렇게

00:17:32.580 --> 00:17:32.920
확장할 수

00:17:32.920 --> 00:17:34.680
있었던 이유는, 인터넷 전체

00:17:34.680 --> 00:17:36.440
규모의 데이터를 활용할 수 있는

00:17:36.440 --> 00:17:37.220
패러다임을 찾았기 때문입니다,

00:17:37.220 --> 00:17:40.020
그렇지 않습니까. 그래서 데이터는

00:17:40.020 --> 00:17:41.920
강화학습에서는 전통적으로 구하기가 어려웠습니다만,

00:17:41.920 --> 00:17:44.440
그런데 음, 이제는 이런

00:17:44.440 --> 00:17:45.957
GPU 가속 환경으로 우리는

00:17:45.957 --> 00:17:46.780
수억

00:17:46.780 --> 00:17:48.260
건의 데이터를 단지

00:17:48.260 --> 00:17:48.500
몇

00:17:48.500 --> 00:17:50.700
시간 만에 수집할 수 있고, 그래서 저는 이것이

00:17:50.700 --> 00:17:52.660
우리에게 정말 좋은 테스트베드가 된다고

00:17:52.660 --> 00:17:53.540
생각합니다, 또한 우리가

00:17:53.540 --> 00:17:56.080
음, 네트워크

00:17:56.080 --> 00:17:58.160
용량을 확장하고 비슷한 종류의 이득을 얻는 방법도 있다고 생각합니다.

00:17:58.160 --> 00:17:58.960
제가 질문드린 게, 혹시

00:17:58.960 --> 00:18:00.340
대규모 언어 모델에서 사전학습을

00:18:00.340 --> 00:18:01.720
다르게

00:18:01.720 --> 00:18:03.530
하실 거라는 말씀이십니까, 그러니까

00:18:03.530 --> 00:18:05.340
뭐가

00:18:05.340 --> 00:18:05.900
그, 뭐가

00:18:05.900 --> 00:18:09.260
차이가 있습니까, 어, 지금 목표가 무엇입니까, 음, 네, 저는

00:18:09.260 --> 00:18:10.900
아주 간단히, 아주 간단히 말하면 그

00:18:10.900 --> 00:18:12.540
패러다임은

00:18:12.540 --> 00:18:12.840
말씀하신

00:18:12.840 --> 00:18:14.130
다음 단어, 또는 다음 토큰 예측입니다,

00:18:14.130 --> 00:18:15.420
그렇습니까?

00:18:15.420 --> 00:18:19.580
그건 매우 견고한데, 그걸 어떻게 바꾸십니까?

00:18:19.580 --> 00:18:20.460
아, 저는

00:18:20.460 --> 00:18:21.260
바꾼다고 말하는 게 아닙니다, 저는

00:18:21.260 --> 00:18:22.040
그 통찰을

00:18:22.040 --> 00:18:24.840
활용해서 모든 것에 적용하고 싶습니다, 저는

00:18:24.840 --> 00:18:25.660
오히려

00:18:25.660 --> 00:18:26.460
반대로 가야 한다고 생각하시는 것 같습니다, 반대로 가야 한다는

00:18:26.460 --> 00:18:26.800
말씀이십니까?

00:18:26.800 --> 00:18:28.560
네, 아마도 그렇습니다, 아마도 그렇습니다.

00:18:28.560 --> 00:18:29.980
그러니까, 그건 아주

00:18:29.980 --> 00:18:31.120
흥미로운 연구 방향이기도 하겠지만,

00:18:31.120 --> 00:18:32.260
사실 네, 심지어

00:18:32.260 --> 00:18:33.500
그 지점에서 제가 생각한 것 중 하나는

00:18:33.500 --> 00:18:33.700
제가

00:18:33.700 --> 00:18:35.520
생각하고 있었던 것은, 우리의

00:18:35.520 --> 00:18:37.880
RL 목표가 작동하는 방식이 어떤 의미에서는

00:18:37.880 --> 00:18:38.620
정확히 다음

00:18:39.120 --> 00:18:41.640
단어 예측은 아니지만, 일종의 다음

00:18:41.640 --> 00:18:42.720
상태 예측과 비슷하다는 것입니다, 그렇지 않습니까, 상상해 보시면

00:18:42.720 --> 00:18:43.800
어떤

00:18:43.800 --> 00:18:44.040
현재

00:18:44.040 --> 00:18:45.270
상태에 있고, 어떤 현재 행동이 있고,

00:18:45.270 --> 00:18:46.500
그리고

00:18:46.500 --> 00:18:47.580
우리는 이

00:18:47.580 --> 00:18:48.060
미래 상태가

00:18:48.060 --> 00:18:50.400
그러니까 어떤 특정 상태가 미래 상태인지,

00:18:50.400 --> 00:18:51.200
같은 궤적을 따라 나온 미래 상태인지, 아니면 다른

00:18:51.200 --> 00:18:51.840
궤적인지를 예측하고 싶습니다.

00:18:52.240 --> 00:18:54.500
그래서 어떤 의미에서는, 저희는 사실

00:18:54.500 --> 00:18:55.700
일종의 암묵적인 월드

00:18:55.700 --> 00:18:56.900
모델을 하고 있는 셈입니다.

00:18:56.900 --> 00:18:58.680
그러면 그게 어, 그러니까

00:18:58.680 --> 00:18:59.920
제가 그런 말을 써도 되는지 모르겠는데,

00:18:59.920 --> 00:19:01.460
나쁜 표현인지 모르겠습니다만, 이런 것들이든 아니면

00:19:01.460 --> 00:19:02.760
언어에서는 크로스

00:19:02.760 --> 00:19:03.560
엔트로피 손실로 다음 토큰을 분류하지 않습니까,

00:19:03.560 --> 00:19:04.160
그렇습니까?

00:19:04.160 --> 00:19:05.050
그리고 여기서는 우리는 그냥 이진

00:19:05.050 --> 00:19:05.940
분류를 하는 겁니다,

00:19:05.940 --> 00:19:06.460
그러니까

00:19:06.460 --> 00:19:08.140
어떤 다음 상태가 어떤 것인지,

00:19:08.140 --> 00:19:09.280
네네, 분류입니다, 네네.

00:19:09.280 --> 00:19:10.420
네.

00:19:10.420 --> 00:19:11.340
그래서 저는

00:19:11.340 --> 00:19:13.600
여기에도 우리가 더 깊게 파고들어야 할

00:19:13.600 --> 00:19:14.710
어떤 평행선이 있다고

00:19:14.710 --> 00:19:15.820
생각합니다.

00:19:15.820 --> 00:19:16.380
더 깊이

00:19:16.380 --> 00:19:18.940
파고들어서, 무엇이 핵심인지,

00:19:18.940 --> 00:19:20.020
딥러닝이 확장될 수 있게 해 주는 핵심이 무엇인지 보고,

00:19:20.020 --> 00:19:21.100
그리고

00:19:21.100 --> 00:19:22.440
그걸 어떻게 활용할지,

00:19:22.440 --> 00:19:23.560
그런 통찰을 어떻게

00:19:23.560 --> 00:19:24.680
추출해서

00:19:24.680 --> 00:19:25.620
언어든 강화

00:19:25.620 --> 00:19:26.560
학습이든, 다양한

00:19:26.560 --> 00:19:26.800
분야에

00:19:26.800 --> 00:19:28.855
걸쳐 적용할 수 있을지,

00:19:28.855 --> 00:19:29.720
네, 어.

00:19:29.720 --> 00:19:31.040
제가 월드 모델 이야기에 대해 말한 의미를

00:19:31.040 --> 00:19:31.360
이해하셨습니까?

00:19:31.360 --> 00:19:33.940
네네, 사실 저도, 그리고 저는

00:19:33.940 --> 00:19:35.600
제가 들은 것 같습니다,

00:19:35.600 --> 00:19:36.400
어제

00:19:36.400 --> 00:19:37.200
아이젠바흐 교수님이 포스터에서 이 얘기를

00:19:37.200 --> 00:19:37.820
하고 계셨고,

00:19:37.820 --> 00:19:39.640
몇 사람에게 설명하시면서

00:19:39.640 --> 00:19:40.300
이게

00:19:40.300 --> 00:19:41.410
표현 학습을 하고

00:19:41.410 --> 00:19:42.520
의미 있는 표현을 학습하려는 것이기 때문에,

00:19:42.520 --> 00:19:43.900
그러니까

00:19:44.360 --> 00:19:45.900
주어진 상태와 행동, 그리고

00:19:45.900 --> 00:19:47.800
주어진 목표에 대해, 어떤 의미에서는

00:19:47.800 --> 00:19:48.700
거의

00:19:48.700 --> 00:19:49.750
환경의 모델을 학습하는 것처럼, 세계의 모델을 학습하는 것처럼

00:19:49.750 --> 00:19:50.800
생각할 수 있지만,

00:19:50.800 --> 00:19:52.580
어떤 종류의

00:19:52.580 --> 00:19:53.220
다음 프레임 예측 같은 것을 할 필요는 없습니다,

00:19:53.220 --> 00:19:54.700
그런 것들은

00:19:54.700 --> 00:19:56.180
조금 더 고차원적이고 복잡하기 때문입니다.

00:19:56.180 --> 00:19:57.120
네네, 저는 이렇게 생각합니다,

00:19:57.120 --> 00:19:58.060
음, 제가

00:19:58.060 --> 00:20:00.120
생각하고 밀고 싶은 관점은

00:20:00.120 --> 00:20:02.160
다음 세계를 예측하는 것을 학습하는 대신,

00:20:02.160 --> 00:20:05.640
기본적으로 가능한 여러

00:20:05.640 --> 00:20:06.860
세계 후보를 생성하고,

00:20:06.860 --> 00:20:07.990
그것들을 분류하는 겁니다,

00:20:07.990 --> 00:20:09.120
말씀하신 것처럼입니다.

00:20:09.120 --> 00:20:11.560
그리고 그게 정확히 제가 하는 방식이기도 합니다,

00:20:11.560 --> 00:20:14.020
네, 그게 제가 일을 하는 방식과 정확히 같습니다.

00:20:14.020 --> 00:20:15.680
예를 들어 제가 포커를 하고 있고, 제가

00:20:15.680 --> 00:20:16.740
당신이 어떤 패를

00:20:16.740 --> 00:20:18.820
가지고 있는지 분류하려고 한다고 해보겠습니다.

00:20:18.820 --> 00:20:20.820
당신이 하는 행동에 따라 가능한 패의 범위가 있고,

00:20:20.820 --> 00:20:21.680
제가 더 많은 정보를 얻을수록

00:20:21.680 --> 00:20:24.240
점점 더 “아, 이제 저는

00:20:24.240 --> 00:20:25.560
당신이 보여주는 걸 바탕으로 정확히 어떤 패를 가졌는지

00:20:25.560 --> 00:20:27.580
알겠다고 확신하게 됩니다”라는 쪽으로 수렴합니다.

00:20:27.580 --> 00:20:28.760
허세를 부리는 것일 수도 있지만, 그건 다른

00:20:28.760 --> 00:20:29.940
문제이고요, 하지만

00:20:29.940 --> 00:20:31.200
무슨 뜻인지 아시겠죠, 그러니까 저는

00:20:31.200 --> 00:20:32.240
그게

00:20:32.240 --> 00:20:34.087
표현의 궁극적인 관점이라고

00:20:34.087 --> 00:20:35.360
생각합니다만,

00:20:35.360 --> 00:20:37.760
그 자체가 일종의 “세계”인데, 그게

00:20:37.760 --> 00:20:38.800
너무 모호한지

00:20:38.800 --> 00:20:40.120
아니면 더 구체적인 유형의

00:20:40.120 --> 00:20:41.440
“세계”

00:20:41.440 --> 00:20:43.720
모델, 이를테면 비디오 생성 쪽 사람들이

00:20:43.720 --> 00:20:44.040
하고 있는 것과 비교하면 어떤지 모르겠습니다.

00:20:44.800 --> 00:20:46.900
그리고 또 한 가지로는,

00:20:46.900 --> 00:20:49.060
저도 탐색하고 있는데, 방금 말씀하신 것처럼

00:20:49.060 --> 00:20:50.440
깊은 모델이

00:20:50.440 --> 00:20:52.000
더 느리거나 더 비싸다는 점이요, 네, 그건

00:20:52.000 --> 00:20:53.560
그렇습니다.

00:20:53.560 --> 00:20:55.140
추론 쪽에서는 모델을 더 얕게 만드는

00:20:55.140 --> 00:20:55.780
경향이 있지요.

00:20:55.780 --> 00:20:58.880
그렇죠, 그래서 제가 생각해본 짧은

00:20:58.880 --> 00:20:59.840
캐치프레이즈가 있는데, “깊은

00:20:59.840 --> 00:21:00.800
교사”,

00:21:02.620 --> 00:21:04.780
“얕은 학생”이 좋은 배포

00:21:04.780 --> 00:21:06.940
패러다임이 될 수 있을지

00:21:06.940 --> 00:21:08.955
궁금합니다, 그러니까 최전선의

00:21:08.955 --> 00:21:10.060
역량을

00:21:10.060 --> 00:21:10.180
깊이로

00:21:10.180 --> 00:21:12.280
더 끌어올린 뒤, 다시 증류해 되돌리는 방식입니다.

00:21:12.280 --> 00:21:14.320
네, 사실 좋은 지적입니다, 그러니까

00:21:14.320 --> 00:21:14.880
저희

00:21:14.880 --> 00:21:17.040
웹사이트를 보시면, 이게 미래

00:21:17.040 --> 00:21:18.420
방향으로 맨 아래에 적어둔 것 중 하나입니다.

00:21:18.420 --> 00:21:21.160
아, 그렇군요, 네, 어, 저희는

00:21:21.160 --> 00:21:23.000
저희도, 우리가

00:21:23.000 --> 00:21:24.120
비슷한 성능을 낼 수 있을지 정말 보고 싶습니다, 그러니까 우리가

00:21:24.120 --> 00:21:25.240
그

00:21:25.240 --> 00:21:26.180
그러니까 저희가

00:21:26.180 --> 00:21:28.380
어, 최첨단 성능을 달성했습니다, 어,

00:21:28.380 --> 00:21:30.300
잭슨 CRL에서 목표-조건 강화학습에서, 꽤

00:21:30.300 --> 00:21:30.980
큰 폭으로요.

00:21:30.980 --> 00:21:32.520
그래서

00:21:32.520 --> 00:21:34.880
강화학습 에이전트를 훈련하는 능력의

00:21:34.880 --> 00:21:36.700
최전선이

00:21:36.700 --> 00:21:39.220
더 밀려 올라가는 걸 보는 게 매우 흥분되고, 우리가

00:21:39.220 --> 00:21:40.980
그걸 어떤 방식으로든

00:21:40.980 --> 00:21:42.560
표준적인

00:21:42.560 --> 00:21:44.740
네트워크만큼 효율적으로 할 수 있다면 정말

00:21:44.740 --> 00:21:45.910
멋질 것입니다, 왜냐하면

00:21:45.910 --> 00:21:47.080
훈련은

00:21:47.080 --> 00:21:48.220
반드시

00:21:48.220 --> 00:21:49.900
추론 시 배포하는 것과 같을 필요는

00:21:49.900 --> 00:21:51.460
없습니다, 무슨 말인지 아시겠죠.

00:21:51.460 --> 00:21:53.400
네, 그래서

00:21:53.400 --> 00:21:54.720
더 작은 모델로 증류한다든지

00:21:54.720 --> 00:21:56.140
모델을 가지치기한다든지 해서

00:21:56.140 --> 00:21:57.280
어쩌면 그러면서도

00:21:57.280 --> 00:21:58.550
성능을 유지할 수 있다면, 그것은 매우

00:21:58.550 --> 00:21:59.740
흥미로운 연구 방향입니다.

00:21:59.740 --> 00:22:00.740
그런데 저희가 택한 것 말고도 다른

00:22:00.740 --> 00:22:02.000
미래 방향들도 이야기해봅시다, 또 어떤 것들이

00:22:02.000 --> 00:22:03.260
있습니까.

00:22:03.260 --> 00:22:06.273
개인적으로 열정을 두는 분야는 무엇입니까, 네, 그러니까 어

00:22:06.273 --> 00:22:07.460
현재 저는

00:22:07.460 --> 00:22:07.860
추구하는

00:22:07.860 --> 00:22:09.768
방향이, 어, 스티칭(stitching)인

00:22:09.768 --> 00:22:11.080
강화학습입니다.

00:22:11.080 --> 00:22:13.480
그래서 저희는 일반화하려고 합니다.

00:22:14.400 --> 00:22:17.046
더 짧은 하위

00:22:17.046 --> 00:22:18.200
행동들로부터 강화학습을요, 그래서

00:22:18.200 --> 00:22:19.720
그것들이 테스트

00:22:19.720 --> 00:22:21.240
시점에 스티치되어 병합되도록 합니다.

00:22:21.240 --> 00:22:23.860
그리고 어, 네, 저는 이게

00:22:23.860 --> 00:22:26.100
제가 박사 과정에서 다룰

00:22:26.100 --> 00:22:28.360
마지막 논문들 중 하나가 될 것 같습니다, 개인적으로 저는

00:22:28.360 --> 00:22:30.760
정말 궁금합니다, 그러니까 우리는

00:22:30.760 --> 00:22:33.500
진짜로 무엇이 가능하고, 우리는

00:22:33.500 --> 00:22:35.240
얼마나 더 밀어붙일 수 있는지 궁금합니다.

00:22:35.240 --> 00:22:36.360
최전선을 가능한 한

00:22:36.360 --> 00:22:37.480
끝까지 전진시키는 데 관심이 있습니다.

00:22:37.480 --> 00:22:38.860
음, 그래서 실제로 저희

00:22:38.860 --> 00:22:39.940
논문을 보면 저희는

00:22:39.940 --> 00:22:42.140
깊이를 스케일링하는 데 초점을 맞추지만, 저희는 또한

00:22:42.140 --> 00:22:43.120
너비를 스케일링하는 것도 실제로 성능을 높인다는 것을

00:22:43.120 --> 00:22:44.100
관찰했습니다.

00:22:44.100 --> 00:22:46.360
그리고 또 저희가 발견한 것은, 실제로

00:22:46.360 --> 00:22:47.490
깊이를 스케일링하면 배치 사이즈까지

00:22:47.490 --> 00:22:48.620
스케일링할 수 있는 능력이

00:22:48.620 --> 00:22:49.100
배치

00:22:49.100 --> 00:22:51.280
사이즈 측면에서도 열리더라는 점입니다, 그래서 이것이 어

00:22:51.280 --> 00:22:53.420
네, 그중 하나이고요, 그러니까 이는 일종의

00:22:53.420 --> 00:22:55.760
공선적인 관계라는 뜻입니다, 네, 그러니까

00:22:55.760 --> 00:22:56.600
맥락을 위해 말씀드리면, 전통적인

00:22:56.600 --> 00:22:57.440
강화학습에서는

00:22:57.440 --> 00:22:59.700
가치 기반 강화학습에서 배치 사이즈를 키우는 것이

00:22:59.700 --> 00:23:00.180
그다지

00:23:00.180 --> 00:23:01.510
효과적이지 않습니다만, 저희가 보기에

00:23:01.510 --> 00:23:02.840
또

00:23:02.840 --> 00:23:04.100
딥러닝의 다른 분야들에서도

00:23:04.100 --> 00:23:04.520
배치 사이즈 스케일링에 관한 연구가 있는데,

00:23:04.520 --> 00:23:06.820
배치 사이즈를 키우는 것이 가장

00:23:06.820 --> 00:23:08.620
효과적인 경우는 충분히 큰

00:23:08.620 --> 00:23:09.710
네트워크 용량이 있어야만, 그

00:23:09.710 --> 00:23:10.800
증가한

00:23:10.800 --> 00:23:13.380
배치 사이즈를 활용할 수 있을 때라는 점을 보여줍니다, 그리고 저희도

00:23:13.380 --> 00:23:13.480
아시다시피

00:23:13.480 --> 00:23:15.420
그래서 하나의 가설로는, 그러니까

00:23:15.420 --> 00:23:16.560
배치 스케일링이

00:23:16.560 --> 00:23:17.700
그렇게

00:23:17.700 --> 00:23:18.220
효과적이지 않았던

00:23:18.220 --> 00:23:19.180
이유가 전통적인 강화학습에서는, 왜냐하면 저희가 계속

00:23:19.180 --> 00:23:20.140
사용해 온

00:23:20.140 --> 00:23:20.940
이런 작은 네트워크들이 그것을

00:23:20.940 --> 00:23:21.540
포착하지

00:23:21.540 --> 00:23:23.620
못했기 때문이라는 것입니다, 그리고 저희 실험 중 하나가

00:23:23.620 --> 00:23:25.640
바로, 저희가

00:23:25.640 --> 00:23:26.840
성공적으로 학습할 수 있게 되었기 때문에,

00:23:26.840 --> 00:23:26.960
즉

00:23:26.960 --> 00:23:29.140
깊은 네트워크를요, 저희는 실제로 할 수 있었고, 이것이

00:23:29.140 --> 00:23:30.760
아시다시피 아주 좋은 테스트베드가 됩니다,

00:23:30.760 --> 00:23:31.620
이

00:23:31.620 --> 00:23:33.800
가설을 시험해 보기 위한요, 그리고 저희는 실제로, 저희가

00:23:33.800 --> 00:23:35.030
네트워크 용량을 키울수록, 또한

00:23:35.030 --> 00:23:36.260
이

00:23:36.260 --> 00:23:36.480
다른

00:23:36.480 --> 00:23:38.960
배치 사이즈 스케일링의 차원도 열리더라는 것을 발견했습니다, 그래서 이런

00:23:38.960 --> 00:23:40.740
모든 이야기를 드리는 이유는, 저는 매우 궁금하다는 것입니다,

00:23:40.740 --> 00:23:42.700
예를 들어

00:23:42.700 --> 00:23:45.180
충분한 컴퓨트가 있는 누군가가 이런

00:23:45.180 --> 00:23:46.330
환경들에서 배치를 키우고, 어, 그리고

00:23:46.330 --> 00:23:47.480
또

00:23:47.480 --> 00:23:48.040
깊이를

00:23:48.040 --> 00:23:49.220
가능한 한 최대치까지 키우고, 또 그와 함께

00:23:49.220 --> 00:23:50.400
그리고 또

00:23:50.400 --> 00:23:51.680
배치 사이즈도 같이 키우면서, 그러면

00:23:51.680 --> 00:23:52.960
기본적으로

00:23:52.960 --> 00:23:53.300
그러니까

00:23:53.300 --> 00:23:55.080
언어 분야에서 우리가

00:23:55.080 --> 00:23:57.420
정말 많은 서로 다른 축으로 스케일링하듯이, 우리도

00:23:57.420 --> 00:23:57.900
또 다른

00:23:57.900 --> 00:23:58.960
스케일링의 차원들을 열 수 있을까요, 그리고 어떤

00:23:58.960 --> 00:24:00.020
역량이

00:24:00.020 --> 00:24:01.620
생기고, 우리는 어디까지 학습의 최전선을 밀어붙일 수 있을까요,

00:24:01.620 --> 00:24:02.440
즉

00:24:02.440 --> 00:24:04.320
이런 강화학습 에이전트들을 이전보다 더 잘

00:24:04.320 --> 00:24:05.880
학습시키는 쪽에서요, 션에게 넘기기 전에, 어, “충분한”

00:24:05.880 --> 00:24:07.060
컴퓨트라고 하실 때, 어떤

00:24:07.060 --> 00:24:09.140
컴퓨트 예산을 갖고 계셨나요, 그리고 그게

00:24:09.140 --> 00:24:11.020
어느 정도인지, 저는 그냥

00:24:11.020 --> 00:24:11.960
여러분이 어떤 자원을 쓰셨는지 보고 싶었습니다, 좋은 질문입니다.

00:24:11.960 --> 00:24:13.320
그래서 저희는 이 점을 확실히 하고 싶었습니다.

00:24:13.320 --> 00:24:15.380
그러니까 저희가 이걸 이렇게 만들고 싶었던 것은,

00:24:15.380 --> 00:24:16.720
말하자면, 어, 아시겠지만,

00:24:16.720 --> 00:24:19.120
꽤 접근하기 쉽게 하려는 것이었고, 좋은 점은,

00:24:19.120 --> 00:24:20.800
저희의 모든 실험이, 심지어

00:24:20.800 --> 00:24:21.200
1,000층짜리

00:24:21.200 --> 00:24:23.340
네트워크도 단 하나의 80

00:24:23.340 --> 00:24:24.295
GB H100 GPU 한 장에서 실행할 수 있다는 점입니다, 음, 그래서

00:24:24.295 --> 00:24:25.250
그게

00:24:25.250 --> 00:24:27.160
그 비용이요.

00:24:27.160 --> 00:24:28.340
네, 맞습니다, 맞습니다.

00:24:28.340 --> 00:24:29.960
네, 그래서 모든 것은 한

00:24:29.960 --> 00:24:32.100
GPU 한 장에서 돌릴 수 있습니다만, 이론적으로는 만약 저희가

00:24:32.100 --> 00:24:33.080
아시다시피 분산

00:24:33.080 --> 00:24:34.290
학습 셋업을 갖추고, 그냥

00:24:34.290 --> 00:24:35.500
연산을

00:24:35.500 --> 00:24:36.380
여기에 마구 쏟아붓고 정말로

00:24:36.380 --> 00:24:37.260
더

00:24:37.260 --> 00:24:37.680
최전선을

00:24:37.680 --> 00:24:39.600
정말로 더 밀어붙인다면, 어떻게 되는지 보는 것이 매우 흥미로울 것입니다.

00:24:39.600 --> 00:24:42.160
네, 좋습니다, 그리고 저는 요즘 적극적으로

00:24:42.160 --> 00:24:42.480
가능한 한 많이 배우려고

00:24:42.480 --> 00:24:43.880
비전-언어-액션

00:24:43.880 --> 00:24:45.470
모델들에 대해, 어, 뉴립스에서의 그런 모델들도요, 그리고

00:24:45.470 --> 00:24:47.060
그리고

00:24:47.060 --> 00:24:47.440
여러

00:24:47.440 --> 00:24:49.114
머신-언어-액션 모델, 비전

00:24:49.114 --> 00:24:50.200
언어-액션 모델, 비전-언어

00:24:50.200 --> 00:24:53.200
네, 음, 그리고 저는 궁금합니다.

00:24:53.200 --> 00:24:55.414
이런 것들에서 표현이 어떻게 적용되는지요.

00:24:55.414 --> 00:24:56.300
네, 맞습니다, 특히

00:24:56.300 --> 00:24:58.040
로보틱스에서요, 음, 그쪽을 더

00:24:58.040 --> 00:25:00.300
적극적으로 탐색하려고 해서, 그냥

00:25:00.300 --> 00:25:01.100
문헌을 많이 읽고, 가능한 한 많은

00:25:01.100 --> 00:25:01.820
사람들과 이야기하고 있습니다.

00:25:01.820 --> 00:25:04.100
네, 저희가 방금

00:25:04.100 --> 00:25:05.790
제너럴 인튜이션과의 에피소드를 공개했는데요, 아, 알겠습니다.

00:25:05.790 --> 00:25:07.480
음.

00:25:07.480 --> 00:25:09.480
좋습니다, 그리고 만약 그들의

00:25:09.480 --> 00:25:10.180
역사에 대해 조금 알고 계시다면,

00:25:10.180 --> 00:25:11.670
원래는 게임 클리핑 회사로 시작했고

00:25:11.670 --> 00:25:13.160
어,

00:25:13.160 --> 00:25:14.110
기본적으로 비전-언어

00:25:14.110 --> 00:25:15.060
액션 모델이 있습니다.

00:25:15.060 --> 00:25:16.120
네, 그리고 그게

00:25:16.120 --> 00:25:17.680
음, 저도, 저도, 저도

00:25:17.680 --> 00:25:20.120
미리보기를 봤는데 매우 인상적이었고, 저는

00:25:20.120 --> 00:25:22.480
그게 정확히 얼마나 잘

00:25:22.480 --> 00:25:25.340
체화된 사용 사례로 전이될지는 잘 모르겠지만, 꼭 그럴 필요는

00:25:25.340 --> 00:25:29.340
없고, 화면 기반이면 충분합니다, 네.

00:25:29.340 --> 00:25:30.300
저도 잘 모르겠는데, 혹시

00:25:30.300 --> 00:25:32.140
의견이 있으신가요, 네, 정말 흥미로운

00:25:32.140 --> 00:25:33.640
연구 방향입니다, 확실히요, 네, 저는

00:25:33.640 --> 00:25:35.140
생각하기에, 음

00:25:35.140 --> 00:25:37.580
그, 그, 그,

00:25:37.580 --> 00:25:38.660
액션을 어떤 것으로

00:25:38.660 --> 00:25:39.740
즉

00:25:39.740 --> 00:25:42.340
출력한다는 개념은 사실 그렇게

00:25:42.340 --> 00:25:44.940
인기가

00:25:44.940 --> 00:25:45.640
산업계에서는

00:25:45.640 --> 00:25:48.093
별로 없습니다, 네, 네, 그건 텍스트가

00:25:48.093 --> 00:25:49.580
지난 3년 동안 완전히

00:25:49.580 --> 00:25:51.880
지배해 왔고, 툴 콜링도

00:25:51.880 --> 00:25:54.320
있는데, 그것도 또 다른 형태의

00:25:54.320 --> 00:25:56.860
구조화된 텍스트일 뿐이라서요, 그리고 저는

00:25:56.860 --> 00:25:59.100
액션 연구가

00:25:59.100 --> 00:26:00.920
좀, 저는 어떻게

00:26:00.920 --> 00:26:02.280
뭐가 일어나야 하는지 잘 모르겠고

00:26:02.280 --> 00:26:05.000
다음 단계가 열리려면

00:26:05.000 --> 00:26:06.420
그 부분을요, 혹시

00:26:06.420 --> 00:26:07.640
여기서 흥미로운 게 있으면 말씀해 주실 수 있습니까.

00:26:07.640 --> 00:26:08.860
네.

00:26:08.860 --> 00:26:09.940
멋진 연구가 많이 있습니다.

00:26:09.940 --> 00:26:11.750
예를 들어 사전학습된 VLM을 활용해서

00:26:11.750 --> 00:26:13.560
그걸

00:26:13.560 --> 00:26:14.920
동결한 다음 적용합니다, 아.

00:26:14.920 --> 00:26:15.720
네, 그리고 나서

00:26:15.720 --> 00:26:17.260
그 위에다 일종의

00:26:17.260 --> 00:26:19.110
액션을 출력하는 전문가를 얹습니다, 음, 또

00:26:19.110 --> 00:26:20.960
시스템으로는

00:26:20.960 --> 00:26:22.400
예를 들어

00:26:22.400 --> 00:26:24.224
계층적 계획을 세우고, 아마

00:26:24.224 --> 00:26:25.280
더 상위 수준의 계획을 출력하는

00:26:25.280 --> 00:26:26.860
방식도 있는데, 이건 더 큰 네트워크라서

00:26:26.860 --> 00:26:29.260
추론에 시간이 오래 걸리거나, 조금

00:26:29.260 --> 00:26:30.840
더 오래 걸리기 때문에, 그래서

00:26:30.840 --> 00:26:31.980
계획을 더 낮은

00:26:31.980 --> 00:26:34.080
빈도로, 어떤 청크 단위로 출력하고, 그리고

00:26:34.080 --> 00:26:36.080
거기서부터는 어떤, 어

00:26:36.080 --> 00:26:37.060
두 번째 시스템이

00:26:37.060 --> 00:26:38.360
조금 더 빠르게 동작합니다, 그리고 저는

00:26:38.360 --> 00:26:39.220
그 방향에 흥미로운 연구가 꽤 많다고

00:26:39.220 --> 00:26:40.080
생각합니다.

00:26:40.080 --> 00:26:40.660
그래서

00:26:40.660 --> 00:26:42.260
저는 그 방향을 기대하고 있습니다, 좋습니다.

00:26:42.260 --> 00:26:43.860
마지막

00:26:43.860 --> 00:26:45.210
질문입니다, 어, 포스터 세션에서

00:26:45.210 --> 00:26:46.560
받으신

00:26:46.560 --> 00:26:47.770
가장 어려운 질문이나, 혹은 가장 인상적인

00:26:47.770 --> 00:26:48.980
만남이 있었나요, 혹시

00:26:48.980 --> 00:26:51.540
유명한 분을 만나셨다든지요, 저는 사실 아직

00:26:51.540 --> 00:26:51.900
그렇게

00:26:51.900 --> 00:26:53.320
학회장을 많이 둘러볼 기회가 없었고요.

00:26:53.320 --> 00:26:55.100
지금은 실제로 풀타임으로 일하고 있어서요, 아, 그렇군요.

00:26:55.100 --> 00:26:57.580
네, 어, 그래서

00:26:57.580 --> 00:26:59.620
지금까지는, 저는 사실 정말로 방금

00:26:59.620 --> 00:27:02.040
세션 직전에 배지를 받았습니다.

00:27:02.040 --> 00:27:03.280
그래서 아마 저는

00:27:03.280 --> 00:27:05.100
그 질문에 답하기에 적절하지 않을 것 같습니다, 아니요.

00:27:05.100 --> 00:27:06.840
아니요 아니요, 그러니까 사람들이

00:27:06.840 --> 00:27:08.000
이런저런 걸 묻기도 하잖아요, 아, 그렇지요.

00:27:08.000 --> 00:27:10.360
그러면 사람들이 질문한다든지, 혹은

00:27:10.360 --> 00:27:13.020
만나서, 아시다시피, 그냥 그냥

00:27:13.020 --> 00:27:13.760
분위기만

00:27:13.760 --> 00:27:14.990
사람들이 뭐라고 하는지요, 네.

00:27:14.990 --> 00:27:16.220
사람들은

00:27:16.220 --> 00:27:18.920
대체로, 저는 약간

00:27:18.920 --> 00:27:20.180
눈이 확 트이는

00:27:20.180 --> 00:27:21.740
반응이 많았던 것 같습니다, 전반적으로는

00:27:21.740 --> 00:27:23.660
많은 분들이 이 논문이 정말 눈이 확 트인다고

00:27:23.660 --> 00:27:24.220
생각하셨는데, 왜냐하면

00:27:24.220 --> 00:27:25.520
목표 함수가 꽤 단순하고, 상당히

00:27:25.520 --> 00:27:26.820
우아하기 때문입니다.

00:27:26.820 --> 00:27:29.180
그리고 저희가 이렇게

00:27:29.180 --> 00:27:30.880
아시다시피, 저는

00:27:30.880 --> 00:27:32.540
뒤집었다고까지는 말하고 싶진 않지만, 어느 정도

00:27:32.540 --> 00:27:33.850
즉 강화학습은

00:27:33.850 --> 00:27:35.160
그렇게

00:27:35.160 --> 00:27:35.540
잘

00:27:35.540 --> 00:27:38.280
스케일되지 않는다는 통념을요, 이를

00:27:38.280 --> 00:27:39.710
1,000층까지 밀어붙였고, 계속

00:27:39.710 --> 00:27:41.140
개선되는

00:27:41.140 --> 00:27:41.880
성능을 봤다는 점입니다, 저는

00:27:41.880 --> 00:27:43.080
제가 받은 전반적인 인상은

00:27:43.080 --> 00:27:44.280
그렇습니다.

00:27:44.280 --> 00:27:47.320
아시다시피, 이것이 정말로

00:27:47.320 --> 00:27:47.600
매우

00:27:47.600 --> 00:27:49.780
멋진 일이 될 수 있고, 우리가 이 방향으로 계속 구축해

00:27:49.780 --> 00:27:52.180
나갈 수 있다면, 우리는

00:27:52.180 --> 00:27:52.800
정말로 스케일링을

00:27:52.800 --> 00:27:53.660
이런 다양한 차원들로 할 수 있고, 또

00:27:53.660 --> 00:27:54.520
최전선을

00:27:54.520 --> 00:27:56.560
강화학습의 역량 측면에서 더 밀어붙일 수 있어서, 저는 매우

00:27:56.560 --> 00:27:57.060
궁금합니다.

00:27:57.060 --> 00:27:58.600
어떻게 될지 지켜보겠습니다.

00:27:58.600 --> 00:28:00.620
들러 주셔서 감사하고, 논문도 다시 축하드립니다.

00:28:00.620 --> 00:28:00.840
다시 한 번요.

00:28:01.380 --> 00:28:03.060
그리고 앞으로의 연구도 행운을 빕니다.

00:28:03.060 --> 00:28:04.360
감사합니다, 초대해 주셔서 감사합니다, 네.
