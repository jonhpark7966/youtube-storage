WEBVTT

00:00:12.060 --> 00:00:14.280
welcome to lane space uh we are basically

00:00:14.280 --> 00:00:17.360
trying to provide the best optimal sort of

00:00:17.360 --> 00:00:17.760
podcast

00:00:17.760 --> 00:00:18.720
experience of europe's for people who are

00:00:18.720 --> 00:00:19.680
not

00:00:19.680 --> 00:00:22.160
here uh and congrats on your paper how's

00:00:22.160 --> 00:00:22.400
it feel

00:00:22.400 --> 00:00:25.120
yeah it was very exciting um yeah we

00:00:25.120 --> 00:00:26.150
had a poster yesterday and then today

00:00:26.150 --> 00:00:27.180
we'll

00:00:27.180 --> 00:00:27.900
have an oral talk

00:00:27.900 --> 00:00:30.060
were you just like mobbed oh yeah there

00:00:30.060 --> 00:00:31.100
was a lot of people it's like three

00:00:31.100 --> 00:00:31.860
hours straight of like

00:00:31.860 --> 00:00:33.480
you know like waves of people to like

00:00:33.480 --> 00:00:35.660
that where we were trying to stupid so

00:00:35.660 --> 00:00:36.540
i've never received the

00:00:36.540 --> 00:00:38.840
best paper did you just find out on

00:00:38.840 --> 00:00:40.860
the website like what uh oh i just

00:00:40.860 --> 00:00:42.940
like woke up one day and

00:00:42.940 --> 00:00:45.400
like checked my email and then ah they

00:00:45.400 --> 00:00:47.000
just thought yeah they was like oh like

00:00:47.000 --> 00:00:48.040
that's like i just saw

00:00:48.040 --> 00:00:49.310
you know you've like been awarded best

00:00:49.310 --> 00:00:50.580
paper

00:00:50.580 --> 00:00:53.140
but maybe you know from the reviews as

00:00:53.140 --> 00:00:53.500
well right

00:00:53.500 --> 00:00:55.700
so yeah we know from the reviews that

00:00:55.700 --> 00:00:58.240
we did well um but there's a difference

00:00:58.240 --> 00:00:59.020
between like doing well

00:00:59.020 --> 00:01:01.260
on the reviews and getting best paper so

00:01:01.260 --> 00:01:02.290
right part we didn't actually know yeah

00:01:02.290 --> 00:01:03.320
okay

00:01:03.320 --> 00:01:04.400
so i i skipped a

00:01:04.400 --> 00:01:05.720
little bit uh maybe we can go sort

00:01:05.720 --> 00:01:07.480
of um one by one and and sort

00:01:07.480 --> 00:01:09.460
of introduce um you know who you are

00:01:09.460 --> 00:01:10.800
and what you did on on on the

00:01:10.800 --> 00:01:14.040
team i'm kevin i was an undergrad from

00:01:14.040 --> 00:01:15.440
from princeton and i just graduated

00:01:15.440 --> 00:01:19.040
and yeah i guess i led the project

00:01:19.040 --> 00:01:21.300
like started the project and then i was

00:01:21.300 --> 00:01:21.920
very happy to collaborate

00:01:21.920 --> 00:01:24.040
with ishan and nicole and ben also um

00:01:24.040 --> 00:01:26.100
right and were you in like the same

00:01:26.100 --> 00:01:27.080
research group like how

00:01:27.080 --> 00:01:29.180
do you how that's your yeah uh social

00:01:29.180 --> 00:01:31.680
context so so yeah so we're all from

00:01:31.680 --> 00:01:33.800
princeton yeah um thanks

00:01:33.800 --> 00:01:35.840
to alan for booking you guys so this

00:01:35.840 --> 00:01:36.880
project actually started from like an iw

00:01:36.880 --> 00:01:37.920
seminar

00:01:37.920 --> 00:01:39.340
so uh like like

00:01:39.340 --> 00:01:40.600
independent work research seminar that ben

00:01:40.600 --> 00:01:41.860
was teaching

00:01:41.860 --> 00:01:44.100
and this was like actually like like one

00:01:44.100 --> 00:01:45.410
of my first experiences in like ml

00:01:45.410 --> 00:01:46.720
research

00:01:46.720 --> 00:01:48.520
um so it was really valuable to like

00:01:48.520 --> 00:01:49.160
get that experience

00:01:49.160 --> 00:01:51.700
and then ishan was also in that seminar

00:01:51.700 --> 00:01:52.740
and working on adjacent things so we

00:01:52.740 --> 00:01:53.780
collaborated

00:01:53.780 --> 00:01:56.240
um a lot during that seminar and then

00:01:56.240 --> 00:01:58.300
yeah the project turned out to have some

00:01:58.300 --> 00:01:59.020
pretty cool results

00:01:59.020 --> 00:02:00.540
and then later on also like the halt

00:02:00.540 --> 00:02:02.980
working on sort of similar things also um

00:02:02.980 --> 00:02:03.840
joined it on the project

00:02:03.840 --> 00:02:05.100
and became like a good collaboration yeah

00:02:05.100 --> 00:02:06.360
and

00:02:06.360 --> 00:02:07.860
um i i don't know if any of

00:02:07.860 --> 00:02:08.640
you guys want to want to

00:02:08.640 --> 00:02:10.500
chime in and on like other elements of

00:02:10.500 --> 00:02:12.350
coming into like deciding on this uh

00:02:12.350 --> 00:02:14.200
problem

00:02:14.200 --> 00:02:15.240
so it's like

00:02:15.240 --> 00:02:16.764
probably my lab works on deep

00:02:16.764 --> 00:02:17.920
reinforcement learning

00:02:17.920 --> 00:02:19.720
but historically deep meant like two or

00:02:19.720 --> 00:02:21.520
three

00:02:21.520 --> 00:02:21.820
or

00:02:21.820 --> 00:02:24.040
four layers not one thousand when kevin

00:02:24.040 --> 00:02:26.260
and

00:02:26.260 --> 00:02:27.060
sean mentioned they want to try really

00:02:27.060 --> 00:02:27.580
deep

00:02:27.580 --> 00:02:28.020
networks

00:02:28.020 --> 00:02:29.880
it's kind of skeptical it was going to

00:02:29.880 --> 00:02:30.790
work i've tried this before it doesn't

00:02:30.790 --> 00:02:31.700
work

00:02:31.700 --> 00:02:32.280
other papers have

00:02:32.280 --> 00:02:33.080
tried this before and it doesn't gonna

00:02:33.080 --> 00:02:33.840
work

00:02:33.840 --> 00:02:36.480
so i was very very skeptical starting out

00:02:36.480 --> 00:02:36.820
i don't know

00:02:36.820 --> 00:02:38.600
if i conveyed this at the time but

00:02:38.600 --> 00:02:41.360
that was my prior going in because but

00:02:41.360 --> 00:02:42.760
do you view your job as like

00:02:42.760 --> 00:02:44.010
screening or like hey guys this is

00:02:44.010 --> 00:02:45.260
probably

00:02:45.260 --> 00:02:46.060
isn't gonna work you should try a

00:02:46.060 --> 00:02:46.360
different

00:02:46.360 --> 00:02:46.840
idea you

00:02:46.840 --> 00:02:47.890
know like or should you be encouraging

00:02:47.890 --> 00:02:48.940
even

00:02:48.940 --> 00:02:53.100
if it's dumb it's selecting bets yeah and

00:02:53.100 --> 00:02:53.660
this was a

00:02:53.660 --> 00:02:55.380
bet i was willing to make what what

00:02:55.380 --> 00:02:57.980
made you willing to make a bet it

00:02:57.980 --> 00:03:00.120
seemed relatively low cost uh in

00:03:00.120 --> 00:03:03.540
that we mihal in particular had spent the

00:03:03.540 --> 00:03:04.480
past year developing infrastructure that

00:03:04.480 --> 00:03:05.420
made a lot

00:03:05.420 --> 00:03:05.700
easier

00:03:05.700 --> 00:03:08.760
to run some of these experiments and the

00:03:08.760 --> 00:03:09.920
precedent was deep renowers should do a

00:03:09.920 --> 00:03:11.080
whole

00:03:11.080 --> 00:03:11.960
lot better like

00:03:11.960 --> 00:03:13.170
that's what the deep learning revolution

00:03:13.170 --> 00:03:14.380
has been

00:03:14.380 --> 00:03:15.620
over the last day yeah i know why

00:03:15.620 --> 00:03:16.280
do we stop making

00:03:16.280 --> 00:03:17.620
them deeper and reinforcement learning was

00:03:17.620 --> 00:03:18.960
like this

00:03:18.960 --> 00:03:21.280
one anomaly where we continue to use these

00:03:21.280 --> 00:03:21.580
really

00:03:21.580 --> 00:03:22.710
shallow networks and that's particularly

00:03:22.710 --> 00:03:23.840
true in the

00:03:23.840 --> 00:03:24.680
settings that we were looking at where

00:03:24.680 --> 00:03:25.520
you're

00:03:25.520 --> 00:03:26.730
starting from scratch you're starting from

00:03:26.730 --> 00:03:27.940
nothing any

00:03:27.940 --> 00:03:28.800
other perspectives you guys want to chime

00:03:28.800 --> 00:03:29.660
in

00:03:29.660 --> 00:03:31.600
with i guess maybe i should just go

00:03:31.600 --> 00:03:34.040
over like an overview of our project yes

00:03:34.040 --> 00:03:36.220
okay sorry yes so the way that

00:03:36.220 --> 00:03:38.160
kind of view our project um is that

00:03:38.160 --> 00:03:39.780
if you look at the landscape of deep

00:03:39.780 --> 00:03:41.000
learning you know you have

00:03:41.000 --> 00:03:44.520
nlp like language vision and then rl and

00:03:44.520 --> 00:03:46.200
like as ben kind of alluded to you

00:03:46.200 --> 00:03:47.540
know like in in language

00:03:47.540 --> 00:03:49.760
in vision we've sort of converged to these

00:03:49.760 --> 00:03:50.740
like paradigms of scaling to massive

00:03:50.740 --> 00:03:51.720
networks right

00:03:51.720 --> 00:03:51.860
like

00:03:51.860 --> 00:03:52.660
hundreds of billions of parameters

00:03:52.660 --> 00:03:53.440
trillions of parameters

00:03:53.440 --> 00:03:56.320
and there's been you know a lot a

00:03:56.320 --> 00:03:56.620
lot

00:03:56.620 --> 00:03:57.420
gained from in deep learning from from

00:03:57.420 --> 00:03:58.100
that

00:03:58.100 --> 00:04:00.000
right and then but then it seems like

00:04:00.000 --> 00:04:00.960
in the third sort of

00:04:00.960 --> 00:04:03.360
branch of deep learning in deep rl that

00:04:03.360 --> 00:04:05.060
has not yet been the case um like

00:04:05.060 --> 00:04:06.040
i was very surprised like

00:04:06.040 --> 00:04:08.340
coming into some like you know ben's class

00:04:08.340 --> 00:04:09.620
and seminar when i was looking at the

00:04:09.620 --> 00:04:10.440
networks oh why

00:04:10.440 --> 00:04:11.680
were you just using like a simple like

00:04:11.680 --> 00:04:14.000
two-layer mlp for like these frontier sort

00:04:14.000 --> 00:04:14.980
of you know state

00:04:14.980 --> 00:04:17.180
of the rl algorithms um and so i

00:04:17.180 --> 00:04:20.340
was very curious like can we design rl

00:04:20.340 --> 00:04:21.760
algorithms can we sort of put

00:04:21.760 --> 00:04:23.820
together a recipe for rl that can allow

00:04:23.820 --> 00:04:25.420
it to scale in potentially you know

00:04:25.420 --> 00:04:27.020
analogous

00:04:27.020 --> 00:04:27.420
ways that

00:04:27.420 --> 00:04:28.570
language envisioned my skill and so what

00:04:28.570 --> 00:04:29.720
we

00:04:29.720 --> 00:04:32.360
did is that we know that traditional rl

00:04:32.360 --> 00:04:32.800
like let's say

00:04:32.800 --> 00:04:34.050
like value value-based rl doesn't really

00:04:34.050 --> 00:04:35.300
scale

00:04:35.300 --> 00:04:36.100
right this is pretty clear from the

00:04:36.100 --> 00:04:36.580
literature

00:04:36.580 --> 00:04:38.340
so we tried a different approach of rl

00:04:38.340 --> 00:04:39.920
um called self-supervised rl where instead

00:04:39.920 --> 00:04:41.500
of

00:04:41.500 --> 00:04:41.980
learning like a

00:04:41.980 --> 00:04:43.552
value function we're learning

00:04:43.552 --> 00:04:45.340
representations of states actions

00:04:45.340 --> 00:04:47.260
and future states such that the

00:04:47.260 --> 00:04:49.091
representations along the same trajectory

00:04:49.091 --> 00:04:49.940
are pushed together

00:04:49.940 --> 00:04:50.860
the representations along different

00:04:50.860 --> 00:04:51.690
trajectories are pushed apart and this is

00:04:51.690 --> 00:04:52.520
just

00:04:52.520 --> 00:04:55.520
like a different approach to uh rl that

00:04:55.520 --> 00:04:56.360
allows us to

00:04:56.360 --> 00:04:57.260
learn in a self-supervised manner so

00:04:57.260 --> 00:04:58.160
there's

00:04:58.160 --> 00:05:00.700
we can solve task reach goals without any

00:05:00.700 --> 00:05:01.060
human

00:05:01.060 --> 00:05:03.420
crafted reward single and so we know that

00:05:03.420 --> 00:05:04.320
self-supervised learning is scalable in

00:05:04.320 --> 00:05:05.220
these

00:05:05.220 --> 00:05:06.330
different areas and if deep learning so

00:05:06.330 --> 00:05:07.440
can

00:05:07.440 --> 00:05:09.210
self-supervised rl scale in similar ways

00:05:09.210 --> 00:05:10.980
when

00:05:10.980 --> 00:05:11.080
we

00:05:11.080 --> 00:05:11.880
first tried it it actually didn't work

00:05:11.880 --> 00:05:12.680
like

00:05:12.680 --> 00:05:14.448
we've made the networks deeper the

00:05:14.448 --> 00:05:15.280
performance like

00:05:15.280 --> 00:05:15.540
totally

00:05:15.540 --> 00:05:17.880
degraded but then we also but then i

00:05:17.880 --> 00:05:19.220
separately was like there's also some

00:05:19.220 --> 00:05:20.560
other work

00:05:20.560 --> 00:05:22.000
like um in in

00:05:22.000 --> 00:05:23.180
our literature like we tried like residual

00:05:23.180 --> 00:05:24.360
connections

00:05:24.360 --> 00:05:26.340
uh and it is other a few other

00:05:26.340 --> 00:05:27.190
architectural components that we had to

00:05:27.190 --> 00:05:28.040
put into

00:05:28.040 --> 00:05:30.000
the recipe and then all of a sudden

00:05:30.000 --> 00:05:30.880
like one day

00:05:30.880 --> 00:05:33.340
like i ran this experiment and there was

00:05:33.340 --> 00:05:34.140
like this one environment in which there

00:05:34.140 --> 00:05:34.880
was

00:05:34.880 --> 00:05:35.060
like

00:05:35.060 --> 00:05:36.160
like going from like like doubling the

00:05:36.160 --> 00:05:37.260
depth

00:05:37.260 --> 00:05:38.260
didn't really do anything but like

00:05:38.260 --> 00:05:39.260
doubling the

00:05:39.260 --> 00:05:39.720
depth again

00:05:40.160 --> 00:05:42.336
with these different components suddenly

00:05:42.336 --> 00:05:43.860
like skyrocketed performance

00:05:43.860 --> 00:05:44.980
in this one environment

00:05:44.980 --> 00:05:47.760
getting this to work was very non-trivial

00:05:47.760 --> 00:05:49.100
in the sense that like usually when we

00:05:49.100 --> 00:05:49.700
think about doing

00:05:49.700 --> 00:05:51.887
hyperparameter optimization we try

00:05:51.887 --> 00:05:52.980
changing a see if

00:05:52.980 --> 00:05:54.840
it makes it better try changing b see

00:05:54.840 --> 00:05:55.100
whether

00:05:55.100 --> 00:05:57.260
it makes it better and if we just

00:05:57.260 --> 00:05:59.520
made the depth bigger makes it worse we

00:05:59.520 --> 00:06:00.480
just had residual connections

00:06:00.480 --> 00:06:02.060
didn't make it better and it was really

00:06:02.060 --> 00:06:03.300
this combination of factors that kevin and

00:06:03.300 --> 00:06:04.540
sean

00:06:04.540 --> 00:06:04.980
figured

00:06:04.980 --> 00:06:07.440
out that really made this work and as

00:06:07.440 --> 00:06:08.920
a precursor to that we also try scaling

00:06:08.920 --> 00:06:09.820
along different dimensions

00:06:09.820 --> 00:06:12.660
so scaling the batch size uh scaling the

00:06:12.660 --> 00:06:13.900
the width of the network so the hidden

00:06:13.900 --> 00:06:15.140
layers in effect

00:06:15.800 --> 00:06:17.500
yeah pretty much kind of similar to just

00:06:17.500 --> 00:06:18.940
scaling depth naively yeah um and then

00:06:18.940 --> 00:06:20.380
once

00:06:20.380 --> 00:06:20.680
we started

00:06:20.680 --> 00:06:22.371
introducing residual connections layer

00:06:22.371 --> 00:06:23.840
norm these specific architectural

00:06:23.840 --> 00:06:25.400
choices that's when we saw

00:06:25.400 --> 00:06:27.413
these significant jumps in performance

00:06:27.413 --> 00:06:28.420
like these critical

00:06:28.420 --> 00:06:29.800
depths at which performance multiplies by

00:06:29.800 --> 00:06:31.180
a

00:06:31.180 --> 00:06:32.290
pretty huge factor and that's where we

00:06:32.290 --> 00:06:33.400
really

00:06:33.400 --> 00:06:35.378
noticed like unlocking some significant

00:06:35.378 --> 00:06:36.240
performance gains

00:06:36.900 --> 00:06:38.270
as opposed to scaling just along with

00:06:38.270 --> 00:06:39.640
which

00:06:39.640 --> 00:06:41.110
did yield some performance improvements um

00:06:41.110 --> 00:06:42.580
but when

00:06:42.580 --> 00:06:42.700
you

00:06:42.700 --> 00:06:44.220
look at the number of parameters that your

00:06:44.220 --> 00:06:46.740
network has as you grow with it's roughly

00:06:46.740 --> 00:06:47.660
a quadratic as

00:06:47.660 --> 00:06:48.560
opposed to something like growing depth so

00:06:48.560 --> 00:06:49.460
it's

00:06:49.460 --> 00:06:50.430
more in some sense it's more parameter

00:06:50.430 --> 00:06:51.400
efficient

00:06:51.400 --> 00:06:51.700
also

00:06:51.700 --> 00:06:52.870
more sample efficient from the experiments

00:06:52.870 --> 00:06:54.040
that we

00:06:54.040 --> 00:06:56.900
conducted nice um in some ways you're sort

00:06:56.900 --> 00:06:57.020
of

00:06:57.020 --> 00:06:59.480
replicating stuff that is seen in the wild

00:06:59.480 --> 00:07:02.140
but on on a very small model that

00:07:02.140 --> 00:07:03.660
you can study is that would

00:07:03.660 --> 00:07:04.720
you would you say that's yeah so i

00:07:04.720 --> 00:07:06.260
kind of add to what kevin said earlier

00:07:06.260 --> 00:07:07.760
we saw these huge performance

00:07:07.760 --> 00:07:09.928
improvements in language models image

00:07:09.928 --> 00:07:11.100
generation models by

00:07:11.100 --> 00:07:12.440
making them larger making them deeper

00:07:12.440 --> 00:07:13.720
which seems very intuitive yeah and so

00:07:13.720 --> 00:07:15.000
that's

00:07:15.000 --> 00:07:17.286
why our work we draw from like

00:07:17.286 --> 00:07:18.200
foundational

00:07:18.200 --> 00:07:18.700
research

00:07:18.700 --> 00:07:20.820
right like uh residual networks which

00:07:20.820 --> 00:07:21.680
employ residual

00:07:21.680 --> 00:07:23.875
connections to avoid uh vanishing

00:07:23.875 --> 00:07:24.740
gradients and

00:07:24.740 --> 00:07:25.800
that's something that we show in some of

00:07:25.800 --> 00:07:27.660
our ablations in our in our paper further

00:07:27.660 --> 00:07:28.000
down

00:07:28.000 --> 00:07:29.050
it's probably in the appendices where you

00:07:29.050 --> 00:07:30.100
did

00:07:30.100 --> 00:07:31.110
experiments without these residual

00:07:31.110 --> 00:07:32.120
connections

00:07:32.120 --> 00:07:33.270
and so it's sort of borrowing these

00:07:33.270 --> 00:07:34.420
concepts

00:07:34.420 --> 00:07:35.750
that have existed in other fields and

00:07:35.750 --> 00:07:37.080
applying

00:07:37.080 --> 00:07:37.260
them

00:07:37.260 --> 00:07:39.560
to this setting with uh rl and showing

00:07:39.560 --> 00:07:41.840
that it works before ben has to have

00:07:41.840 --> 00:07:43.040
to go i'll leave the sort

00:07:43.040 --> 00:07:45.400
of last word uh to him what additional

00:07:45.400 --> 00:07:47.360
work does this inspire that like that you

00:07:47.360 --> 00:07:49.560
want to push on next i think

00:07:49.560 --> 00:07:50.360
there's one thing i'd clarify about the

00:07:50.360 --> 00:07:51.020
paper

00:07:51.020 --> 00:07:52.330
and then i'll directly answer the question

00:07:52.330 --> 00:07:53.640
yes

00:07:53.640 --> 00:07:53.880
i think

00:07:53.880 --> 00:07:55.240
the thing i might clarify about the paper

00:07:55.240 --> 00:07:56.200
is i think a lot of people reading

00:07:56.200 --> 00:07:57.800
the title are like wow big networks

00:07:57.800 --> 00:07:58.840
they're great i'll take big networks you

00:07:58.840 --> 00:07:59.880
solved

00:07:59.880 --> 00:08:01.240
it now we can just go yeah just

00:08:01.240 --> 00:08:01.700
take big networks

00:08:01.700 --> 00:08:03.900
add them to ppo add them to sac

00:08:03.900 --> 00:08:04.890
add them to your favorite reinforcement

00:08:04.890 --> 00:08:05.880
learning algorithm

00:08:05.880 --> 00:08:06.560
but i think

00:08:06.560 --> 00:08:07.360
that's actually not the main conclusion i

00:08:07.360 --> 00:08:07.980
think

00:08:07.980 --> 00:08:09.130
the main conclusion is that using big

00:08:09.130 --> 00:08:10.280
networks

00:08:10.280 --> 00:08:11.060
not only

00:08:11.060 --> 00:08:13.475
requires these architectural tricks but

00:08:13.475 --> 00:08:14.280
also as kevin

00:08:14.280 --> 00:08:15.260
mentioned before it requires using a

00:08:15.260 --> 00:08:16.240
different

00:08:16.240 --> 00:08:19.117
objective this objective doesn't actually

00:08:19.117 --> 00:08:20.100
use rewards in

00:08:20.100 --> 00:08:22.280
it and so there's another word in the

00:08:22.280 --> 00:08:23.670
title reinforcement learning that also

00:08:23.670 --> 00:08:25.060
might be a

00:08:25.060 --> 00:08:27.460
little bit of a misnomer because we aren't

00:08:27.460 --> 00:08:28.880
directly trying to maximize rewards our

00:08:28.880 --> 00:08:30.300
code doesn't

00:08:30.300 --> 00:08:31.440
have a line of code saying maximize

00:08:31.440 --> 00:08:32.580
rewards

00:08:32.580 --> 00:08:35.400
here and so is at the end of

00:08:35.400 --> 00:08:36.260
the day this a reinforcement learning

00:08:36.260 --> 00:08:37.120
method i

00:08:37.120 --> 00:08:37.440
don't know

00:08:37.440 --> 00:08:39.460
it looks much more similar to the self

00:08:39.460 --> 00:08:40.770
-supervised methods in other areas of

00:08:40.770 --> 00:08:42.080
machine learning

00:08:42.080 --> 00:08:43.060
and so i

00:08:43.060 --> 00:08:44.250
think that the method the work really

00:08:44.250 --> 00:08:45.440
stands

00:08:45.440 --> 00:08:46.710
in some sort of interesting intersection

00:08:46.710 --> 00:08:47.980
of reinforcement

00:08:47.980 --> 00:08:51.310
learning and self-supervised learning

00:08:51.310 --> 00:08:52.660
research and we

00:08:52.660 --> 00:08:54.520
had this little figure on the bottom left

00:08:54.520 --> 00:08:55.160
of

00:08:55.160 --> 00:08:58.000
the poster which was the screenshot of a

00:08:58.000 --> 00:08:59.220
slide from young lakun talking about how

00:08:59.220 --> 00:09:00.440
to

00:09:00.440 --> 00:09:01.020
build intelligent

00:09:01.020 --> 00:09:02.030
systems and whether that's going to be

00:09:02.030 --> 00:09:03.040
done

00:09:03.040 --> 00:09:04.460
by unsupervised learning or supervised

00:09:04.460 --> 00:09:05.880
learning and

00:09:05.880 --> 00:09:06.910
reinforcement learning and i think what

00:09:06.910 --> 00:09:07.940
our paper

00:09:07.940 --> 00:09:09.250
really suggests is that the boundary

00:09:09.250 --> 00:09:10.560
between these

00:09:10.560 --> 00:09:12.460
things is really blurry and maybe the keys

00:09:12.460 --> 00:09:13.260
to building intelligent systems are going

00:09:13.260 --> 00:09:13.940
to be

00:09:13.940 --> 00:09:14.240
leveraging

00:09:14.240 --> 00:09:16.660
insights from all of them yeah the layer

00:09:16.660 --> 00:09:20.400
kick exactly uh well thank you for your

00:09:20.400 --> 00:09:21.500
time i know i know you have to

00:09:21.500 --> 00:09:21.660
go

00:09:21.660 --> 00:09:24.060
soon yeah thank you so much for coming

00:09:24.060 --> 00:09:26.780
i think that that insight of like blurring

00:09:26.780 --> 00:09:27.760
things is interesting

00:09:27.760 --> 00:09:29.320
i don't know if you like you were

00:09:29.320 --> 00:09:30.460
talking about so like uh the abstraction

00:09:30.460 --> 00:09:31.600
layer

00:09:31.600 --> 00:09:32.520
of representation

00:09:32.520 --> 00:09:33.750
learning i don't know if that triggers

00:09:33.750 --> 00:09:34.980
anything

00:09:34.980 --> 00:09:37.680
in terms of like the mix between self

00:09:37.680 --> 00:09:38.440
-supervised and

00:09:38.440 --> 00:09:39.530
reinforcement learning is that is there

00:09:39.530 --> 00:09:40.620
something fundamental

00:09:40.620 --> 00:09:42.740
that you've discovered or that we've

00:09:42.740 --> 00:09:43.820
that people don't understand when they

00:09:43.820 --> 00:09:44.900
when they

00:09:44.900 --> 00:09:46.700
read the paper yeah i think the best

00:09:46.700 --> 00:09:47.260
way that i would

00:09:47.260 --> 00:09:50.100
explain it is that we know that standard

00:09:50.100 --> 00:09:52.540
rl is not super scalable and so like

00:09:52.540 --> 00:09:53.680
why can this a different

00:09:53.680 --> 00:09:54.900
approach or different objective rl be

00:09:54.900 --> 00:09:56.120
scalable i

00:09:56.120 --> 00:09:57.170
think it's because we're fundamentally

00:09:57.170 --> 00:09:58.220
shifting the

00:09:58.780 --> 00:10:00.340
burden of learning from something like

00:10:00.340 --> 00:10:01.900
like q

00:10:01.900 --> 00:10:02.850
learning or like regressing to like td

00:10:02.850 --> 00:10:03.800
errors

00:10:03.800 --> 00:10:04.240
which

00:10:04.240 --> 00:10:05.780
we know is quite spurious and noisy and

00:10:05.780 --> 00:10:07.714
biased to fundamentally like a

00:10:07.714 --> 00:10:09.520
classification problem we're

00:10:09.520 --> 00:10:10.900
trying to classify whether future states

00:10:10.900 --> 00:10:12.280
is along

00:10:12.280 --> 00:10:13.130
the same trajectory or along a different

00:10:13.130 --> 00:10:13.980
trajectory

00:10:14.540 --> 00:10:15.640
and we do this with representation

00:10:15.640 --> 00:10:16.740
learning right

00:10:16.740 --> 00:10:18.640
and we know that classification

00:10:18.640 --> 00:10:19.620
cross-entry loss

00:10:19.620 --> 00:10:20.890
and representation learning is scalable in

00:10:20.890 --> 00:10:22.160
the deep

00:10:22.160 --> 00:10:22.960
learning literature right if we think

00:10:22.960 --> 00:10:23.760
about language

00:10:24.700 --> 00:10:26.920
and like some of the objectives there so

00:10:26.920 --> 00:10:30.060
in some sense we're kind of blurring the

00:10:30.060 --> 00:10:30.660
the lines we're

00:10:30.660 --> 00:10:31.610
doing reinforcement learning it's still an

00:10:31.610 --> 00:10:32.560
actor critic

00:10:32.560 --> 00:10:33.640
reinforcement learning algorithm it's like

00:10:33.640 --> 00:10:34.720
a goal

00:10:34.720 --> 00:10:36.763
condition reinforcement algorithm but the

00:10:36.763 --> 00:10:37.760
objective the burden

00:10:37.760 --> 00:10:39.940
of like learning of the of solving that

00:10:39.940 --> 00:10:40.160
rl

00:10:40.160 --> 00:10:41.420
task shifts to something that's more

00:10:41.420 --> 00:10:42.680
similar to

00:10:42.680 --> 00:10:43.980
objectives that you might see in language

00:10:43.980 --> 00:10:45.280
and

00:10:45.280 --> 00:10:46.560
vision that we know have scaled so much

00:10:46.560 --> 00:10:48.580
and so i think yeah i think that's

00:10:48.580 --> 00:10:49.500
like one of the fundamental

00:10:49.500 --> 00:10:52.280
insights that we've seen is that um it

00:10:52.280 --> 00:10:53.250
seems like by approaching rl in this

00:10:53.250 --> 00:10:54.220
different

00:10:54.220 --> 00:10:55.400
approach we're

00:10:55.400 --> 00:10:57.540
able to like get so much more out

00:10:57.540 --> 00:10:59.060
of we were able to scale our networks

00:10:59.060 --> 00:11:00.960
like significantly beyond what

00:11:00.960 --> 00:11:02.940
was like standard used in ara can i

00:11:02.940 --> 00:11:05.100
jump in i will just give a bit

00:11:05.100 --> 00:11:06.700
of more of context about the

00:11:06.700 --> 00:11:09.258
architecture because uh yeah we use

00:11:09.258 --> 00:11:10.500
another objective

00:11:10.500 --> 00:11:14.040
and the uh influences so the contrastive

00:11:14.040 --> 00:11:15.660
laws however the architecture is uh quite

00:11:15.660 --> 00:11:17.280
similar

00:11:17.280 --> 00:11:20.100
to the previous works uh of previous uh

00:11:20.100 --> 00:11:20.800
papers like

00:11:20.800 --> 00:11:24.320
bro or or uh simba simba v1 simba

00:11:24.320 --> 00:11:29.200
v2 uh simba v1 simba v2 so we

00:11:29.200 --> 00:11:31.000
we also tweaked a bit of this

00:11:31.000 --> 00:11:32.800
uh architecture however it's not that we

00:11:32.800 --> 00:11:34.600
like

00:11:34.600 --> 00:11:36.760
uh invented the wheel for the first time

00:11:36.760 --> 00:11:37.720
it's the

00:11:37.720 --> 00:11:39.060
merging between the architecture and the

00:11:39.060 --> 00:11:40.400
objective that

00:11:40.400 --> 00:11:43.520
makes the scale uh really uh like go

00:11:43.520 --> 00:11:44.460
up and

00:11:44.460 --> 00:11:46.700
and and performance follow the the scale i

00:11:46.700 --> 00:11:47.840
think that's something that we should uh

00:11:47.840 --> 00:11:48.980
probably

00:11:48.980 --> 00:11:50.000
mine deeper

00:11:50.000 --> 00:11:52.800
um do you think i guess like what

00:11:52.800 --> 00:11:53.940
domains what industry like you've applied

00:11:53.940 --> 00:11:55.080
it on

00:11:55.080 --> 00:11:56.000
multiple different

00:11:56.000 --> 00:11:58.120
uh types of networks that are or data

00:11:58.120 --> 00:11:59.590
sets is there a particular affinity that

00:11:59.590 --> 00:12:01.060
you

00:12:01.060 --> 00:12:01.820
think like has is

00:12:01.820 --> 00:12:04.100
like kind of low-hanging food yeah so

00:12:04.100 --> 00:12:05.180
actually if you look at a lot of

00:12:05.180 --> 00:12:07.080
our tasks they're particularly

00:12:07.080 --> 00:12:09.700
sort of like robotics tasks um so this

00:12:09.700 --> 00:12:12.420
is a person i'd be very curious about

00:12:12.420 --> 00:12:13.780
how a work like this could

00:12:13.780 --> 00:12:14.980
impact like the robotics field like my

00:12:14.980 --> 00:12:16.180
understanding

00:12:16.180 --> 00:12:17.760
of robotics is that a lot of robotics

00:12:17.760 --> 00:12:18.100
are now

00:12:18.100 --> 00:12:19.080
there's kind of multi a few different

00:12:19.080 --> 00:12:20.060
approaches

00:12:20.060 --> 00:12:22.220
like one approach is we want to train

00:12:22.220 --> 00:12:23.360
robots using

00:12:23.360 --> 00:12:24.160
imitation learning so we try to collect

00:12:24.160 --> 00:12:24.940
like

00:12:24.940 --> 00:12:26.560
an insane amount of data we have a

00:12:26.560 --> 00:12:27.300
ton of human

00:12:27.820 --> 00:12:29.320
supervision and we try to scale up this

00:12:29.320 --> 00:12:30.180
data and we're like learning with

00:12:30.180 --> 00:12:31.040
imitation learning

00:12:31.040 --> 00:12:32.440
like but on the other hand potentially

00:12:32.440 --> 00:12:33.840
like

00:12:33.840 --> 00:12:34.930
perhaps there's another approach which is

00:12:34.930 --> 00:12:36.020
like

00:12:36.020 --> 00:12:37.292
for example like goal condition

00:12:37.292 --> 00:12:38.440
reinforcement learning where

00:12:38.440 --> 00:12:39.900
we can actually train robotic

00:12:39.900 --> 00:12:41.070
agents and training rl agents to solve

00:12:41.070 --> 00:12:42.240
meaningful

00:12:42.240 --> 00:12:43.570
tasks with absolutely no human supervision

00:12:43.570 --> 00:12:44.900
no

00:12:44.900 --> 00:12:46.140
demonstration it's much more scalable yeah

00:12:46.140 --> 00:12:47.380
so yeah

00:12:47.380 --> 00:12:48.410
so this could serve as an alternate

00:12:48.410 --> 00:12:49.440
approach

00:12:49.440 --> 00:12:50.400
and perhaps instead of like scaling data

00:12:50.400 --> 00:12:51.360
like

00:12:51.360 --> 00:12:52.660
scaling manual like human supervision

00:12:52.660 --> 00:12:53.960
which

00:12:53.960 --> 00:12:56.140
which is you know not super scalable if

00:12:56.140 --> 00:12:58.040
there are ways to sort of make goal

00:12:58.040 --> 00:12:58.640
condition reinforcement

00:12:58.640 --> 00:12:59.820
learning scalable and like we can just

00:12:59.820 --> 00:13:01.000
scale

00:13:01.000 --> 00:13:01.860
the architecture or we can scale because

00:13:01.860 --> 00:13:02.720
you're

00:13:02.720 --> 00:13:03.620
focused on your objectives yeah right with

00:13:03.620 --> 00:13:04.520
with

00:13:04.520 --> 00:13:05.320
certain different objectives i think that

00:13:05.320 --> 00:13:05.980
could be

00:13:05.980 --> 00:13:07.580
very exciting and see how to see how

00:13:07.580 --> 00:13:09.360
that can affect a field like robotics for

00:13:09.360 --> 00:13:11.260
example yeah uh double

00:13:11.260 --> 00:13:13.020
click on on just one one thing on

00:13:13.020 --> 00:13:13.900
the efficiency which you guys are talking

00:13:13.900 --> 00:13:14.780
about

00:13:14.780 --> 00:13:15.780
i would expect

00:13:16.320 --> 00:13:18.640
the very deep the deeper it is there

00:13:18.640 --> 00:13:19.980
should be quadratically worse i am not

00:13:19.980 --> 00:13:21.320
familiar

00:13:21.320 --> 00:13:21.740
with like

00:13:21.740 --> 00:13:22.750
the the pre-existing literature i'm just

00:13:22.750 --> 00:13:23.760
like

00:13:23.760 --> 00:13:25.690
sort of working out intuitions but um

00:13:25.690 --> 00:13:27.620
basically

00:13:27.620 --> 00:13:28.420
uh what

00:13:28.420 --> 00:13:30.680
are the trade-offs that you've found that

00:13:30.680 --> 00:13:32.080
i think you might want to warn people

00:13:32.080 --> 00:13:33.640
about because because you

00:13:33.640 --> 00:13:34.510
you are the guy who mentioned efficiency

00:13:34.510 --> 00:13:35.380
so

00:13:35.380 --> 00:13:37.200
yeah sure sure yeah so i was referring

00:13:37.200 --> 00:13:37.680
to like one of the

00:13:37.680 --> 00:13:39.320
figures on our poster also in our paper

00:13:39.320 --> 00:13:40.310
where we compare like the number of

00:13:40.310 --> 00:13:41.300
parameters

00:13:41.300 --> 00:13:42.480
that models have

00:13:42.480 --> 00:13:44.220
as we see along the axis of depth

00:13:44.220 --> 00:13:46.080
and as we scale along the axis of

00:13:46.080 --> 00:13:47.480
width yeah from our baseline

00:13:47.480 --> 00:13:48.590
architecture the most baseline one would

00:13:48.590 --> 00:13:49.700
be like

00:13:49.700 --> 00:13:52.360
a width of 256 like the hidden layers

00:13:52.360 --> 00:13:53.320
of 256 neurons

00:13:53.320 --> 00:13:55.180
and then the depth is for four layers

00:13:55.180 --> 00:13:58.320
or hidden layers um and so the point

00:13:58.320 --> 00:13:59.040
i was making there is

00:13:59.040 --> 00:14:00.760
that when you scale along depth your the

00:14:00.760 --> 00:14:01.630
number of parameters that your model has

00:14:01.630 --> 00:14:02.500
is

00:14:02.500 --> 00:14:02.820
going to grow

00:14:02.820 --> 00:14:05.528
roughly linearly uh whereas with with

00:14:05.528 --> 00:14:06.480
you're making

00:14:06.480 --> 00:14:07.980
your network outputs wider and then the

00:14:07.980 --> 00:14:09.480
input

00:14:09.480 --> 00:14:10.080
to the net

00:14:10.080 --> 00:14:12.640
next network is also growing as well and

00:14:12.640 --> 00:14:13.440
so the the number of parameters your

00:14:13.440 --> 00:14:14.220
network's

00:14:14.220 --> 00:14:14.480
then going

00:14:14.480 --> 00:14:15.920
to have grows approximately quadratically

00:14:15.920 --> 00:14:17.360
and so one

00:14:17.360 --> 00:14:18.640
of the experiments we did was sort of

00:14:18.640 --> 00:14:19.160
examining

00:14:19.680 --> 00:14:21.280
as we grow the number of parameters in

00:14:21.280 --> 00:14:22.150
our model by scaling along these two

00:14:22.150 --> 00:14:23.020
different

00:14:23.020 --> 00:14:23.520
choices

00:14:23.980 --> 00:14:25.310
which one for the same like approximate

00:14:25.310 --> 00:14:26.640
number

00:14:26.640 --> 00:14:27.800
of parameters yields a better performance

00:14:27.800 --> 00:14:28.960
and the

00:14:28.960 --> 00:14:29.140
depth

00:14:29.140 --> 00:14:30.460
curve kind of goes like this it jumps

00:14:30.460 --> 00:14:31.680
up pretty fast that's like present

00:14:31.680 --> 00:14:32.900
throughout our

00:14:32.900 --> 00:14:33.900
paper for with

00:14:33.900 --> 00:14:35.180
it grows a little bit more slowly and

00:14:35.180 --> 00:14:36.740
so that the kind of takeaway from that

00:14:36.740 --> 00:14:38.220
is that if you

00:14:38.220 --> 00:14:39.560
are a bit more resource constrained

00:14:39.560 --> 00:14:40.900
scaling along

00:14:40.900 --> 00:14:41.700
depth might be better because there's

00:14:41.700 --> 00:14:42.180
fewer

00:14:42.180 --> 00:14:43.400
parameters with a smaller model to a

00:14:43.400 --> 00:14:44.620
smaller

00:14:44.620 --> 00:14:46.000
number tool learnable parameters width is

00:14:46.000 --> 00:14:47.380
expensive

00:14:47.380 --> 00:14:48.660
width is expensive exactly and in general

00:14:48.660 --> 00:14:49.940
of

00:14:49.940 --> 00:14:50.740
course like more parameters is also going

00:14:50.740 --> 00:14:51.340
to

00:14:51.340 --> 00:14:51.600
be more

00:14:51.600 --> 00:14:53.652
expensive so that's just like another

00:14:53.652 --> 00:14:54.540
consideration to

00:14:54.540 --> 00:14:55.640
think about when using these networks and

00:14:55.640 --> 00:14:56.740
suppose

00:14:56.740 --> 00:14:59.280
yeah any other sort of rules of thumbs

00:14:59.280 --> 00:15:00.720
like that that i can extract that this

00:15:00.720 --> 00:15:01.400
is just the most basic

00:15:01.400 --> 00:15:03.640
one that i could think of yeah uh

00:15:03.640 --> 00:15:05.680
i don't know if there's any others yeah

00:15:05.680 --> 00:15:07.140
i guess like your original

00:15:07.140 --> 00:15:09.240
question of like the trade-offs um like

00:15:09.240 --> 00:15:10.260
one of the trade-offs one of the

00:15:10.260 --> 00:15:11.180
limitations that we say is

00:15:11.180 --> 00:15:11.980
like obviously if you make the networks

00:15:11.980 --> 00:15:12.620
bigger

00:15:12.620 --> 00:15:15.140
the it will take longer to run right

00:15:15.140 --> 00:15:17.100
so if you like

00:15:17.100 --> 00:15:18.700
double the depth at some level of depth

00:15:18.700 --> 00:15:20.580
you you it might take like twice as

00:15:20.580 --> 00:15:21.820
much to like take make a

00:15:21.820 --> 00:15:23.120
forward pass through the network right

00:15:23.120 --> 00:15:24.420
however this

00:15:24.420 --> 00:15:26.520
is not so like within our paper like

00:15:26.520 --> 00:15:27.800
for most

00:15:27.800 --> 00:15:29.060
environments um we are able to like

00:15:29.060 --> 00:15:30.320
saturate

00:15:30.320 --> 00:15:31.750
like get to like almost perfect

00:15:31.750 --> 00:15:32.580
performance within

00:15:32.580 --> 00:15:33.480
just you know

00:15:33.480 --> 00:15:34.580
we don't even need to get to like

00:15:34.580 --> 00:15:35.540
a thousand layers like maybe just 64

00:15:35.540 --> 00:15:36.500
layers

00:15:36.500 --> 00:15:37.540
for example is

00:15:37.540 --> 00:15:40.840
sufficient um and in this regime like like

00:15:40.840 --> 00:15:41.670
the latency of the network is not

00:15:41.670 --> 00:15:42.500
necessarily

00:15:42.500 --> 00:15:42.740
actually

00:15:42.740 --> 00:15:44.190
even uh not necessarily like a significant

00:15:44.190 --> 00:15:45.640
bottleneck

00:15:45.640 --> 00:15:47.000
like you can imagine there's a lot of

00:15:47.000 --> 00:15:47.500
tasks in

00:15:47.500 --> 00:15:49.284
which especially in rl that like

00:15:49.284 --> 00:15:50.120
collecting data

00:15:50.120 --> 00:15:51.290
might be the bottleneck right and making

00:15:51.290 --> 00:15:52.460
four

00:15:52.460 --> 00:15:52.700
passes

00:15:52.700 --> 00:15:53.590
through our network may not be the

00:15:53.590 --> 00:15:54.480
bottleneck

00:15:54.480 --> 00:15:56.940
and so in our environment we in our

00:15:56.940 --> 00:15:57.340
research we

00:15:57.340 --> 00:15:59.297
specifically use the jacks gcrl

00:15:59.297 --> 00:16:00.560
environment which is

00:16:00.560 --> 00:16:01.800
a jack space gpu accelerator environment

00:16:01.800 --> 00:16:03.040
so we

00:16:03.040 --> 00:16:03.180
can

00:16:03.180 --> 00:16:05.543
collect like thousands of like environment

00:16:05.543 --> 00:16:06.500
trajectories like

00:16:06.500 --> 00:16:07.760
in parallel at the same time

00:16:07.760 --> 00:16:09.980
so that we're able to like make uh

00:16:09.980 --> 00:16:12.460
like oh this is built in right this

00:16:12.460 --> 00:16:13.760
is built in so that we can

00:16:13.760 --> 00:16:14.830
collect you know like like a thousand

00:16:14.830 --> 00:16:15.900
trajectories

00:16:15.900 --> 00:16:16.890
at the same time along all these

00:16:16.890 --> 00:16:17.880
environments

00:16:17.880 --> 00:16:18.140
and

00:16:18.140 --> 00:16:20.520
so um makes that make sure that like

00:16:20.520 --> 00:16:22.600
we have enough data to like saturate the

00:16:22.600 --> 00:16:23.340
learning from wow

00:16:24.000 --> 00:16:25.200
yeah that's like work they've been called

00:16:25.200 --> 00:16:26.400
okay

00:16:26.400 --> 00:16:27.840
and you i don't know if you want

00:16:27.840 --> 00:16:29.280
to explore expand upon

00:16:29.280 --> 00:16:32.440
that on the jargon zcrl uh maybe and

00:16:32.440 --> 00:16:33.240
you know most people are familiar with

00:16:33.240 --> 00:16:33.820
pytor

00:16:33.820 --> 00:16:34.340
is maybe less

00:16:34.340 --> 00:16:36.040
familiar with jacks uh with jacks i think

00:16:36.040 --> 00:16:37.800
jacks is getting the uh the traction

00:16:37.800 --> 00:16:39.560
especially

00:16:39.560 --> 00:16:40.540
in rl field

00:16:40.540 --> 00:16:42.937
because the in for online reinforcement

00:16:42.937 --> 00:16:43.920
learning getting

00:16:43.920 --> 00:16:46.760
as much data as you can is is

00:16:46.760 --> 00:16:47.080
the most

00:16:47.080 --> 00:16:47.980
important there's got to be a pytorch

00:16:47.980 --> 00:16:48.880
equivalence

00:16:48.880 --> 00:16:52.140
but anyway any tips for other people also

00:16:52.140 --> 00:16:52.540
exploring

00:16:52.540 --> 00:16:54.920
this kind of uh rollout yeah yeah so

00:16:54.920 --> 00:16:57.640
i think i can also recommend like uh

00:16:57.640 --> 00:16:58.880
for for gold conditioned

00:16:58.880 --> 00:17:00.150
rl i'm recommending jacks this year but

00:17:00.150 --> 00:17:01.420
there

00:17:01.420 --> 00:17:02.990
are also like multi-agent jacks uh

00:17:02.990 --> 00:17:04.560
implementation

00:17:04.560 --> 00:17:05.240
and

00:17:05.240 --> 00:17:07.660
other so going back to our paper if

00:17:07.660 --> 00:17:09.540
you look at the plots we only see

00:17:09.540 --> 00:17:11.580
this like huge performance

00:17:11.580 --> 00:17:15.840
increase when we cross like 50 uh millions

00:17:15.840 --> 00:17:19.660
of uh transitions uh gap so so i

00:17:19.660 --> 00:17:21.100
think the data is crucial

00:17:21.100 --> 00:17:23.540
like here yeah i guess even to build

00:17:23.540 --> 00:17:25.880
on that like i like drawing analogies to

00:17:25.880 --> 00:17:27.240
like successes in other

00:17:27.240 --> 00:17:29.980
areas of deep learning like for example in

00:17:29.980 --> 00:17:31.280
large language models the reason why we're

00:17:31.280 --> 00:17:32.580
able

00:17:32.580 --> 00:17:32.920
to scale

00:17:32.920 --> 00:17:34.680
to such large networks is that we found

00:17:34.680 --> 00:17:36.440
a paradigm in which we can leverage the

00:17:36.440 --> 00:17:37.220
entire internet scale

00:17:37.220 --> 00:17:40.020
of data is alert right and so data

00:17:40.020 --> 00:17:41.920
in rl traditionally has been hard to come

00:17:41.920 --> 00:17:44.440
by um but now with these like

00:17:44.440 --> 00:17:45.957
gpu accelerated environments we can

00:17:45.957 --> 00:17:46.780
collect hundreds of

00:17:46.780 --> 00:17:48.260
millions of times of the data within just

00:17:48.260 --> 00:17:48.500
a few

00:17:48.500 --> 00:17:50.700
hours and so i think that this serves

00:17:50.700 --> 00:17:52.660
as like a really good test bed for

00:17:52.660 --> 00:17:53.540
us to be able to also

00:17:53.540 --> 00:17:56.080
find ways to scale up um like network

00:17:56.080 --> 00:17:58.160
capacity and get similar kind of games i

00:17:58.160 --> 00:17:58.960
think i asked are you

00:17:58.960 --> 00:18:00.340
saying that you have a difference you

00:18:00.340 --> 00:18:01.720
would

00:18:01.720 --> 00:18:03.530
do pre-training differently in llms like

00:18:03.530 --> 00:18:05.340
what's

00:18:05.340 --> 00:18:05.900
the what's

00:18:05.900 --> 00:18:09.260
the difference uh objective now um yeah i

00:18:09.260 --> 00:18:10.900
mean very simply very simply the the

00:18:10.900 --> 00:18:12.540
paradigm

00:18:12.540 --> 00:18:12.840
that you're

00:18:12.840 --> 00:18:14.130
referencing is next word or next token

00:18:14.130 --> 00:18:15.420
right

00:18:15.420 --> 00:18:19.580
it's very robust how do you change that

00:18:19.580 --> 00:18:20.460
oh i'm not

00:18:20.460 --> 00:18:21.260
saying that we're changing i want to

00:18:21.260 --> 00:18:22.040
leverage

00:18:22.040 --> 00:18:24.840
insights from that to apply to all i

00:18:24.840 --> 00:18:25.660
feel like

00:18:25.660 --> 00:18:26.800
you should go the other way you think

00:18:26.800 --> 00:18:28.560
you should go the other way yeah maybe

00:18:28.560 --> 00:18:29.980
i mean i would be a very

00:18:29.980 --> 00:18:31.120
interesting research direction too but

00:18:31.120 --> 00:18:32.260
actually yeah even

00:18:32.260 --> 00:18:33.500
on that point like one of the things

00:18:33.500 --> 00:18:33.700
i was

00:18:33.700 --> 00:18:35.520
thinking about is that the way that our

00:18:35.520 --> 00:18:37.880
rl objective works is in some set it's

00:18:37.880 --> 00:18:38.620
not exactly next

00:18:39.120 --> 00:18:41.640
word prediction but it's kind of like next

00:18:41.640 --> 00:18:42.720
state prediction right you imagine you're

00:18:42.720 --> 00:18:43.800
at some

00:18:43.800 --> 00:18:44.040
current

00:18:44.040 --> 00:18:45.270
state and you're at some current action

00:18:45.270 --> 00:18:46.500
and

00:18:46.500 --> 00:18:47.580
we want to predict whether or not this

00:18:47.580 --> 00:18:48.060
future state

00:18:48.060 --> 00:18:50.400
this this certain state is a future state

00:18:50.400 --> 00:18:51.200
along the same trajectory or a different

00:18:51.200 --> 00:18:51.840
trajectory

00:18:52.240 --> 00:18:54.500
and so in some sense we are actually

00:18:54.500 --> 00:18:55.700
doing some sort of like implicit world

00:18:55.700 --> 00:18:56.900
model

00:18:56.900 --> 00:18:58.680
is it like uh you

00:18:58.680 --> 00:18:59.920
know like i don't know if that's a

00:18:59.920 --> 00:19:01.460
bad word these things or is or like

00:19:01.460 --> 00:19:02.760
in language you you do a cross

00:19:02.760 --> 00:19:03.560
entry loss to classify the next token

00:19:03.560 --> 00:19:04.160
right

00:19:04.160 --> 00:19:05.050
and here we're just doing a binary

00:19:05.050 --> 00:19:05.940
classification

00:19:05.940 --> 00:19:06.460
of like

00:19:06.460 --> 00:19:08.140
whether or not some next state is some

00:19:08.140 --> 00:19:09.280
yeah yeah it's a classification yeah yeah

00:19:09.280 --> 00:19:10.420
yeah

00:19:10.420 --> 00:19:11.340
and so i do

00:19:11.340 --> 00:19:13.600
see that there are some like sort of

00:19:13.600 --> 00:19:14.710
parallels here that perhaps we should dig

00:19:14.710 --> 00:19:15.820
into

00:19:15.820 --> 00:19:16.380
deeper and

00:19:16.380 --> 00:19:18.940
see like what is the core to of

00:19:18.940 --> 00:19:20.020
what enables deep learning to scale and

00:19:20.020 --> 00:19:21.100
then

00:19:21.100 --> 00:19:22.440
how can we like leverage

00:19:22.440 --> 00:19:23.560
that how can we distill those like

00:19:23.560 --> 00:19:24.680
insights

00:19:24.680 --> 00:19:25.620
and then apply those across like all

00:19:25.620 --> 00:19:26.560
different

00:19:26.560 --> 00:19:26.800
fields

00:19:26.800 --> 00:19:28.855
whether it's language or reinforcement

00:19:28.855 --> 00:19:29.720
learning yeah uh

00:19:29.720 --> 00:19:31.040
did you did you get my my meaning

00:19:31.040 --> 00:19:31.360
about the

00:19:31.360 --> 00:19:33.940
world model stuff yeah yeah actually and i

00:19:33.940 --> 00:19:35.600
i heard i think i might have heard

00:19:35.600 --> 00:19:36.400
professor eisenbach

00:19:36.400 --> 00:19:37.200
yesterday talking about this at a poster

00:19:37.200 --> 00:19:37.820
and

00:19:37.820 --> 00:19:39.640
he's explaining to a couple of people that

00:19:39.640 --> 00:19:40.300
because

00:19:40.300 --> 00:19:41.410
this is like doing representation learning

00:19:41.410 --> 00:19:42.520
and trying

00:19:42.520 --> 00:19:43.900
to learn these meaningful representations

00:19:44.360 --> 00:19:45.900
for a given state of action been for

00:19:45.900 --> 00:19:47.800
a given goal in some sense you can

00:19:47.800 --> 00:19:48.700
think of it almost like

00:19:48.700 --> 00:19:49.750
learning a model of environment learning a

00:19:49.750 --> 00:19:50.800
model

00:19:50.800 --> 00:19:52.580
of the world but without having to do

00:19:52.580 --> 00:19:53.220
any sort of like

00:19:53.220 --> 00:19:54.700
next frame prediction or stuff like that

00:19:54.700 --> 00:19:56.180
that's

00:19:56.180 --> 00:19:57.120
a little bit more high dimensional and

00:19:57.120 --> 00:19:58.060
complex

00:19:58.060 --> 00:20:00.120
yeah yeah i would think like

00:20:00.120 --> 00:20:02.160
um the the angle that i'm trying to

00:20:02.160 --> 00:20:05.640
think about and push is instead of learn

00:20:05.640 --> 00:20:06.860
the next world they're

00:20:06.860 --> 00:20:07.990
basically like generate a number of

00:20:07.990 --> 00:20:09.120
candidates possible

00:20:09.120 --> 00:20:11.560
worlds and classify them uh to your point

00:20:11.560 --> 00:20:14.020
uh which is exactly how i do things

00:20:14.020 --> 00:20:15.680
let's say i'm playing poker and i'm trying

00:20:15.680 --> 00:20:16.740
to classify what hands you

00:20:16.740 --> 00:20:18.820
have well there's a range of hands based

00:20:18.820 --> 00:20:20.820
on what you're you're doing and the more

00:20:20.820 --> 00:20:21.680
information i get

00:20:21.680 --> 00:20:24.240
the more i resolve to oh i know

00:20:24.240 --> 00:20:25.560
exactly what hand you have based on what

00:20:25.560 --> 00:20:27.580
you're showing you know or

00:20:27.580 --> 00:20:28.760
you're buffing but that's a different

00:20:28.760 --> 00:20:29.940
thing but

00:20:29.940 --> 00:20:31.200
you know what i mean like i i

00:20:31.200 --> 00:20:32.240
feel like that is

00:20:32.240 --> 00:20:34.087
the ultimate sort of angle of

00:20:34.087 --> 00:20:35.360
representation which

00:20:35.360 --> 00:20:37.760
is a world but i don't know if

00:20:37.760 --> 00:20:38.800
that is too vague

00:20:38.800 --> 00:20:40.120
compared to the more concrete types of

00:20:40.120 --> 00:20:41.440
world

00:20:41.440 --> 00:20:43.720
models that let's say the video gen people

00:20:43.720 --> 00:20:44.040
are doing

00:20:44.800 --> 00:20:46.900
and then i guess one other thing like

00:20:46.900 --> 00:20:49.060
i i'm also exploring i you mentioned like

00:20:49.060 --> 00:20:50.440
the deep models

00:20:50.440 --> 00:20:52.000
being slower or more expensive yeah that

00:20:52.000 --> 00:20:53.560
is

00:20:53.560 --> 00:20:55.140
a trend in the inference world of making

00:20:55.140 --> 00:20:55.780
models shallower

00:20:55.780 --> 00:20:58.880
right and i wonder if this like short

00:20:58.880 --> 00:20:59.840
catchphrase i was thinking about like deep

00:20:59.840 --> 00:21:00.800
teacher

00:21:02.620 --> 00:21:04.780
shallow student would be a good deployment

00:21:04.780 --> 00:21:06.940
paradigm

00:21:06.940 --> 00:21:08.955
yeah like you push the frontier

00:21:08.955 --> 00:21:10.060
capabilities with

00:21:10.060 --> 00:21:10.180
the

00:21:10.180 --> 00:21:12.280
with death and then you distill it back

00:21:12.280 --> 00:21:14.320
yeah actually this is a good point like

00:21:14.320 --> 00:21:14.880
if you go out to our

00:21:14.880 --> 00:21:17.040
website like this is one of the future

00:21:17.040 --> 00:21:18.420
directions that we list at the very bottom

00:21:18.420 --> 00:21:21.160
oh okay yeah uh we

00:21:21.160 --> 00:21:23.000
we we would love to see if we

00:21:23.000 --> 00:21:24.120
could get similar performance like we push

00:21:24.120 --> 00:21:25.240
the

00:21:25.240 --> 00:21:26.180
you know like we do

00:21:26.180 --> 00:21:28.380
achieve state-of-the-art performance on uh

00:21:28.380 --> 00:21:30.300
gold condition rl in jackson crl by a

00:21:30.300 --> 00:21:30.980
significant amount

00:21:30.980 --> 00:21:32.520
and so it's very exciting to see the

00:21:32.520 --> 00:21:34.880
like the the sort of frontier of the

00:21:34.880 --> 00:21:36.700
ability to train rl agents uh

00:21:36.700 --> 00:21:39.220
sort of pushed um and if we can

00:21:39.220 --> 00:21:40.980
do that in a way that also sort

00:21:40.980 --> 00:21:42.560
of is just as efficient as a standard

00:21:42.560 --> 00:21:44.740
uh you know networks that would be very

00:21:44.740 --> 00:21:45.910
cool so you know like yeah because

00:21:45.910 --> 00:21:47.080
training

00:21:47.080 --> 00:21:48.220
uh doesn't have

00:21:48.220 --> 00:21:49.900
to be the same thing that you deploy

00:21:49.900 --> 00:21:51.460
at inference right you know what i mean

00:21:51.460 --> 00:21:53.400
like yeah so yeah so

00:21:53.400 --> 00:21:54.720
if there's ways to like distill down to

00:21:54.720 --> 00:21:56.140
a smaller model or prune the model and

00:21:56.140 --> 00:21:57.280
maybe not and still

00:21:57.280 --> 00:21:58.550
retain performance that's a very

00:21:58.550 --> 00:21:59.740
interesting research structure

00:21:59.740 --> 00:22:00.740
that we choose but let's talk about

00:22:00.740 --> 00:22:02.000
other uh future directions what what else

00:22:02.000 --> 00:22:03.260
is

00:22:03.260 --> 00:22:06.273
your personal passions yeah so uh

00:22:06.273 --> 00:22:07.460
currently i'm

00:22:07.460 --> 00:22:07.860
pursuing

00:22:07.860 --> 00:22:09.768
the direction of uh stitching in

00:22:09.768 --> 00:22:11.080
reinforcement learning

00:22:11.080 --> 00:22:13.480
so we are trying to generalize

00:22:14.400 --> 00:22:17.046
reinforcement learning from shorter sub

00:22:17.046 --> 00:22:18.200
behaviors so that

00:22:18.200 --> 00:22:19.720
they are stitched merged during the test

00:22:19.720 --> 00:22:21.240
time

00:22:21.240 --> 00:22:23.860
and uh yeah i think this is one

00:22:23.860 --> 00:22:26.100
of my uh last papers that i will

00:22:26.100 --> 00:22:28.360
tackle during the phd personally i

00:22:28.360 --> 00:22:30.760
would i'm very curious of like can we

00:22:30.760 --> 00:22:33.500
like what's the like real like can we

00:22:33.500 --> 00:22:35.240
push i'm i'm curious about

00:22:35.240 --> 00:22:36.360
like advancing the frontier as much as

00:22:36.360 --> 00:22:37.480
possible

00:22:37.480 --> 00:22:38.860
um so if you actually look at our

00:22:38.860 --> 00:22:39.940
paper we focus on

00:22:39.940 --> 00:22:42.140
scaling depth but we notice that we see

00:22:42.140 --> 00:22:43.120
that scaling width actually also improves

00:22:43.120 --> 00:22:44.100
performance

00:22:44.100 --> 00:22:46.360
and we also find that actually by scaling

00:22:46.360 --> 00:22:47.490
depth we actually unlock the ability to

00:22:47.490 --> 00:22:48.620
scale

00:22:48.620 --> 00:22:49.100
along batch

00:22:49.100 --> 00:22:51.280
size as well um so this is uh

00:22:51.280 --> 00:22:53.420
one of yeah so okay it's like a

00:22:53.420 --> 00:22:55.760
collinear like yeah right so like

00:22:55.760 --> 00:22:56.600
okay i guess for context like in

00:22:56.600 --> 00:22:57.440
traditional

00:22:57.440 --> 00:22:59.700
rl like value-based rl scaling batch size

00:22:59.700 --> 00:23:00.180
is not super

00:23:00.180 --> 00:23:01.510
effective but there's we also can see

00:23:01.510 --> 00:23:02.840
there's

00:23:02.840 --> 00:23:04.100
also other work in other areas of deep

00:23:04.100 --> 00:23:04.520
learning that

00:23:04.520 --> 00:23:06.820
show that scaling batch size is only most

00:23:06.820 --> 00:23:08.620
effective when there's like a large enough

00:23:08.620 --> 00:23:09.710
network capacity to take advantage of the

00:23:09.710 --> 00:23:10.800
scaled

00:23:10.800 --> 00:23:13.380
batch size and we actually find that you

00:23:13.380 --> 00:23:13.480
know

00:23:13.480 --> 00:23:15.420
perhaps so one hypothesis and i'd be like

00:23:15.420 --> 00:23:16.560
perhaps the reason why scale batches isn't

00:23:16.560 --> 00:23:17.700
that

00:23:17.700 --> 00:23:18.220
effective

00:23:18.220 --> 00:23:19.180
in traditional rls because like we've been

00:23:19.180 --> 00:23:20.140
using

00:23:20.140 --> 00:23:20.940
these tiny networks that haven't been able

00:23:20.940 --> 00:23:21.540
to

00:23:21.540 --> 00:23:23.620
capture that and one of our experiments is

00:23:23.620 --> 00:23:25.640
that like because we are enabled

00:23:25.640 --> 00:23:26.840
successful training

00:23:26.840 --> 00:23:26.960
of

00:23:26.960 --> 00:23:29.140
deep network we actually were able to this

00:23:29.140 --> 00:23:30.760
is a great test bed for you know

00:23:30.760 --> 00:23:31.620
like testing this

00:23:31.620 --> 00:23:33.800
hypothesis and we find that indeed as we

00:23:33.800 --> 00:23:35.030
scale to network capacity we also unlock

00:23:35.030 --> 00:23:36.260
this

00:23:36.260 --> 00:23:36.480
different

00:23:36.480 --> 00:23:38.960
dimension of scaling batch size and so all

00:23:38.960 --> 00:23:40.740
that to say is that i'm very curious

00:23:40.740 --> 00:23:42.700
for someone like

00:23:42.700 --> 00:23:45.180
with enough compute to like take some of

00:23:45.180 --> 00:23:46.330
these environments scale up batch uh scale

00:23:46.330 --> 00:23:47.480
up

00:23:47.480 --> 00:23:48.040
depth to

00:23:48.040 --> 00:23:49.220
the maximum capability also scale along

00:23:49.220 --> 00:23:50.400
with also

00:23:50.400 --> 00:23:51.680
scale along batch size and let's like

00:23:51.680 --> 00:23:52.960
basically

00:23:52.960 --> 00:23:53.300
like

00:23:53.300 --> 00:23:55.080
in the same way that in language we're

00:23:55.080 --> 00:23:57.420
scaling on so many different axes can we

00:23:57.420 --> 00:23:57.900
unlock different

00:23:57.900 --> 00:23:58.960
dimensions of scaling as well and what

00:23:58.960 --> 00:24:00.020
capabilities

00:24:00.020 --> 00:24:01.620
and how far can we push the frontier

00:24:01.620 --> 00:24:02.440
of training

00:24:02.440 --> 00:24:04.320
these rl agents from doing them before we

00:24:04.320 --> 00:24:05.880
pass it sean uh when you say enough

00:24:05.880 --> 00:24:07.060
compute what kind

00:24:07.060 --> 00:24:09.140
of compute budget did you have how does

00:24:09.140 --> 00:24:11.020
it how i just want to see what

00:24:11.020 --> 00:24:11.960
you guys got good question

00:24:11.960 --> 00:24:13.320
so we we wanted to make sure that

00:24:13.320 --> 00:24:15.380
this is we we wanted to make it

00:24:15.380 --> 00:24:16.720
such that like uh you know it's

00:24:16.720 --> 00:24:19.120
quite accessible so i can the nice thing

00:24:19.120 --> 00:24:20.800
is that all of our experiments even a

00:24:20.800 --> 00:24:21.200
thousand layer

00:24:21.200 --> 00:24:23.340
networks can be run on one single 80

00:24:23.340 --> 00:24:25.250
gigabyte h100 gpu um so that's those

00:24:25.250 --> 00:24:27.160
dollars

00:24:27.160 --> 00:24:28.340
yeah right right

00:24:28.340 --> 00:24:29.960
right so everything can be run on one

00:24:29.960 --> 00:24:32.100
gpu um but in theory if we had

00:24:32.100 --> 00:24:33.080
you know like a distributed

00:24:33.080 --> 00:24:34.290
training setup and like can just like

00:24:34.290 --> 00:24:35.500
blast

00:24:35.500 --> 00:24:36.380
compute through this and really wanted to

00:24:36.380 --> 00:24:37.260
push

00:24:37.260 --> 00:24:37.680
the frontier

00:24:37.680 --> 00:24:39.600
it'd be very interesting to see how things

00:24:39.600 --> 00:24:42.160
go yeah cool and i've actively been trying

00:24:42.160 --> 00:24:42.480
to learn as

00:24:42.480 --> 00:24:43.880
much as i can about vision language action

00:24:43.880 --> 00:24:45.470
models uh role models at europe's and

00:24:45.470 --> 00:24:47.060
going

00:24:47.060 --> 00:24:47.440
to a lot of

00:24:47.440 --> 00:24:49.114
machine language action models vision

00:24:49.114 --> 00:24:50.200
language vision language

00:24:50.200 --> 00:24:53.200
yeah um and yeah curious about

00:24:53.200 --> 00:24:55.414
applications of representation for these

00:24:55.414 --> 00:24:56.300
yeah exactly for

00:24:56.300 --> 00:24:58.040
robotics um actively trying to explore

00:24:58.040 --> 00:25:00.300
more in that area so just reading a

00:25:00.300 --> 00:25:01.100
lot of literature talking to as many

00:25:01.100 --> 00:25:01.820
people

00:25:01.820 --> 00:25:04.100
yeah we just released our

00:25:04.100 --> 00:25:05.790
episode with uh general intuition oh okay

00:25:05.790 --> 00:25:07.480
um

00:25:07.480 --> 00:25:09.480
awesome where if you know a bit about

00:25:09.480 --> 00:25:10.180
their history they

00:25:10.180 --> 00:25:11.670
started as a gaming clipping company and

00:25:11.670 --> 00:25:13.160
uh

00:25:13.160 --> 00:25:14.110
they basically have a vision language

00:25:14.110 --> 00:25:15.060
action model

00:25:15.060 --> 00:25:16.120
yeah which

00:25:16.120 --> 00:25:17.680
um i i saw i saw i saw

00:25:17.680 --> 00:25:20.120
a preview it was very impressive i'm not

00:25:20.120 --> 00:25:22.480
sure exactly how transferable it is to

00:25:22.480 --> 00:25:25.340
embodied use cases but it doesn't have to

00:25:25.340 --> 00:25:29.340
like screen is fine you know like yeah

00:25:29.340 --> 00:25:30.300
i i don't know if you

00:25:30.300 --> 00:25:32.140
have any takes on yeah that's an exciting

00:25:32.140 --> 00:25:33.640
research direction definitely yeah i i

00:25:33.640 --> 00:25:35.140
think um

00:25:35.140 --> 00:25:37.580
the the the the

00:25:37.580 --> 00:25:38.660
the concepts of actions as as something

00:25:38.660 --> 00:25:39.740
that

00:25:39.740 --> 00:25:42.340
you are outputting is actually not that

00:25:42.340 --> 00:25:44.940
popular

00:25:44.940 --> 00:25:45.640
in industry

00:25:45.640 --> 00:25:48.093
right right only because text has

00:25:48.093 --> 00:25:49.580
completely dominated

00:25:49.580 --> 00:25:51.880
the last three years and tool calling

00:25:51.880 --> 00:25:54.320
and which is a just another form of

00:25:54.320 --> 00:25:56.860
structured text uh and i i feel like

00:25:56.860 --> 00:25:59.100
the uh action research is is

00:25:59.100 --> 00:26:00.920
kind of like i don't know how i

00:26:00.920 --> 00:26:02.280
don't know what needs to happen in order

00:26:02.280 --> 00:26:05.000
to unlock the next phase in

00:26:05.000 --> 00:26:06.420
that i don't know if you i think

00:26:06.420 --> 00:26:07.640
anything interesting out here shut it out

00:26:07.640 --> 00:26:08.860
yeah

00:26:08.860 --> 00:26:09.940
there's a lot of cool work

00:26:09.940 --> 00:26:11.750
on like leveraging pre-trained vlms and

00:26:11.750 --> 00:26:13.560
you

00:26:13.560 --> 00:26:14.920
freeze it and then you apply it oh

00:26:14.920 --> 00:26:15.720
yeah and then you're

00:26:15.720 --> 00:26:17.260
recording on top of that like sort of

00:26:17.260 --> 00:26:19.110
experts to output actions um also like

00:26:19.110 --> 00:26:20.960
systems

00:26:20.960 --> 00:26:22.400
for doing like

00:26:22.400 --> 00:26:24.224
hierarchical planning maybe outputting

00:26:24.224 --> 00:26:25.280
some higher level plan

00:26:25.280 --> 00:26:26.860
that and this is like a larger network

00:26:26.860 --> 00:26:29.260
that takes a long time to a little

00:26:29.260 --> 00:26:30.840
longer to do inference and so it outputs

00:26:30.840 --> 00:26:31.980
its plans with less

00:26:31.980 --> 00:26:34.080
frequency like some sort of chunk and then

00:26:34.080 --> 00:26:36.080
from there there's like some sort of uh

00:26:36.080 --> 00:26:37.060
second system that

00:26:37.060 --> 00:26:38.360
operates a bit more fast i think there's

00:26:38.360 --> 00:26:39.220
quite a bit of interesting research in

00:26:39.220 --> 00:26:40.080
that

00:26:40.080 --> 00:26:40.660
direction so

00:26:40.660 --> 00:26:42.260
that's what i'm looking forward to cool

00:26:42.260 --> 00:26:43.860
final

00:26:43.860 --> 00:26:45.210
question uh hardest question you were

00:26:45.210 --> 00:26:46.560
asked at

00:26:46.560 --> 00:26:47.770
the poster session or just favorite

00:26:47.770 --> 00:26:48.980
encounter anyone

00:26:48.980 --> 00:26:51.540
famous that you met so i actually haven't

00:26:51.540 --> 00:26:51.900
gotten a

00:26:51.900 --> 00:26:53.320
chance to go to the conference that much

00:26:53.320 --> 00:26:55.100
i'm actually working full-time now so oh

00:26:55.100 --> 00:26:57.580
damn yeah uh so

00:26:57.580 --> 00:26:59.620
so far i i actually literally just got

00:26:59.620 --> 00:27:02.040
my badge like a few moments before session

00:27:02.040 --> 00:27:03.280
so i guess i wouldn't

00:27:03.280 --> 00:27:05.100
be the best to answer that question no

00:27:05.100 --> 00:27:06.840
no like you see you like people ask

00:27:06.840 --> 00:27:08.000
you stuff right oh that

00:27:08.000 --> 00:27:10.360
might i might close people asking you or

00:27:10.360 --> 00:27:13.020
meeting you and like you know just just

00:27:13.020 --> 00:27:13.760
give a vibe of

00:27:13.760 --> 00:27:14.990
like what people are saying and yeah

00:27:14.990 --> 00:27:16.220
people

00:27:16.220 --> 00:27:18.920
were very i think it's sort of like

00:27:18.920 --> 00:27:20.180
a very eye-opening

00:27:20.180 --> 00:27:21.740
i think that the general question is that

00:27:21.740 --> 00:27:23.660
people thought is a very eye-opening paper

00:27:23.660 --> 00:27:24.220
because like

00:27:24.220 --> 00:27:25.520
the objective is quite simple it's quite

00:27:25.520 --> 00:27:26.820
elegant

00:27:26.820 --> 00:27:29.180
and for us to be able to like

00:27:29.180 --> 00:27:30.880
you know like i don't

00:27:30.880 --> 00:27:32.540
want to say overturn but like sort of

00:27:32.540 --> 00:27:33.850
challenge the conventional wisdom that

00:27:33.850 --> 00:27:35.160
like rl is

00:27:35.160 --> 00:27:35.540
not super

00:27:35.540 --> 00:27:38.280
scalable and push it to such limits like

00:27:38.280 --> 00:27:39.710
a thousand layers d and see continuing

00:27:39.710 --> 00:27:41.140
improve

00:27:41.140 --> 00:27:41.880
performance i

00:27:41.880 --> 00:27:43.080
think the general impression that i've

00:27:43.080 --> 00:27:44.280
gotten is

00:27:44.280 --> 00:27:47.320
that you know this this could be like

00:27:47.320 --> 00:27:47.600
a really

00:27:47.600 --> 00:27:49.780
cool like if we can sort of build

00:27:49.780 --> 00:27:52.180
along this direction and that like we can

00:27:52.180 --> 00:27:52.800
really scale along

00:27:52.800 --> 00:27:53.660
all these different dimensions and push

00:27:53.660 --> 00:27:54.520
the frontier

00:27:54.520 --> 00:27:56.560
of the ability for rl i'm very curious

00:27:56.560 --> 00:27:57.060
to see how that

00:27:57.060 --> 00:27:58.600
goes all right well thank you so much

00:27:58.600 --> 00:28:00.620
for dropping by uh congrats on the paper

00:28:00.620 --> 00:28:00.840
again

00:28:01.380 --> 00:28:03.060
and uh good luck in your future work

00:28:03.060 --> 00:28:04.360
thank you thanks for having us yeah
