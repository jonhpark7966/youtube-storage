{
  "title": "[NeurIPS 최우수 논문] 자기지도 RL을 위한 1000층 네트워크 — Kevin Wang 외, Princeton",
  "description": "Princeton의 학부 연구 세미나에서 시작해 *NeurIPS 2025 최우수 논문상*을 수상하기까지, *Kevin Wang, Ishaan Javali, Michał Bortkiewicz, Tomasz Trzcinski, Benjamin Eysenbach*는 강화학습 네트워크를 *1,000층 깊이*까지 확장하며 기존 통념을 뒤엎었고, RL 커뮤니티가 불가능하다고 여겼던 성능 향상을 이끌어냈습니다.\n우리는 *NeurIPS* 현장에서 팀을 만나 *RL1000*의 뒷이야기를 파헤쳤습니다: 딥 네트워크가 언어와 비전에서는 작동했지만 RL에서는 10년 넘게 실패해 온 이유(스포일러: 깊이만의 문제가 아니라 objective의 문제입니다), 가치 기반 방법이 무너지는 곳에서 *self-supervised RL*(대조 학습을 통해 상태, 행동, 미래 상태의 표현을 학습)을 어떻게 스케일할 수 있음을 발견했는지, 이를 가능하게 한 핵심 아키텍처 트릭(*residual connections, layer normalization, 그리고 regression에서 classification으로의 전환*), 폭을 키우는 것보다 깊이를 키우는 것이 왜 더 파라미터 효율적인지(선형 vs. 이차 성장), *Jax and GPU-accelerated environments*가 수시간 만에 수억 번의 transition을 수집하게 해 준 방법(애초에 스케일링을 가능하게 한 데이터의 풍부함), 15M+ transitions를 넘기고 올바른 아키텍처 구성요소를 더하면 성능이 단순히 좋아지는 것을 넘어 *배로* 증가하는 \\\"critical depth\\\" 현상, 이것이 단순히 \\\"make networks bigger\\\"가 아니라 RL objectives의 근본적 전환인 이유(그들의 코드에는 \\\"maximize rewards\\\"라고 적힌 줄이 없고, 순수한 자기지도 표현 학습입니다), *deep teacher, shallow student* distillation이 어떻게 대규모 배포를 열 수 있는지(1000층으로 최전선 성능을 학습한 뒤 효율적인 추론 모델로 distill), 로보틱스에 주는 함의(사람의 감독이나 demonstrations 없이 goal-conditioned RL, 수작업 데이터 수집을 늘리는 대신 아키텍처를 스케일링), 그리고 *RL이 마침내 언어와 비전처럼 스케일할 준비가 되었다*는 그들의 주장—가치 함수에 컴퓨트를 쏟아붓는 것이 아니라, 나머지 딥러닝을 작동하게 만든 자기지도·표현학습 패러다임을 차용함으로써.\nWe discuss:\n\n* *self-supervised RL objective:* 가치 함수를 학습하는 대신(노이즈, 편향, spurious), 동일한 trajectory의 상태는 서로 가깝게, 다른 trajectory의 상태는 멀어지도록 하는 표현을 학습해 RL을 classification 문제로 전환\n* *naive scaling failed:* depth를 두 배로 늘리면 성능이 악화됐지만, residual connections와 layer norm을 더해 다시 두 배로 늘리자 한 환경에서 성능이 갑자기 급등하며 \\\"critical depth\\\" 현상을 열어젖힘\n* *Scaling depth vs. width:* depth는 파라미터가 선형으로 늘고 width는 이차로 늘어—같은 성능에서 depth가 더 파라미터 효율적이고 샘플 효율적\n* *Jax + GPU-accelerated environments* unlock: 수천 개의 trajectory를 병렬로 수집해 데이터가 병목이 아니었고, 15M+ transitions를 넘어서며 딥 네트워크가 진짜로 효과를 냄\n* *RL과 self-supervised learning의 경계 희미해짐:* 그들의 코드는 보상을 직접 maximize하지 않고 actor-critic goal-conditioned RL 알고리즘이지만, 학습의 부담은 TD error regression 대신 classification(cross-entropy loss, representation learning)으로 이동\n* *scaling batch size unlocks at depth:* 전통적인 RL은 네트워크가 신호를 활용하기엔 너무 작아 더 큰 batch의 이점을 못 얻지만, depth를 스케일하면 batch size가 또 하나의 유효한 스케일링 차원이 됨\n\n—\nRL1000 Team (Princeton)\n\n* *1000 Layer Networks for Self-Supervised RL: Scaling Depth Can Enable New Goal-Reaching Capabilities* (https://openreview.net/forum?id=s0JVsx3bx1): https://openreview.net/forum?id=s0JVsx3bx1\n\n00:00:00 소개: 최우수 논문상과 NeurIPS 포스터 경험\n00:01:11 팀 소개와 Princeton 연구의 출발\n00:03:35 딥러닝의 이상 현상: 왜 RL은 얕게 머물렀나\n00:04:35 Self-Supervised RL: 스케일링에 대한 다른 접근\n00:05:13 돌파구: Residual Connections와 Critical Depth\n00:07:15 아키텍처 선택: ResNets에서 차용하고 기울기 소실을 피하기\n00:07:50 논문 정리: 단지 큰 네트워크가 아니라 다른 Objective\n00:08:46 경계 흐리기: RL과 Self-Supervised Learning의 만남\n00:09:44 TD Errors에서 Classification으로: 이 Objective가 스케일하는 이유\n00:11:06 아키텍처 세부: Braw와 SymbaFowl 기반 구축\n00:12:05 로보틱스 적용: 사람의 감독 없이 Goal-Conditioned RL\n00:13:15 효율성 트레이드오프: Depth vs Width와 파라미터 스케일링\n00:15:48 JAX and GPU-Accelerated Environments: 데이터 인프라\n00:18:05 World Models와 다음 상태 Classification\n00:22:37 네트워크 용량을 통한 Batch Size 스케일링 잠금 해제\n00:24:10 컴퓨트 요구사항: 단일 GPU에서의 SOTA\n00:21:02 향후 방향: Distillation, VLMs, 그리고 Hierarchical Planning\n00:27:15 마무리: RL 스케일링의 통념에 도전"
}