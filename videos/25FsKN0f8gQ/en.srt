1
00:00:12,060 --> 00:00:14,280
welcome to lane space uh we are basically

2
00:00:14,280 --> 00:00:17,360
trying to provide the best optimal sort of

3
00:00:17,360 --> 00:00:17,760
podcast

4
00:00:17,760 --> 00:00:18,720
experience of europe's for people who are

5
00:00:18,720 --> 00:00:19,680
not

6
00:00:19,680 --> 00:00:22,160
here uh and congrats on your paper how's

7
00:00:22,160 --> 00:00:22,400
it feel

8
00:00:22,400 --> 00:00:25,120
yeah it was very exciting um yeah we

9
00:00:25,120 --> 00:00:26,150
had a poster yesterday and then today

10
00:00:26,150 --> 00:00:27,180
we'll

11
00:00:27,180 --> 00:00:27,900
have an oral talk

12
00:00:27,900 --> 00:00:30,060
were you just like mobbed oh yeah there

13
00:00:30,060 --> 00:00:31,100
was a lot of people it's like three

14
00:00:31,100 --> 00:00:31,860
hours straight of like

15
00:00:31,860 --> 00:00:33,480
you know like waves of people to like

16
00:00:33,480 --> 00:00:35,660
that where we were trying to stupid so

17
00:00:35,660 --> 00:00:36,540
i've never received the

18
00:00:36,540 --> 00:00:38,840
best paper did you just find out on

19
00:00:38,840 --> 00:00:40,860
the website like what uh oh i just

20
00:00:40,860 --> 00:00:42,940
like woke up one day and

21
00:00:42,940 --> 00:00:45,400
like checked my email and then ah they

22
00:00:45,400 --> 00:00:47,000
just thought yeah they was like oh like

23
00:00:47,000 --> 00:00:48,040
that's like i just saw

24
00:00:48,040 --> 00:00:49,310
you know you've like been awarded best

25
00:00:49,310 --> 00:00:50,580
paper

26
00:00:50,580 --> 00:00:53,140
but maybe you know from the reviews as

27
00:00:53,140 --> 00:00:53,500
well right

28
00:00:53,500 --> 00:00:55,700
so yeah we know from the reviews that

29
00:00:55,700 --> 00:00:58,240
we did well um but there's a difference

30
00:00:58,240 --> 00:00:59,020
between like doing well

31
00:00:59,020 --> 00:01:01,260
on the reviews and getting best paper so

32
00:01:01,260 --> 00:01:02,290
right part we didn't actually know yeah

33
00:01:02,290 --> 00:01:03,320
okay

34
00:01:03,320 --> 00:01:04,400
so i i skipped a

35
00:01:04,400 --> 00:01:05,720
little bit uh maybe we can go sort

36
00:01:05,720 --> 00:01:07,480
of um one by one and and sort

37
00:01:07,480 --> 00:01:09,460
of introduce um you know who you are

38
00:01:09,460 --> 00:01:10,800
and what you did on on on the

39
00:01:10,800 --> 00:01:14,040
team i'm kevin i was an undergrad from

40
00:01:14,040 --> 00:01:15,440
from princeton and i just graduated

41
00:01:15,440 --> 00:01:19,040
and yeah i guess i led the project

42
00:01:19,040 --> 00:01:21,300
like started the project and then i was

43
00:01:21,300 --> 00:01:21,920
very happy to collaborate

44
00:01:21,920 --> 00:01:24,040
with ishan and nicole and ben also um

45
00:01:24,040 --> 00:01:26,100
right and were you in like the same

46
00:01:26,100 --> 00:01:27,080
research group like how

47
00:01:27,080 --> 00:01:29,180
do you how that's your yeah uh social

48
00:01:29,180 --> 00:01:31,680
context so so yeah so we're all from

49
00:01:31,680 --> 00:01:33,800
princeton yeah um thanks

50
00:01:33,800 --> 00:01:35,840
to alan for booking you guys so this

51
00:01:35,840 --> 00:01:36,880
project actually started from like an iw

52
00:01:36,880 --> 00:01:37,920
seminar

53
00:01:37,920 --> 00:01:39,340
so uh like like

54
00:01:39,340 --> 00:01:40,600
independent work research seminar that ben

55
00:01:40,600 --> 00:01:41,860
was teaching

56
00:01:41,860 --> 00:01:44,100
and this was like actually like like one

57
00:01:44,100 --> 00:01:45,410
of my first experiences in like ml

58
00:01:45,410 --> 00:01:46,720
research

59
00:01:46,720 --> 00:01:48,520
um so it was really valuable to like

60
00:01:48,520 --> 00:01:49,160
get that experience

61
00:01:49,160 --> 00:01:51,700
and then ishan was also in that seminar

62
00:01:51,700 --> 00:01:52,740
and working on adjacent things so we

63
00:01:52,740 --> 00:01:53,780
collaborated

64
00:01:53,780 --> 00:01:56,240
um a lot during that seminar and then

65
00:01:56,240 --> 00:01:58,300
yeah the project turned out to have some

66
00:01:58,300 --> 00:01:59,020
pretty cool results

67
00:01:59,020 --> 00:02:00,540
and then later on also like the halt

68
00:02:00,540 --> 00:02:02,980
working on sort of similar things also um

69
00:02:02,980 --> 00:02:03,840
joined it on the project

70
00:02:03,840 --> 00:02:05,100
and became like a good collaboration yeah

71
00:02:05,100 --> 00:02:06,360
and

72
00:02:06,360 --> 00:02:07,860
um i i don't know if any of

73
00:02:07,860 --> 00:02:08,640
you guys want to want to

74
00:02:08,640 --> 00:02:10,500
chime in and on like other elements of

75
00:02:10,500 --> 00:02:12,350
coming into like deciding on this uh

76
00:02:12,350 --> 00:02:14,200
problem

77
00:02:14,200 --> 00:02:15,240
so it's like

78
00:02:15,240 --> 00:02:16,764
probably my lab works on deep

79
00:02:16,764 --> 00:02:17,920
reinforcement learning

80
00:02:17,920 --> 00:02:19,720
but historically deep meant like two or

81
00:02:19,720 --> 00:02:21,520
three

82
00:02:21,520 --> 00:02:21,820
or

83
00:02:21,820 --> 00:02:24,040
four layers not one thousand when kevin

84
00:02:24,040 --> 00:02:26,260
and

85
00:02:26,260 --> 00:02:27,060
sean mentioned they want to try really

86
00:02:27,060 --> 00:02:27,580
deep

87
00:02:27,580 --> 00:02:28,020
networks

88
00:02:28,020 --> 00:02:29,880
it's kind of skeptical it was going to

89
00:02:29,880 --> 00:02:30,790
work i've tried this before it doesn't

90
00:02:30,790 --> 00:02:31,700
work

91
00:02:31,700 --> 00:02:32,280
other papers have

92
00:02:32,280 --> 00:02:33,080
tried this before and it doesn't gonna

93
00:02:33,080 --> 00:02:33,840
work

94
00:02:33,840 --> 00:02:36,480
so i was very very skeptical starting out

95
00:02:36,480 --> 00:02:36,820
i don't know

96
00:02:36,820 --> 00:02:38,600
if i conveyed this at the time but

97
00:02:38,600 --> 00:02:41,360
that was my prior going in because but

98
00:02:41,360 --> 00:02:42,760
do you view your job as like

99
00:02:42,760 --> 00:02:44,010
screening or like hey guys this is

100
00:02:44,010 --> 00:02:45,260
probably

101
00:02:45,260 --> 00:02:46,060
isn't gonna work you should try a

102
00:02:46,060 --> 00:02:46,360
different

103
00:02:46,360 --> 00:02:46,840
idea you

104
00:02:46,840 --> 00:02:47,890
know like or should you be encouraging

105
00:02:47,890 --> 00:02:48,940
even

106
00:02:48,940 --> 00:02:53,100
if it's dumb it's selecting bets yeah and

107
00:02:53,100 --> 00:02:53,660
this was a

108
00:02:53,660 --> 00:02:55,380
bet i was willing to make what what

109
00:02:55,380 --> 00:02:57,980
made you willing to make a bet it

110
00:02:57,980 --> 00:03:00,120
seemed relatively low cost uh in

111
00:03:00,120 --> 00:03:03,540
that we mihal in particular had spent the

112
00:03:03,540 --> 00:03:04,480
past year developing infrastructure that

113
00:03:04,480 --> 00:03:05,420
made a lot

114
00:03:05,420 --> 00:03:05,700
easier

115
00:03:05,700 --> 00:03:08,760
to run some of these experiments and the

116
00:03:08,760 --> 00:03:09,920
precedent was deep renowers should do a

117
00:03:09,920 --> 00:03:11,080
whole

118
00:03:11,080 --> 00:03:11,960
lot better like

119
00:03:11,960 --> 00:03:13,170
that's what the deep learning revolution

120
00:03:13,170 --> 00:03:14,380
has been

121
00:03:14,380 --> 00:03:15,620
over the last day yeah i know why

122
00:03:15,620 --> 00:03:16,280
do we stop making

123
00:03:16,280 --> 00:03:17,620
them deeper and reinforcement learning was

124
00:03:17,620 --> 00:03:18,960
like this

125
00:03:18,960 --> 00:03:21,280
one anomaly where we continue to use these

126
00:03:21,280 --> 00:03:21,580
really

127
00:03:21,580 --> 00:03:22,710
shallow networks and that's particularly

128
00:03:22,710 --> 00:03:23,840
true in the

129
00:03:23,840 --> 00:03:24,680
settings that we were looking at where

130
00:03:24,680 --> 00:03:25,520
you're

131
00:03:25,520 --> 00:03:26,730
starting from scratch you're starting from

132
00:03:26,730 --> 00:03:27,940
nothing any

133
00:03:27,940 --> 00:03:28,800
other perspectives you guys want to chime

134
00:03:28,800 --> 00:03:29,660
in

135
00:03:29,660 --> 00:03:31,600
with i guess maybe i should just go

136
00:03:31,600 --> 00:03:34,040
over like an overview of our project yes

137
00:03:34,040 --> 00:03:36,220
okay sorry yes so the way that

138
00:03:36,220 --> 00:03:38,160
kind of view our project um is that

139
00:03:38,160 --> 00:03:39,780
if you look at the landscape of deep

140
00:03:39,780 --> 00:03:41,000
learning you know you have

141
00:03:41,000 --> 00:03:44,520
nlp like language vision and then rl and

142
00:03:44,520 --> 00:03:46,200
like as ben kind of alluded to you

143
00:03:46,200 --> 00:03:47,540
know like in in language

144
00:03:47,540 --> 00:03:49,760
in vision we've sort of converged to these

145
00:03:49,760 --> 00:03:50,740
like paradigms of scaling to massive

146
00:03:50,740 --> 00:03:51,720
networks right

147
00:03:51,720 --> 00:03:51,860
like

148
00:03:51,860 --> 00:03:52,660
hundreds of billions of parameters

149
00:03:52,660 --> 00:03:53,440
trillions of parameters

150
00:03:53,440 --> 00:03:56,320
and there's been you know a lot a

151
00:03:56,320 --> 00:03:56,620
lot

152
00:03:56,620 --> 00:03:57,420
gained from in deep learning from from

153
00:03:57,420 --> 00:03:58,100
that

154
00:03:58,100 --> 00:04:00,000
right and then but then it seems like

155
00:04:00,000 --> 00:04:00,960
in the third sort of

156
00:04:00,960 --> 00:04:03,360
branch of deep learning in deep rl that

157
00:04:03,360 --> 00:04:05,060
has not yet been the case um like

158
00:04:05,060 --> 00:04:06,040
i was very surprised like

159
00:04:06,040 --> 00:04:08,340
coming into some like you know ben's class

160
00:04:08,340 --> 00:04:09,620
and seminar when i was looking at the

161
00:04:09,620 --> 00:04:10,440
networks oh why

162
00:04:10,440 --> 00:04:11,680
were you just using like a simple like

163
00:04:11,680 --> 00:04:14,000
two-layer mlp for like these frontier sort

164
00:04:14,000 --> 00:04:14,980
of you know state

165
00:04:14,980 --> 00:04:17,180
of the rl algorithms um and so i

166
00:04:17,180 --> 00:04:20,340
was very curious like can we design rl

167
00:04:20,340 --> 00:04:21,760
algorithms can we sort of put

168
00:04:21,760 --> 00:04:23,820
together a recipe for rl that can allow

169
00:04:23,820 --> 00:04:25,420
it to scale in potentially you know

170
00:04:25,420 --> 00:04:27,020
analogous

171
00:04:27,020 --> 00:04:27,420
ways that

172
00:04:27,420 --> 00:04:28,570
language envisioned my skill and so what

173
00:04:28,570 --> 00:04:29,720
we

174
00:04:29,720 --> 00:04:32,360
did is that we know that traditional rl

175
00:04:32,360 --> 00:04:32,800
like let's say

176
00:04:32,800 --> 00:04:34,050
like value value-based rl doesn't really

177
00:04:34,050 --> 00:04:35,300
scale

178
00:04:35,300 --> 00:04:36,100
right this is pretty clear from the

179
00:04:36,100 --> 00:04:36,580
literature

180
00:04:36,580 --> 00:04:38,340
so we tried a different approach of rl

181
00:04:38,340 --> 00:04:39,920
um called self-supervised rl where instead

182
00:04:39,920 --> 00:04:41,500
of

183
00:04:41,500 --> 00:04:41,980
learning like a

184
00:04:41,980 --> 00:04:43,552
value function we're learning

185
00:04:43,552 --> 00:04:45,340
representations of states actions

186
00:04:45,340 --> 00:04:47,260
and future states such that the

187
00:04:47,260 --> 00:04:49,091
representations along the same trajectory

188
00:04:49,091 --> 00:04:49,940
are pushed together

189
00:04:49,940 --> 00:04:50,860
the representations along different

190
00:04:50,860 --> 00:04:51,690
trajectories are pushed apart and this is

191
00:04:51,690 --> 00:04:52,520
just

192
00:04:52,520 --> 00:04:55,520
like a different approach to uh rl that

193
00:04:55,520 --> 00:04:56,360
allows us to

194
00:04:56,360 --> 00:04:57,260
learn in a self-supervised manner so

195
00:04:57,260 --> 00:04:58,160
there's

196
00:04:58,160 --> 00:05:00,700
we can solve task reach goals without any

197
00:05:00,700 --> 00:05:01,060
human

198
00:05:01,060 --> 00:05:03,420
crafted reward single and so we know that

199
00:05:03,420 --> 00:05:04,320
self-supervised learning is scalable in

200
00:05:04,320 --> 00:05:05,220
these

201
00:05:05,220 --> 00:05:06,330
different areas and if deep learning so

202
00:05:06,330 --> 00:05:07,440
can

203
00:05:07,440 --> 00:05:09,210
self-supervised rl scale in similar ways

204
00:05:09,210 --> 00:05:10,980
when

205
00:05:10,980 --> 00:05:11,080
we

206
00:05:11,080 --> 00:05:11,880
first tried it it actually didn't work

207
00:05:11,880 --> 00:05:12,680
like

208
00:05:12,680 --> 00:05:14,448
we've made the networks deeper the

209
00:05:14,448 --> 00:05:15,280
performance like

210
00:05:15,280 --> 00:05:15,540
totally

211
00:05:15,540 --> 00:05:17,880
degraded but then we also but then i

212
00:05:17,880 --> 00:05:19,220
separately was like there's also some

213
00:05:19,220 --> 00:05:20,560
other work

214
00:05:20,560 --> 00:05:22,000
like um in in

215
00:05:22,000 --> 00:05:23,180
our literature like we tried like residual

216
00:05:23,180 --> 00:05:24,360
connections

217
00:05:24,360 --> 00:05:26,340
uh and it is other a few other

218
00:05:26,340 --> 00:05:27,190
architectural components that we had to

219
00:05:27,190 --> 00:05:28,040
put into

220
00:05:28,040 --> 00:05:30,000
the recipe and then all of a sudden

221
00:05:30,000 --> 00:05:30,880
like one day

222
00:05:30,880 --> 00:05:33,340
like i ran this experiment and there was

223
00:05:33,340 --> 00:05:34,140
like this one environment in which there

224
00:05:34,140 --> 00:05:34,880
was

225
00:05:34,880 --> 00:05:35,060
like

226
00:05:35,060 --> 00:05:36,160
like going from like like doubling the

227
00:05:36,160 --> 00:05:37,260
depth

228
00:05:37,260 --> 00:05:38,260
didn't really do anything but like

229
00:05:38,260 --> 00:05:39,260
doubling the

230
00:05:39,260 --> 00:05:39,720
depth again

231
00:05:40,160 --> 00:05:42,336
with these different components suddenly

232
00:05:42,336 --> 00:05:43,860
like skyrocketed performance

233
00:05:43,860 --> 00:05:44,980
in this one environment

234
00:05:44,980 --> 00:05:47,760
getting this to work was very non-trivial

235
00:05:47,760 --> 00:05:49,100
in the sense that like usually when we

236
00:05:49,100 --> 00:05:49,700
think about doing

237
00:05:49,700 --> 00:05:51,887
hyperparameter optimization we try

238
00:05:51,887 --> 00:05:52,980
changing a see if

239
00:05:52,980 --> 00:05:54,840
it makes it better try changing b see

240
00:05:54,840 --> 00:05:55,100
whether

241
00:05:55,100 --> 00:05:57,260
it makes it better and if we just

242
00:05:57,260 --> 00:05:59,520
made the depth bigger makes it worse we

243
00:05:59,520 --> 00:06:00,480
just had residual connections

244
00:06:00,480 --> 00:06:02,060
didn't make it better and it was really

245
00:06:02,060 --> 00:06:03,300
this combination of factors that kevin and

246
00:06:03,300 --> 00:06:04,540
sean

247
00:06:04,540 --> 00:06:04,980
figured

248
00:06:04,980 --> 00:06:07,440
out that really made this work and as

249
00:06:07,440 --> 00:06:08,920
a precursor to that we also try scaling

250
00:06:08,920 --> 00:06:09,820
along different dimensions

251
00:06:09,820 --> 00:06:12,660
so scaling the batch size uh scaling the

252
00:06:12,660 --> 00:06:13,900
the width of the network so the hidden

253
00:06:13,900 --> 00:06:15,140
layers in effect

254
00:06:15,800 --> 00:06:17,500
yeah pretty much kind of similar to just

255
00:06:17,500 --> 00:06:18,940
scaling depth naively yeah um and then

256
00:06:18,940 --> 00:06:20,380
once

257
00:06:20,380 --> 00:06:20,680
we started

258
00:06:20,680 --> 00:06:22,371
introducing residual connections layer

259
00:06:22,371 --> 00:06:23,840
norm these specific architectural

260
00:06:23,840 --> 00:06:25,400
choices that's when we saw

261
00:06:25,400 --> 00:06:27,413
these significant jumps in performance

262
00:06:27,413 --> 00:06:28,420
like these critical

263
00:06:28,420 --> 00:06:29,800
depths at which performance multiplies by

264
00:06:29,800 --> 00:06:31,180
a

265
00:06:31,180 --> 00:06:32,290
pretty huge factor and that's where we

266
00:06:32,290 --> 00:06:33,400
really

267
00:06:33,400 --> 00:06:35,378
noticed like unlocking some significant

268
00:06:35,378 --> 00:06:36,240
performance gains

269
00:06:36,900 --> 00:06:38,270
as opposed to scaling just along with

270
00:06:38,270 --> 00:06:39,640
which

271
00:06:39,640 --> 00:06:41,110
did yield some performance improvements um

272
00:06:41,110 --> 00:06:42,580
but when

273
00:06:42,580 --> 00:06:42,700
you

274
00:06:42,700 --> 00:06:44,220
look at the number of parameters that your

275
00:06:44,220 --> 00:06:46,740
network has as you grow with it's roughly

276
00:06:46,740 --> 00:06:47,660
a quadratic as

277
00:06:47,660 --> 00:06:48,560
opposed to something like growing depth so

278
00:06:48,560 --> 00:06:49,460
it's

279
00:06:49,460 --> 00:06:50,430
more in some sense it's more parameter

280
00:06:50,430 --> 00:06:51,400
efficient

281
00:06:51,400 --> 00:06:51,700
also

282
00:06:51,700 --> 00:06:52,870
more sample efficient from the experiments

283
00:06:52,870 --> 00:06:54,040
that we

284
00:06:54,040 --> 00:06:56,900
conducted nice um in some ways you're sort

285
00:06:56,900 --> 00:06:57,020
of

286
00:06:57,020 --> 00:06:59,480
replicating stuff that is seen in the wild

287
00:06:59,480 --> 00:07:02,140
but on on a very small model that

288
00:07:02,140 --> 00:07:03,660
you can study is that would

289
00:07:03,660 --> 00:07:04,720
you would you say that's yeah so i

290
00:07:04,720 --> 00:07:06,260
kind of add to what kevin said earlier

291
00:07:06,260 --> 00:07:07,760
we saw these huge performance

292
00:07:07,760 --> 00:07:09,928
improvements in language models image

293
00:07:09,928 --> 00:07:11,100
generation models by

294
00:07:11,100 --> 00:07:12,440
making them larger making them deeper

295
00:07:12,440 --> 00:07:13,720
which seems very intuitive yeah and so

296
00:07:13,720 --> 00:07:15,000
that's

297
00:07:15,000 --> 00:07:17,286
why our work we draw from like

298
00:07:17,286 --> 00:07:18,200
foundational

299
00:07:18,200 --> 00:07:18,700
research

300
00:07:18,700 --> 00:07:20,820
right like uh residual networks which

301
00:07:20,820 --> 00:07:21,680
employ residual

302
00:07:21,680 --> 00:07:23,875
connections to avoid uh vanishing

303
00:07:23,875 --> 00:07:24,740
gradients and

304
00:07:24,740 --> 00:07:25,800
that's something that we show in some of

305
00:07:25,800 --> 00:07:27,660
our ablations in our in our paper further

306
00:07:27,660 --> 00:07:28,000
down

307
00:07:28,000 --> 00:07:29,050
it's probably in the appendices where you

308
00:07:29,050 --> 00:07:30,100
did

309
00:07:30,100 --> 00:07:31,110
experiments without these residual

310
00:07:31,110 --> 00:07:32,120
connections

311
00:07:32,120 --> 00:07:33,270
and so it's sort of borrowing these

312
00:07:33,270 --> 00:07:34,420
concepts

313
00:07:34,420 --> 00:07:35,750
that have existed in other fields and

314
00:07:35,750 --> 00:07:37,080
applying

315
00:07:37,080 --> 00:07:37,260
them

316
00:07:37,260 --> 00:07:39,560
to this setting with uh rl and showing

317
00:07:39,560 --> 00:07:41,840
that it works before ben has to have

318
00:07:41,840 --> 00:07:43,040
to go i'll leave the sort

319
00:07:43,040 --> 00:07:45,400
of last word uh to him what additional

320
00:07:45,400 --> 00:07:47,360
work does this inspire that like that you

321
00:07:47,360 --> 00:07:49,560
want to push on next i think

322
00:07:49,560 --> 00:07:50,360
there's one thing i'd clarify about the

323
00:07:50,360 --> 00:07:51,020
paper

324
00:07:51,020 --> 00:07:52,330
and then i'll directly answer the question

325
00:07:52,330 --> 00:07:53,640
yes

326
00:07:53,640 --> 00:07:53,880
i think

327
00:07:53,880 --> 00:07:55,240
the thing i might clarify about the paper

328
00:07:55,240 --> 00:07:56,200
is i think a lot of people reading

329
00:07:56,200 --> 00:07:57,800
the title are like wow big networks

330
00:07:57,800 --> 00:07:58,840
they're great i'll take big networks you

331
00:07:58,840 --> 00:07:59,880
solved

332
00:07:59,880 --> 00:08:01,240
it now we can just go yeah just

333
00:08:01,240 --> 00:08:01,700
take big networks

334
00:08:01,700 --> 00:08:03,900
add them to ppo add them to sac

335
00:08:03,900 --> 00:08:04,890
add them to your favorite reinforcement

336
00:08:04,890 --> 00:08:05,880
learning algorithm

337
00:08:05,880 --> 00:08:06,560
but i think

338
00:08:06,560 --> 00:08:07,360
that's actually not the main conclusion i

339
00:08:07,360 --> 00:08:07,980
think

340
00:08:07,980 --> 00:08:09,130
the main conclusion is that using big

341
00:08:09,130 --> 00:08:10,280
networks

342
00:08:10,280 --> 00:08:11,060
not only

343
00:08:11,060 --> 00:08:13,475
requires these architectural tricks but

344
00:08:13,475 --> 00:08:14,280
also as kevin

345
00:08:14,280 --> 00:08:15,260
mentioned before it requires using a

346
00:08:15,260 --> 00:08:16,240
different

347
00:08:16,240 --> 00:08:19,117
objective this objective doesn't actually

348
00:08:19,117 --> 00:08:20,100
use rewards in

349
00:08:20,100 --> 00:08:22,280
it and so there's another word in the

350
00:08:22,280 --> 00:08:23,670
title reinforcement learning that also

351
00:08:23,670 --> 00:08:25,060
might be a

352
00:08:25,060 --> 00:08:27,460
little bit of a misnomer because we aren't

353
00:08:27,460 --> 00:08:28,880
directly trying to maximize rewards our

354
00:08:28,880 --> 00:08:30,300
code doesn't

355
00:08:30,300 --> 00:08:31,440
have a line of code saying maximize

356
00:08:31,440 --> 00:08:32,580
rewards

357
00:08:32,580 --> 00:08:35,400
here and so is at the end of

358
00:08:35,400 --> 00:08:36,260
the day this a reinforcement learning

359
00:08:36,260 --> 00:08:37,120
method i

360
00:08:37,120 --> 00:08:37,440
don't know

361
00:08:37,440 --> 00:08:39,460
it looks much more similar to the self

362
00:08:39,460 --> 00:08:40,770
-supervised methods in other areas of

363
00:08:40,770 --> 00:08:42,080
machine learning

364
00:08:42,080 --> 00:08:43,060
and so i

365
00:08:43,060 --> 00:08:44,250
think that the method the work really

366
00:08:44,250 --> 00:08:45,440
stands

367
00:08:45,440 --> 00:08:46,710
in some sort of interesting intersection

368
00:08:46,710 --> 00:08:47,980
of reinforcement

369
00:08:47,980 --> 00:08:51,310
learning and self-supervised learning

370
00:08:51,310 --> 00:08:52,660
research and we

371
00:08:52,660 --> 00:08:54,520
had this little figure on the bottom left

372
00:08:54,520 --> 00:08:55,160
of

373
00:08:55,160 --> 00:08:58,000
the poster which was the screenshot of a

374
00:08:58,000 --> 00:08:59,220
slide from young lakun talking about how

375
00:08:59,220 --> 00:09:00,440
to

376
00:09:00,440 --> 00:09:01,020
build intelligent

377
00:09:01,020 --> 00:09:02,030
systems and whether that's going to be

378
00:09:02,030 --> 00:09:03,040
done

379
00:09:03,040 --> 00:09:04,460
by unsupervised learning or supervised

380
00:09:04,460 --> 00:09:05,880
learning and

381
00:09:05,880 --> 00:09:06,910
reinforcement learning and i think what

382
00:09:06,910 --> 00:09:07,940
our paper

383
00:09:07,940 --> 00:09:09,250
really suggests is that the boundary

384
00:09:09,250 --> 00:09:10,560
between these

385
00:09:10,560 --> 00:09:12,460
things is really blurry and maybe the keys

386
00:09:12,460 --> 00:09:13,260
to building intelligent systems are going

387
00:09:13,260 --> 00:09:13,940
to be

388
00:09:13,940 --> 00:09:14,240
leveraging

389
00:09:14,240 --> 00:09:16,660
insights from all of them yeah the layer

390
00:09:16,660 --> 00:09:20,400
kick exactly uh well thank you for your

391
00:09:20,400 --> 00:09:21,500
time i know i know you have to

392
00:09:21,500 --> 00:09:21,660
go

393
00:09:21,660 --> 00:09:24,060
soon yeah thank you so much for coming

394
00:09:24,060 --> 00:09:26,780
i think that that insight of like blurring

395
00:09:26,780 --> 00:09:27,760
things is interesting

396
00:09:27,760 --> 00:09:29,320
i don't know if you like you were

397
00:09:29,320 --> 00:09:30,460
talking about so like uh the abstraction

398
00:09:30,460 --> 00:09:31,600
layer

399
00:09:31,600 --> 00:09:32,520
of representation

400
00:09:32,520 --> 00:09:33,750
learning i don't know if that triggers

401
00:09:33,750 --> 00:09:34,980
anything

402
00:09:34,980 --> 00:09:37,680
in terms of like the mix between self

403
00:09:37,680 --> 00:09:38,440
-supervised and

404
00:09:38,440 --> 00:09:39,530
reinforcement learning is that is there

405
00:09:39,530 --> 00:09:40,620
something fundamental

406
00:09:40,620 --> 00:09:42,740
that you've discovered or that we've

407
00:09:42,740 --> 00:09:43,820
that people don't understand when they

408
00:09:43,820 --> 00:09:44,900
when they

409
00:09:44,900 --> 00:09:46,700
read the paper yeah i think the best

410
00:09:46,700 --> 00:09:47,260
way that i would

411
00:09:47,260 --> 00:09:50,100
explain it is that we know that standard

412
00:09:50,100 --> 00:09:52,540
rl is not super scalable and so like

413
00:09:52,540 --> 00:09:53,680
why can this a different

414
00:09:53,680 --> 00:09:54,900
approach or different objective rl be

415
00:09:54,900 --> 00:09:56,120
scalable i

416
00:09:56,120 --> 00:09:57,170
think it's because we're fundamentally

417
00:09:57,170 --> 00:09:58,220
shifting the

418
00:09:58,780 --> 00:10:00,340
burden of learning from something like

419
00:10:00,340 --> 00:10:01,900
like q

420
00:10:01,900 --> 00:10:02,850
learning or like regressing to like td

421
00:10:02,850 --> 00:10:03,800
errors

422
00:10:03,800 --> 00:10:04,240
which

423
00:10:04,240 --> 00:10:05,780
we know is quite spurious and noisy and

424
00:10:05,780 --> 00:10:07,714
biased to fundamentally like a

425
00:10:07,714 --> 00:10:09,520
classification problem we're

426
00:10:09,520 --> 00:10:10,900
trying to classify whether future states

427
00:10:10,900 --> 00:10:12,280
is along

428
00:10:12,280 --> 00:10:13,130
the same trajectory or along a different

429
00:10:13,130 --> 00:10:13,980
trajectory

430
00:10:14,540 --> 00:10:15,640
and we do this with representation

431
00:10:15,640 --> 00:10:16,740
learning right

432
00:10:16,740 --> 00:10:18,640
and we know that classification

433
00:10:18,640 --> 00:10:19,620
cross-entry loss

434
00:10:19,620 --> 00:10:20,890
and representation learning is scalable in

435
00:10:20,890 --> 00:10:22,160
the deep

436
00:10:22,160 --> 00:10:22,960
learning literature right if we think

437
00:10:22,960 --> 00:10:23,760
about language

438
00:10:24,700 --> 00:10:26,920
and like some of the objectives there so

439
00:10:26,920 --> 00:10:30,060
in some sense we're kind of blurring the

440
00:10:30,060 --> 00:10:30,660
the lines we're

441
00:10:30,660 --> 00:10:31,610
doing reinforcement learning it's still an

442
00:10:31,610 --> 00:10:32,560
actor critic

443
00:10:32,560 --> 00:10:33,640
reinforcement learning algorithm it's like

444
00:10:33,640 --> 00:10:34,720
a goal

445
00:10:34,720 --> 00:10:36,763
condition reinforcement algorithm but the

446
00:10:36,763 --> 00:10:37,760
objective the burden

447
00:10:37,760 --> 00:10:39,940
of like learning of the of solving that

448
00:10:39,940 --> 00:10:40,160
rl

449
00:10:40,160 --> 00:10:41,420
task shifts to something that's more

450
00:10:41,420 --> 00:10:42,680
similar to

451
00:10:42,680 --> 00:10:43,980
objectives that you might see in language

452
00:10:43,980 --> 00:10:45,280
and

453
00:10:45,280 --> 00:10:46,560
vision that we know have scaled so much

454
00:10:46,560 --> 00:10:48,580
and so i think yeah i think that's

455
00:10:48,580 --> 00:10:49,500
like one of the fundamental

456
00:10:49,500 --> 00:10:52,280
insights that we've seen is that um it

457
00:10:52,280 --> 00:10:53,250
seems like by approaching rl in this

458
00:10:53,250 --> 00:10:54,220
different

459
00:10:54,220 --> 00:10:55,400
approach we're

460
00:10:55,400 --> 00:10:57,540
able to like get so much more out

461
00:10:57,540 --> 00:10:59,060
of we were able to scale our networks

462
00:10:59,060 --> 00:11:00,960
like significantly beyond what

463
00:11:00,960 --> 00:11:02,940
was like standard used in ara can i

464
00:11:02,940 --> 00:11:05,100
jump in i will just give a bit

465
00:11:05,100 --> 00:11:06,700
of more of context about the

466
00:11:06,700 --> 00:11:09,258
architecture because uh yeah we use

467
00:11:09,258 --> 00:11:10,500
another objective

468
00:11:10,500 --> 00:11:14,040
and the uh influences so the contrastive

469
00:11:14,040 --> 00:11:15,660
laws however the architecture is uh quite

470
00:11:15,660 --> 00:11:17,280
similar

471
00:11:17,280 --> 00:11:20,100
to the previous works uh of previous uh

472
00:11:20,100 --> 00:11:20,800
papers like

473
00:11:20,800 --> 00:11:24,320
bro or or uh simba simba v1 simba

474
00:11:24,320 --> 00:11:29,200
v2 uh simba v1 simba v2 so we

475
00:11:29,200 --> 00:11:31,000
we also tweaked a bit of this

476
00:11:31,000 --> 00:11:32,800
uh architecture however it's not that we

477
00:11:32,800 --> 00:11:34,600
like

478
00:11:34,600 --> 00:11:36,760
uh invented the wheel for the first time

479
00:11:36,760 --> 00:11:37,720
it's the

480
00:11:37,720 --> 00:11:39,060
merging between the architecture and the

481
00:11:39,060 --> 00:11:40,400
objective that

482
00:11:40,400 --> 00:11:43,520
makes the scale uh really uh like go

483
00:11:43,520 --> 00:11:44,460
up and

484
00:11:44,460 --> 00:11:46,700
and and performance follow the the scale i

485
00:11:46,700 --> 00:11:47,840
think that's something that we should uh

486
00:11:47,840 --> 00:11:48,980
probably

487
00:11:48,980 --> 00:11:50,000
mine deeper

488
00:11:50,000 --> 00:11:52,800
um do you think i guess like what

489
00:11:52,800 --> 00:11:53,940
domains what industry like you've applied

490
00:11:53,940 --> 00:11:55,080
it on

491
00:11:55,080 --> 00:11:56,000
multiple different

492
00:11:56,000 --> 00:11:58,120
uh types of networks that are or data

493
00:11:58,120 --> 00:11:59,590
sets is there a particular affinity that

494
00:11:59,590 --> 00:12:01,060
you

495
00:12:01,060 --> 00:12:01,820
think like has is

496
00:12:01,820 --> 00:12:04,100
like kind of low-hanging food yeah so

497
00:12:04,100 --> 00:12:05,180
actually if you look at a lot of

498
00:12:05,180 --> 00:12:07,080
our tasks they're particularly

499
00:12:07,080 --> 00:12:09,700
sort of like robotics tasks um so this

500
00:12:09,700 --> 00:12:12,420
is a person i'd be very curious about

501
00:12:12,420 --> 00:12:13,780
how a work like this could

502
00:12:13,780 --> 00:12:14,980
impact like the robotics field like my

503
00:12:14,980 --> 00:12:16,180
understanding

504
00:12:16,180 --> 00:12:17,760
of robotics is that a lot of robotics

505
00:12:17,760 --> 00:12:18,100
are now

506
00:12:18,100 --> 00:12:19,080
there's kind of multi a few different

507
00:12:19,080 --> 00:12:20,060
approaches

508
00:12:20,060 --> 00:12:22,220
like one approach is we want to train

509
00:12:22,220 --> 00:12:23,360
robots using

510
00:12:23,360 --> 00:12:24,160
imitation learning so we try to collect

511
00:12:24,160 --> 00:12:24,940
like

512
00:12:24,940 --> 00:12:26,560
an insane amount of data we have a

513
00:12:26,560 --> 00:12:27,300
ton of human

514
00:12:27,820 --> 00:12:29,320
supervision and we try to scale up this

515
00:12:29,320 --> 00:12:30,180
data and we're like learning with

516
00:12:30,180 --> 00:12:31,040
imitation learning

517
00:12:31,040 --> 00:12:32,440
like but on the other hand potentially

518
00:12:32,440 --> 00:12:33,840
like

519
00:12:33,840 --> 00:12:34,930
perhaps there's another approach which is

520
00:12:34,930 --> 00:12:36,020
like

521
00:12:36,020 --> 00:12:37,292
for example like goal condition

522
00:12:37,292 --> 00:12:38,440
reinforcement learning where

523
00:12:38,440 --> 00:12:39,900
we can actually train robotic

524
00:12:39,900 --> 00:12:41,070
agents and training rl agents to solve

525
00:12:41,070 --> 00:12:42,240
meaningful

526
00:12:42,240 --> 00:12:43,570
tasks with absolutely no human supervision

527
00:12:43,570 --> 00:12:44,900
no

528
00:12:44,900 --> 00:12:46,140
demonstration it's much more scalable yeah

529
00:12:46,140 --> 00:12:47,380
so yeah

530
00:12:47,380 --> 00:12:48,410
so this could serve as an alternate

531
00:12:48,410 --> 00:12:49,440
approach

532
00:12:49,440 --> 00:12:50,400
and perhaps instead of like scaling data

533
00:12:50,400 --> 00:12:51,360
like

534
00:12:51,360 --> 00:12:52,660
scaling manual like human supervision

535
00:12:52,660 --> 00:12:53,960
which

536
00:12:53,960 --> 00:12:56,140
which is you know not super scalable if

537
00:12:56,140 --> 00:12:58,040
there are ways to sort of make goal

538
00:12:58,040 --> 00:12:58,640
condition reinforcement

539
00:12:58,640 --> 00:12:59,820
learning scalable and like we can just

540
00:12:59,820 --> 00:13:01,000
scale

541
00:13:01,000 --> 00:13:01,860
the architecture or we can scale because

542
00:13:01,860 --> 00:13:02,720
you're

543
00:13:02,720 --> 00:13:03,620
focused on your objectives yeah right with

544
00:13:03,620 --> 00:13:04,520
with

545
00:13:04,520 --> 00:13:05,320
certain different objectives i think that

546
00:13:05,320 --> 00:13:05,980
could be

547
00:13:05,980 --> 00:13:07,580
very exciting and see how to see how

548
00:13:07,580 --> 00:13:09,360
that can affect a field like robotics for

549
00:13:09,360 --> 00:13:11,260
example yeah uh double

550
00:13:11,260 --> 00:13:13,020
click on on just one one thing on

551
00:13:13,020 --> 00:13:13,900
the efficiency which you guys are talking

552
00:13:13,900 --> 00:13:14,780
about

553
00:13:14,780 --> 00:13:15,780
i would expect

554
00:13:16,320 --> 00:13:18,640
the very deep the deeper it is there

555
00:13:18,640 --> 00:13:19,980
should be quadratically worse i am not

556
00:13:19,980 --> 00:13:21,320
familiar

557
00:13:21,320 --> 00:13:21,740
with like

558
00:13:21,740 --> 00:13:22,750
the the pre-existing literature i'm just

559
00:13:22,750 --> 00:13:23,760
like

560
00:13:23,760 --> 00:13:25,690
sort of working out intuitions but um

561
00:13:25,690 --> 00:13:27,620
basically

562
00:13:27,620 --> 00:13:28,420
uh what

563
00:13:28,420 --> 00:13:30,680
are the trade-offs that you've found that

564
00:13:30,680 --> 00:13:32,080
i think you might want to warn people

565
00:13:32,080 --> 00:13:33,640
about because because you

566
00:13:33,640 --> 00:13:34,510
you are the guy who mentioned efficiency

567
00:13:34,510 --> 00:13:35,380
so

568
00:13:35,380 --> 00:13:37,200
yeah sure sure yeah so i was referring

569
00:13:37,200 --> 00:13:37,680
to like one of the

570
00:13:37,680 --> 00:13:39,320
figures on our poster also in our paper

571
00:13:39,320 --> 00:13:40,310
where we compare like the number of

572
00:13:40,310 --> 00:13:41,300
parameters

573
00:13:41,300 --> 00:13:42,480
that models have

574
00:13:42,480 --> 00:13:44,220
as we see along the axis of depth

575
00:13:44,220 --> 00:13:46,080
and as we scale along the axis of

576
00:13:46,080 --> 00:13:47,480
width yeah from our baseline

577
00:13:47,480 --> 00:13:48,590
architecture the most baseline one would

578
00:13:48,590 --> 00:13:49,700
be like

579
00:13:49,700 --> 00:13:52,360
a width of 256 like the hidden layers

580
00:13:52,360 --> 00:13:53,320
of 256 neurons

581
00:13:53,320 --> 00:13:55,180
and then the depth is for four layers

582
00:13:55,180 --> 00:13:58,320
or hidden layers um and so the point

583
00:13:58,320 --> 00:13:59,040
i was making there is

584
00:13:59,040 --> 00:14:00,760
that when you scale along depth your the

585
00:14:00,760 --> 00:14:01,630
number of parameters that your model has

586
00:14:01,630 --> 00:14:02,500
is

587
00:14:02,500 --> 00:14:02,820
going to grow

588
00:14:02,820 --> 00:14:05,528
roughly linearly uh whereas with with

589
00:14:05,528 --> 00:14:06,480
you're making

590
00:14:06,480 --> 00:14:07,980
your network outputs wider and then the

591
00:14:07,980 --> 00:14:09,480
input

592
00:14:09,480 --> 00:14:10,080
to the net

593
00:14:10,080 --> 00:14:12,640
next network is also growing as well and

594
00:14:12,640 --> 00:14:13,440
so the the number of parameters your

595
00:14:13,440 --> 00:14:14,220
network's

596
00:14:14,220 --> 00:14:14,480
then going

597
00:14:14,480 --> 00:14:15,920
to have grows approximately quadratically

598
00:14:15,920 --> 00:14:17,360
and so one

599
00:14:17,360 --> 00:14:18,640
of the experiments we did was sort of

600
00:14:18,640 --> 00:14:19,160
examining

601
00:14:19,680 --> 00:14:21,280
as we grow the number of parameters in

602
00:14:21,280 --> 00:14:22,150
our model by scaling along these two

603
00:14:22,150 --> 00:14:23,020
different

604
00:14:23,020 --> 00:14:23,520
choices

605
00:14:23,980 --> 00:14:25,310
which one for the same like approximate

606
00:14:25,310 --> 00:14:26,640
number

607
00:14:26,640 --> 00:14:27,800
of parameters yields a better performance

608
00:14:27,800 --> 00:14:28,960
and the

609
00:14:28,960 --> 00:14:29,140
depth

610
00:14:29,140 --> 00:14:30,460
curve kind of goes like this it jumps

611
00:14:30,460 --> 00:14:31,680
up pretty fast that's like present

612
00:14:31,680 --> 00:14:32,900
throughout our

613
00:14:32,900 --> 00:14:33,900
paper for with

614
00:14:33,900 --> 00:14:35,180
it grows a little bit more slowly and

615
00:14:35,180 --> 00:14:36,740
so that the kind of takeaway from that

616
00:14:36,740 --> 00:14:38,220
is that if you

617
00:14:38,220 --> 00:14:39,560
are a bit more resource constrained

618
00:14:39,560 --> 00:14:40,900
scaling along

619
00:14:40,900 --> 00:14:41,700
depth might be better because there's

620
00:14:41,700 --> 00:14:42,180
fewer

621
00:14:42,180 --> 00:14:43,400
parameters with a smaller model to a

622
00:14:43,400 --> 00:14:44,620
smaller

623
00:14:44,620 --> 00:14:46,000
number tool learnable parameters width is

624
00:14:46,000 --> 00:14:47,380
expensive

625
00:14:47,380 --> 00:14:48,660
width is expensive exactly and in general

626
00:14:48,660 --> 00:14:49,940
of

627
00:14:49,940 --> 00:14:50,740
course like more parameters is also going

628
00:14:50,740 --> 00:14:51,340
to

629
00:14:51,340 --> 00:14:51,600
be more

630
00:14:51,600 --> 00:14:53,652
expensive so that's just like another

631
00:14:53,652 --> 00:14:54,540
consideration to

632
00:14:54,540 --> 00:14:55,640
think about when using these networks and

633
00:14:55,640 --> 00:14:56,740
suppose

634
00:14:56,740 --> 00:14:59,280
yeah any other sort of rules of thumbs

635
00:14:59,280 --> 00:15:00,720
like that that i can extract that this

636
00:15:00,720 --> 00:15:01,400
is just the most basic

637
00:15:01,400 --> 00:15:03,640
one that i could think of yeah uh

638
00:15:03,640 --> 00:15:05,680
i don't know if there's any others yeah

639
00:15:05,680 --> 00:15:07,140
i guess like your original

640
00:15:07,140 --> 00:15:09,240
question of like the trade-offs um like

641
00:15:09,240 --> 00:15:10,260
one of the trade-offs one of the

642
00:15:10,260 --> 00:15:11,180
limitations that we say is

643
00:15:11,180 --> 00:15:11,980
like obviously if you make the networks

644
00:15:11,980 --> 00:15:12,620
bigger

645
00:15:12,620 --> 00:15:15,140
the it will take longer to run right

646
00:15:15,140 --> 00:15:17,100
so if you like

647
00:15:17,100 --> 00:15:18,700
double the depth at some level of depth

648
00:15:18,700 --> 00:15:20,580
you you it might take like twice as

649
00:15:20,580 --> 00:15:21,820
much to like take make a

650
00:15:21,820 --> 00:15:23,120
forward pass through the network right

651
00:15:23,120 --> 00:15:24,420
however this

652
00:15:24,420 --> 00:15:26,520
is not so like within our paper like

653
00:15:26,520 --> 00:15:27,800
for most

654
00:15:27,800 --> 00:15:29,060
environments um we are able to like

655
00:15:29,060 --> 00:15:30,320
saturate

656
00:15:30,320 --> 00:15:31,750
like get to like almost perfect

657
00:15:31,750 --> 00:15:32,580
performance within

658
00:15:32,580 --> 00:15:33,480
just you know

659
00:15:33,480 --> 00:15:34,580
we don't even need to get to like

660
00:15:34,580 --> 00:15:35,540
a thousand layers like maybe just 64

661
00:15:35,540 --> 00:15:36,500
layers

662
00:15:36,500 --> 00:15:37,540
for example is

663
00:15:37,540 --> 00:15:40,840
sufficient um and in this regime like like

664
00:15:40,840 --> 00:15:41,670
the latency of the network is not

665
00:15:41,670 --> 00:15:42,500
necessarily

666
00:15:42,500 --> 00:15:42,740
actually

667
00:15:42,740 --> 00:15:44,190
even uh not necessarily like a significant

668
00:15:44,190 --> 00:15:45,640
bottleneck

669
00:15:45,640 --> 00:15:47,000
like you can imagine there's a lot of

670
00:15:47,000 --> 00:15:47,500
tasks in

671
00:15:47,500 --> 00:15:49,284
which especially in rl that like

672
00:15:49,284 --> 00:15:50,120
collecting data

673
00:15:50,120 --> 00:15:51,290
might be the bottleneck right and making

674
00:15:51,290 --> 00:15:52,460
four

675
00:15:52,460 --> 00:15:52,700
passes

676
00:15:52,700 --> 00:15:53,590
through our network may not be the

677
00:15:53,590 --> 00:15:54,480
bottleneck

678
00:15:54,480 --> 00:15:56,940
and so in our environment we in our

679
00:15:56,940 --> 00:15:57,340
research we

680
00:15:57,340 --> 00:15:59,297
specifically use the jacks gcrl

681
00:15:59,297 --> 00:16:00,560
environment which is

682
00:16:00,560 --> 00:16:01,800
a jack space gpu accelerator environment

683
00:16:01,800 --> 00:16:03,040
so we

684
00:16:03,040 --> 00:16:03,180
can

685
00:16:03,180 --> 00:16:05,543
collect like thousands of like environment

686
00:16:05,543 --> 00:16:06,500
trajectories like

687
00:16:06,500 --> 00:16:07,760
in parallel at the same time

688
00:16:07,760 --> 00:16:09,980
so that we're able to like make uh

689
00:16:09,980 --> 00:16:12,460
like oh this is built in right this

690
00:16:12,460 --> 00:16:13,760
is built in so that we can

691
00:16:13,760 --> 00:16:14,830
collect you know like like a thousand

692
00:16:14,830 --> 00:16:15,900
trajectories

693
00:16:15,900 --> 00:16:16,890
at the same time along all these

694
00:16:16,890 --> 00:16:17,880
environments

695
00:16:17,880 --> 00:16:18,140
and

696
00:16:18,140 --> 00:16:20,520
so um makes that make sure that like

697
00:16:20,520 --> 00:16:22,600
we have enough data to like saturate the

698
00:16:22,600 --> 00:16:23,340
learning from wow

699
00:16:24,000 --> 00:16:25,200
yeah that's like work they've been called

700
00:16:25,200 --> 00:16:26,400
okay

701
00:16:26,400 --> 00:16:27,840
and you i don't know if you want

702
00:16:27,840 --> 00:16:29,280
to explore expand upon

703
00:16:29,280 --> 00:16:32,440
that on the jargon zcrl uh maybe and

704
00:16:32,440 --> 00:16:33,240
you know most people are familiar with

705
00:16:33,240 --> 00:16:33,820
pytor

706
00:16:33,820 --> 00:16:34,340
is maybe less

707
00:16:34,340 --> 00:16:36,040
familiar with jacks uh with jacks i think

708
00:16:36,040 --> 00:16:37,800
jacks is getting the uh the traction

709
00:16:37,800 --> 00:16:39,560
especially

710
00:16:39,560 --> 00:16:40,540
in rl field

711
00:16:40,540 --> 00:16:42,937
because the in for online reinforcement

712
00:16:42,937 --> 00:16:43,920
learning getting

713
00:16:43,920 --> 00:16:46,760
as much data as you can is is

714
00:16:46,760 --> 00:16:47,080
the most

715
00:16:47,080 --> 00:16:47,980
important there's got to be a pytorch

716
00:16:47,980 --> 00:16:48,880
equivalence

717
00:16:48,880 --> 00:16:52,140
but anyway any tips for other people also

718
00:16:52,140 --> 00:16:52,540
exploring

719
00:16:52,540 --> 00:16:54,920
this kind of uh rollout yeah yeah so

720
00:16:54,920 --> 00:16:57,640
i think i can also recommend like uh

721
00:16:57,640 --> 00:16:58,880
for for gold conditioned

722
00:16:58,880 --> 00:17:00,150
rl i'm recommending jacks this year but

723
00:17:00,150 --> 00:17:01,420
there

724
00:17:01,420 --> 00:17:02,990
are also like multi-agent jacks uh

725
00:17:02,990 --> 00:17:04,560
implementation

726
00:17:04,560 --> 00:17:05,240
and

727
00:17:05,240 --> 00:17:07,660
other so going back to our paper if

728
00:17:07,660 --> 00:17:09,540
you look at the plots we only see

729
00:17:09,540 --> 00:17:11,580
this like huge performance

730
00:17:11,580 --> 00:17:15,840
increase when we cross like 50 uh millions

731
00:17:15,840 --> 00:17:19,660
of uh transitions uh gap so so i

732
00:17:19,660 --> 00:17:21,100
think the data is crucial

733
00:17:21,100 --> 00:17:23,540
like here yeah i guess even to build

734
00:17:23,540 --> 00:17:25,880
on that like i like drawing analogies to

735
00:17:25,880 --> 00:17:27,240
like successes in other

736
00:17:27,240 --> 00:17:29,980
areas of deep learning like for example in

737
00:17:29,980 --> 00:17:31,280
large language models the reason why we're

738
00:17:31,280 --> 00:17:32,580
able

739
00:17:32,580 --> 00:17:32,920
to scale

740
00:17:32,920 --> 00:17:34,680
to such large networks is that we found

741
00:17:34,680 --> 00:17:36,440
a paradigm in which we can leverage the

742
00:17:36,440 --> 00:17:37,220
entire internet scale

743
00:17:37,220 --> 00:17:40,020
of data is alert right and so data

744
00:17:40,020 --> 00:17:41,920
in rl traditionally has been hard to come

745
00:17:41,920 --> 00:17:44,440
by um but now with these like

746
00:17:44,440 --> 00:17:45,957
gpu accelerated environments we can

747
00:17:45,957 --> 00:17:46,780
collect hundreds of

748
00:17:46,780 --> 00:17:48,260
millions of times of the data within just

749
00:17:48,260 --> 00:17:48,500
a few

750
00:17:48,500 --> 00:17:50,700
hours and so i think that this serves

751
00:17:50,700 --> 00:17:52,660
as like a really good test bed for

752
00:17:52,660 --> 00:17:53,540
us to be able to also

753
00:17:53,540 --> 00:17:56,080
find ways to scale up um like network

754
00:17:56,080 --> 00:17:58,160
capacity and get similar kind of games i

755
00:17:58,160 --> 00:17:58,960
think i asked are you

756
00:17:58,960 --> 00:18:00,340
saying that you have a difference you

757
00:18:00,340 --> 00:18:01,720
would

758
00:18:01,720 --> 00:18:03,530
do pre-training differently in llms like

759
00:18:03,530 --> 00:18:05,340
what's

760
00:18:05,340 --> 00:18:05,900
the what's

761
00:18:05,900 --> 00:18:09,260
the difference uh objective now um yeah i

762
00:18:09,260 --> 00:18:10,900
mean very simply very simply the the

763
00:18:10,900 --> 00:18:12,540
paradigm

764
00:18:12,540 --> 00:18:12,840
that you're

765
00:18:12,840 --> 00:18:14,130
referencing is next word or next token

766
00:18:14,130 --> 00:18:15,420
right

767
00:18:15,420 --> 00:18:19,580
it's very robust how do you change that

768
00:18:19,580 --> 00:18:20,460
oh i'm not

769
00:18:20,460 --> 00:18:21,260
saying that we're changing i want to

770
00:18:21,260 --> 00:18:22,040
leverage

771
00:18:22,040 --> 00:18:24,840
insights from that to apply to all i

772
00:18:24,840 --> 00:18:25,660
feel like

773
00:18:25,660 --> 00:18:26,800
you should go the other way you think

774
00:18:26,800 --> 00:18:28,560
you should go the other way yeah maybe

775
00:18:28,560 --> 00:18:29,980
i mean i would be a very

776
00:18:29,980 --> 00:18:31,120
interesting research direction too but

777
00:18:31,120 --> 00:18:32,260
actually yeah even

778
00:18:32,260 --> 00:18:33,500
on that point like one of the things

779
00:18:33,500 --> 00:18:33,700
i was

780
00:18:33,700 --> 00:18:35,520
thinking about is that the way that our

781
00:18:35,520 --> 00:18:37,880
rl objective works is in some set it's

782
00:18:37,880 --> 00:18:38,620
not exactly next

783
00:18:39,120 --> 00:18:41,640
word prediction but it's kind of like next

784
00:18:41,640 --> 00:18:42,720
state prediction right you imagine you're

785
00:18:42,720 --> 00:18:43,800
at some

786
00:18:43,800 --> 00:18:44,040
current

787
00:18:44,040 --> 00:18:45,270
state and you're at some current action

788
00:18:45,270 --> 00:18:46,500
and

789
00:18:46,500 --> 00:18:47,580
we want to predict whether or not this

790
00:18:47,580 --> 00:18:48,060
future state

791
00:18:48,060 --> 00:18:50,400
this this certain state is a future state

792
00:18:50,400 --> 00:18:51,200
along the same trajectory or a different

793
00:18:51,200 --> 00:18:51,840
trajectory

794
00:18:52,240 --> 00:18:54,500
and so in some sense we are actually

795
00:18:54,500 --> 00:18:55,700
doing some sort of like implicit world

796
00:18:55,700 --> 00:18:56,900
model

797
00:18:56,900 --> 00:18:58,680
is it like uh you

798
00:18:58,680 --> 00:18:59,920
know like i don't know if that's a

799
00:18:59,920 --> 00:19:01,460
bad word these things or is or like

800
00:19:01,460 --> 00:19:02,760
in language you you do a cross

801
00:19:02,760 --> 00:19:03,560
entry loss to classify the next token

802
00:19:03,560 --> 00:19:04,160
right

803
00:19:04,160 --> 00:19:05,050
and here we're just doing a binary

804
00:19:05,050 --> 00:19:05,940
classification

805
00:19:05,940 --> 00:19:06,460
of like

806
00:19:06,460 --> 00:19:08,140
whether or not some next state is some

807
00:19:08,140 --> 00:19:09,280
yeah yeah it's a classification yeah yeah

808
00:19:09,280 --> 00:19:10,420
yeah

809
00:19:10,420 --> 00:19:11,340
and so i do

810
00:19:11,340 --> 00:19:13,600
see that there are some like sort of

811
00:19:13,600 --> 00:19:14,710
parallels here that perhaps we should dig

812
00:19:14,710 --> 00:19:15,820
into

813
00:19:15,820 --> 00:19:16,380
deeper and

814
00:19:16,380 --> 00:19:18,940
see like what is the core to of

815
00:19:18,940 --> 00:19:20,020
what enables deep learning to scale and

816
00:19:20,020 --> 00:19:21,100
then

817
00:19:21,100 --> 00:19:22,440
how can we like leverage

818
00:19:22,440 --> 00:19:23,560
that how can we distill those like

819
00:19:23,560 --> 00:19:24,680
insights

820
00:19:24,680 --> 00:19:25,620
and then apply those across like all

821
00:19:25,620 --> 00:19:26,560
different

822
00:19:26,560 --> 00:19:26,800
fields

823
00:19:26,800 --> 00:19:28,855
whether it's language or reinforcement

824
00:19:28,855 --> 00:19:29,720
learning yeah uh

825
00:19:29,720 --> 00:19:31,040
did you did you get my my meaning

826
00:19:31,040 --> 00:19:31,360
about the

827
00:19:31,360 --> 00:19:33,940
world model stuff yeah yeah actually and i

828
00:19:33,940 --> 00:19:35,600
i heard i think i might have heard

829
00:19:35,600 --> 00:19:36,400
professor eisenbach

830
00:19:36,400 --> 00:19:37,200
yesterday talking about this at a poster

831
00:19:37,200 --> 00:19:37,820
and

832
00:19:37,820 --> 00:19:39,640
he's explaining to a couple of people that

833
00:19:39,640 --> 00:19:40,300
because

834
00:19:40,300 --> 00:19:41,410
this is like doing representation learning

835
00:19:41,410 --> 00:19:42,520
and trying

836
00:19:42,520 --> 00:19:43,900
to learn these meaningful representations

837
00:19:44,360 --> 00:19:45,900
for a given state of action been for

838
00:19:45,900 --> 00:19:47,800
a given goal in some sense you can

839
00:19:47,800 --> 00:19:48,700
think of it almost like

840
00:19:48,700 --> 00:19:49,750
learning a model of environment learning a

841
00:19:49,750 --> 00:19:50,800
model

842
00:19:50,800 --> 00:19:52,580
of the world but without having to do

843
00:19:52,580 --> 00:19:53,220
any sort of like

844
00:19:53,220 --> 00:19:54,700
next frame prediction or stuff like that

845
00:19:54,700 --> 00:19:56,180
that's

846
00:19:56,180 --> 00:19:57,120
a little bit more high dimensional and

847
00:19:57,120 --> 00:19:58,060
complex

848
00:19:58,060 --> 00:20:00,120
yeah yeah i would think like

849
00:20:00,120 --> 00:20:02,160
um the the angle that i'm trying to

850
00:20:02,160 --> 00:20:05,640
think about and push is instead of learn

851
00:20:05,640 --> 00:20:06,860
the next world they're

852
00:20:06,860 --> 00:20:07,990
basically like generate a number of

853
00:20:07,990 --> 00:20:09,120
candidates possible

854
00:20:09,120 --> 00:20:11,560
worlds and classify them uh to your point

855
00:20:11,560 --> 00:20:14,020
uh which is exactly how i do things

856
00:20:14,020 --> 00:20:15,680
let's say i'm playing poker and i'm trying

857
00:20:15,680 --> 00:20:16,740
to classify what hands you

858
00:20:16,740 --> 00:20:18,820
have well there's a range of hands based

859
00:20:18,820 --> 00:20:20,820
on what you're you're doing and the more

860
00:20:20,820 --> 00:20:21,680
information i get

861
00:20:21,680 --> 00:20:24,240
the more i resolve to oh i know

862
00:20:24,240 --> 00:20:25,560
exactly what hand you have based on what

863
00:20:25,560 --> 00:20:27,580
you're showing you know or

864
00:20:27,580 --> 00:20:28,760
you're buffing but that's a different

865
00:20:28,760 --> 00:20:29,940
thing but

866
00:20:29,940 --> 00:20:31,200
you know what i mean like i i

867
00:20:31,200 --> 00:20:32,240
feel like that is

868
00:20:32,240 --> 00:20:34,087
the ultimate sort of angle of

869
00:20:34,087 --> 00:20:35,360
representation which

870
00:20:35,360 --> 00:20:37,760
is a world but i don't know if

871
00:20:37,760 --> 00:20:38,800
that is too vague

872
00:20:38,800 --> 00:20:40,120
compared to the more concrete types of

873
00:20:40,120 --> 00:20:41,440
world

874
00:20:41,440 --> 00:20:43,720
models that let's say the video gen people

875
00:20:43,720 --> 00:20:44,040
are doing

876
00:20:44,800 --> 00:20:46,900
and then i guess one other thing like

877
00:20:46,900 --> 00:20:49,060
i i'm also exploring i you mentioned like

878
00:20:49,060 --> 00:20:50,440
the deep models

879
00:20:50,440 --> 00:20:52,000
being slower or more expensive yeah that

880
00:20:52,000 --> 00:20:53,560
is

881
00:20:53,560 --> 00:20:55,140
a trend in the inference world of making

882
00:20:55,140 --> 00:20:55,780
models shallower

883
00:20:55,780 --> 00:20:58,880
right and i wonder if this like short

884
00:20:58,880 --> 00:20:59,840
catchphrase i was thinking about like deep

885
00:20:59,840 --> 00:21:00,800
teacher

886
00:21:02,620 --> 00:21:04,780
shallow student would be a good deployment

887
00:21:04,780 --> 00:21:06,940
paradigm

888
00:21:06,940 --> 00:21:08,955
yeah like you push the frontier

889
00:21:08,955 --> 00:21:10,060
capabilities with

890
00:21:10,060 --> 00:21:10,180
the

891
00:21:10,180 --> 00:21:12,280
with death and then you distill it back

892
00:21:12,280 --> 00:21:14,320
yeah actually this is a good point like

893
00:21:14,320 --> 00:21:14,880
if you go out to our

894
00:21:14,880 --> 00:21:17,040
website like this is one of the future

895
00:21:17,040 --> 00:21:18,420
directions that we list at the very bottom

896
00:21:18,420 --> 00:21:21,160
oh okay yeah uh we

897
00:21:21,160 --> 00:21:23,000
we we would love to see if we

898
00:21:23,000 --> 00:21:24,120
could get similar performance like we push

899
00:21:24,120 --> 00:21:25,240
the

900
00:21:25,240 --> 00:21:26,180
you know like we do

901
00:21:26,180 --> 00:21:28,380
achieve state-of-the-art performance on uh

902
00:21:28,380 --> 00:21:30,300
gold condition rl in jackson crl by a

903
00:21:30,300 --> 00:21:30,980
significant amount

904
00:21:30,980 --> 00:21:32,520
and so it's very exciting to see the

905
00:21:32,520 --> 00:21:34,880
like the the sort of frontier of the

906
00:21:34,880 --> 00:21:36,700
ability to train rl agents uh

907
00:21:36,700 --> 00:21:39,220
sort of pushed um and if we can

908
00:21:39,220 --> 00:21:40,980
do that in a way that also sort

909
00:21:40,980 --> 00:21:42,560
of is just as efficient as a standard

910
00:21:42,560 --> 00:21:44,740
uh you know networks that would be very

911
00:21:44,740 --> 00:21:45,910
cool so you know like yeah because

912
00:21:45,910 --> 00:21:47,080
training

913
00:21:47,080 --> 00:21:48,220
uh doesn't have

914
00:21:48,220 --> 00:21:49,900
to be the same thing that you deploy

915
00:21:49,900 --> 00:21:51,460
at inference right you know what i mean

916
00:21:51,460 --> 00:21:53,400
like yeah so yeah so

917
00:21:53,400 --> 00:21:54,720
if there's ways to like distill down to

918
00:21:54,720 --> 00:21:56,140
a smaller model or prune the model and

919
00:21:56,140 --> 00:21:57,280
maybe not and still

920
00:21:57,280 --> 00:21:58,550
retain performance that's a very

921
00:21:58,550 --> 00:21:59,740
interesting research structure

922
00:21:59,740 --> 00:22:00,740
that we choose but let's talk about

923
00:22:00,740 --> 00:22:02,000
other uh future directions what what else

924
00:22:02,000 --> 00:22:03,260
is

925
00:22:03,260 --> 00:22:06,273
your personal passions yeah so uh

926
00:22:06,273 --> 00:22:07,460
currently i'm

927
00:22:07,460 --> 00:22:07,860
pursuing

928
00:22:07,860 --> 00:22:09,768
the direction of uh stitching in

929
00:22:09,768 --> 00:22:11,080
reinforcement learning

930
00:22:11,080 --> 00:22:13,480
so we are trying to generalize

931
00:22:14,400 --> 00:22:17,046
reinforcement learning from shorter sub

932
00:22:17,046 --> 00:22:18,200
behaviors so that

933
00:22:18,200 --> 00:22:19,720
they are stitched merged during the test

934
00:22:19,720 --> 00:22:21,240
time

935
00:22:21,240 --> 00:22:23,860
and uh yeah i think this is one

936
00:22:23,860 --> 00:22:26,100
of my uh last papers that i will

937
00:22:26,100 --> 00:22:28,360
tackle during the phd personally i

938
00:22:28,360 --> 00:22:30,760
would i'm very curious of like can we

939
00:22:30,760 --> 00:22:33,500
like what's the like real like can we

940
00:22:33,500 --> 00:22:35,240
push i'm i'm curious about

941
00:22:35,240 --> 00:22:36,360
like advancing the frontier as much as

942
00:22:36,360 --> 00:22:37,480
possible

943
00:22:37,480 --> 00:22:38,860
um so if you actually look at our

944
00:22:38,860 --> 00:22:39,940
paper we focus on

945
00:22:39,940 --> 00:22:42,140
scaling depth but we notice that we see

946
00:22:42,140 --> 00:22:43,120
that scaling width actually also improves

947
00:22:43,120 --> 00:22:44,100
performance

948
00:22:44,100 --> 00:22:46,360
and we also find that actually by scaling

949
00:22:46,360 --> 00:22:47,490
depth we actually unlock the ability to

950
00:22:47,490 --> 00:22:48,620
scale

951
00:22:48,620 --> 00:22:49,100
along batch

952
00:22:49,100 --> 00:22:51,280
size as well um so this is uh

953
00:22:51,280 --> 00:22:53,420
one of yeah so okay it's like a

954
00:22:53,420 --> 00:22:55,760
collinear like yeah right so like

955
00:22:55,760 --> 00:22:56,600
okay i guess for context like in

956
00:22:56,600 --> 00:22:57,440
traditional

957
00:22:57,440 --> 00:22:59,700
rl like value-based rl scaling batch size

958
00:22:59,700 --> 00:23:00,180
is not super

959
00:23:00,180 --> 00:23:01,510
effective but there's we also can see

960
00:23:01,510 --> 00:23:02,840
there's

961
00:23:02,840 --> 00:23:04,100
also other work in other areas of deep

962
00:23:04,100 --> 00:23:04,520
learning that

963
00:23:04,520 --> 00:23:06,820
show that scaling batch size is only most

964
00:23:06,820 --> 00:23:08,620
effective when there's like a large enough

965
00:23:08,620 --> 00:23:09,710
network capacity to take advantage of the

966
00:23:09,710 --> 00:23:10,800
scaled

967
00:23:10,800 --> 00:23:13,380
batch size and we actually find that you

968
00:23:13,380 --> 00:23:13,480
know

969
00:23:13,480 --> 00:23:15,420
perhaps so one hypothesis and i'd be like

970
00:23:15,420 --> 00:23:16,560
perhaps the reason why scale batches isn't

971
00:23:16,560 --> 00:23:17,700
that

972
00:23:17,700 --> 00:23:18,220
effective

973
00:23:18,220 --> 00:23:19,180
in traditional rls because like we've been

974
00:23:19,180 --> 00:23:20,140
using

975
00:23:20,140 --> 00:23:20,940
these tiny networks that haven't been able

976
00:23:20,940 --> 00:23:21,540
to

977
00:23:21,540 --> 00:23:23,620
capture that and one of our experiments is

978
00:23:23,620 --> 00:23:25,640
that like because we are enabled

979
00:23:25,640 --> 00:23:26,840
successful training

980
00:23:26,840 --> 00:23:26,960
of

981
00:23:26,960 --> 00:23:29,140
deep network we actually were able to this

982
00:23:29,140 --> 00:23:30,760
is a great test bed for you know

983
00:23:30,760 --> 00:23:31,620
like testing this

984
00:23:31,620 --> 00:23:33,800
hypothesis and we find that indeed as we

985
00:23:33,800 --> 00:23:35,030
scale to network capacity we also unlock

986
00:23:35,030 --> 00:23:36,260
this

987
00:23:36,260 --> 00:23:36,480
different

988
00:23:36,480 --> 00:23:38,960
dimension of scaling batch size and so all

989
00:23:38,960 --> 00:23:40,740
that to say is that i'm very curious

990
00:23:40,740 --> 00:23:42,700
for someone like

991
00:23:42,700 --> 00:23:45,180
with enough compute to like take some of

992
00:23:45,180 --> 00:23:46,330
these environments scale up batch uh scale

993
00:23:46,330 --> 00:23:47,480
up

994
00:23:47,480 --> 00:23:48,040
depth to

995
00:23:48,040 --> 00:23:49,220
the maximum capability also scale along

996
00:23:49,220 --> 00:23:50,400
with also

997
00:23:50,400 --> 00:23:51,680
scale along batch size and let's like

998
00:23:51,680 --> 00:23:52,960
basically

999
00:23:52,960 --> 00:23:53,300
like

1000
00:23:53,300 --> 00:23:55,080
in the same way that in language we're

1001
00:23:55,080 --> 00:23:57,420
scaling on so many different axes can we

1002
00:23:57,420 --> 00:23:57,900
unlock different

1003
00:23:57,900 --> 00:23:58,960
dimensions of scaling as well and what

1004
00:23:58,960 --> 00:24:00,020
capabilities

1005
00:24:00,020 --> 00:24:01,620
and how far can we push the frontier

1006
00:24:01,620 --> 00:24:02,440
of training

1007
00:24:02,440 --> 00:24:04,320
these rl agents from doing them before we

1008
00:24:04,320 --> 00:24:05,880
pass it sean uh when you say enough

1009
00:24:05,880 --> 00:24:07,060
compute what kind

1010
00:24:07,060 --> 00:24:09,140
of compute budget did you have how does

1011
00:24:09,140 --> 00:24:11,020
it how i just want to see what

1012
00:24:11,020 --> 00:24:11,960
you guys got good question

1013
00:24:11,960 --> 00:24:13,320
so we we wanted to make sure that

1014
00:24:13,320 --> 00:24:15,380
this is we we wanted to make it

1015
00:24:15,380 --> 00:24:16,720
such that like uh you know it's

1016
00:24:16,720 --> 00:24:19,120
quite accessible so i can the nice thing

1017
00:24:19,120 --> 00:24:20,800
is that all of our experiments even a

1018
00:24:20,800 --> 00:24:21,200
thousand layer

1019
00:24:21,200 --> 00:24:23,340
networks can be run on one single 80

1020
00:24:23,340 --> 00:24:25,250
gigabyte h100 gpu um so that's those

1021
00:24:25,250 --> 00:24:27,160
dollars

1022
00:24:27,160 --> 00:24:28,340
yeah right right

1023
00:24:28,340 --> 00:24:29,960
right so everything can be run on one

1024
00:24:29,960 --> 00:24:32,100
gpu um but in theory if we had

1025
00:24:32,100 --> 00:24:33,080
you know like a distributed

1026
00:24:33,080 --> 00:24:34,290
training setup and like can just like

1027
00:24:34,290 --> 00:24:35,500
blast

1028
00:24:35,500 --> 00:24:36,380
compute through this and really wanted to

1029
00:24:36,380 --> 00:24:37,260
push

1030
00:24:37,260 --> 00:24:37,680
the frontier

1031
00:24:37,680 --> 00:24:39,600
it'd be very interesting to see how things

1032
00:24:39,600 --> 00:24:42,160
go yeah cool and i've actively been trying

1033
00:24:42,160 --> 00:24:42,480
to learn as

1034
00:24:42,480 --> 00:24:43,880
much as i can about vision language action

1035
00:24:43,880 --> 00:24:45,470
models uh role models at europe's and

1036
00:24:45,470 --> 00:24:47,060
going

1037
00:24:47,060 --> 00:24:47,440
to a lot of

1038
00:24:47,440 --> 00:24:49,114
machine language action models vision

1039
00:24:49,114 --> 00:24:50,200
language vision language

1040
00:24:50,200 --> 00:24:53,200
yeah um and yeah curious about

1041
00:24:53,200 --> 00:24:55,414
applications of representation for these

1042
00:24:55,414 --> 00:24:56,300
yeah exactly for

1043
00:24:56,300 --> 00:24:58,040
robotics um actively trying to explore

1044
00:24:58,040 --> 00:25:00,300
more in that area so just reading a

1045
00:25:00,300 --> 00:25:01,100
lot of literature talking to as many

1046
00:25:01,100 --> 00:25:01,820
people

1047
00:25:01,820 --> 00:25:04,100
yeah we just released our

1048
00:25:04,100 --> 00:25:05,790
episode with uh general intuition oh okay

1049
00:25:05,790 --> 00:25:07,480
um

1050
00:25:07,480 --> 00:25:09,480
awesome where if you know a bit about

1051
00:25:09,480 --> 00:25:10,180
their history they

1052
00:25:10,180 --> 00:25:11,670
started as a gaming clipping company and

1053
00:25:11,670 --> 00:25:13,160
uh

1054
00:25:13,160 --> 00:25:14,110
they basically have a vision language

1055
00:25:14,110 --> 00:25:15,060
action model

1056
00:25:15,060 --> 00:25:16,120
yeah which

1057
00:25:16,120 --> 00:25:17,680
um i i saw i saw i saw

1058
00:25:17,680 --> 00:25:20,120
a preview it was very impressive i'm not

1059
00:25:20,120 --> 00:25:22,480
sure exactly how transferable it is to

1060
00:25:22,480 --> 00:25:25,340
embodied use cases but it doesn't have to

1061
00:25:25,340 --> 00:25:29,340
like screen is fine you know like yeah

1062
00:25:29,340 --> 00:25:30,300
i i don't know if you

1063
00:25:30,300 --> 00:25:32,140
have any takes on yeah that's an exciting

1064
00:25:32,140 --> 00:25:33,640
research direction definitely yeah i i

1065
00:25:33,640 --> 00:25:35,140
think um

1066
00:25:35,140 --> 00:25:37,580
the the the the

1067
00:25:37,580 --> 00:25:38,660
the concepts of actions as as something

1068
00:25:38,660 --> 00:25:39,740
that

1069
00:25:39,740 --> 00:25:42,340
you are outputting is actually not that

1070
00:25:42,340 --> 00:25:44,940
popular

1071
00:25:44,940 --> 00:25:45,640
in industry

1072
00:25:45,640 --> 00:25:48,093
right right only because text has

1073
00:25:48,093 --> 00:25:49,580
completely dominated

1074
00:25:49,580 --> 00:25:51,880
the last three years and tool calling

1075
00:25:51,880 --> 00:25:54,320
and which is a just another form of

1076
00:25:54,320 --> 00:25:56,860
structured text uh and i i feel like

1077
00:25:56,860 --> 00:25:59,100
the uh action research is is

1078
00:25:59,100 --> 00:26:00,920
kind of like i don't know how i

1079
00:26:00,920 --> 00:26:02,280
don't know what needs to happen in order

1080
00:26:02,280 --> 00:26:05,000
to unlock the next phase in

1081
00:26:05,000 --> 00:26:06,420
that i don't know if you i think

1082
00:26:06,420 --> 00:26:07,640
anything interesting out here shut it out

1083
00:26:07,640 --> 00:26:08,860
yeah

1084
00:26:08,860 --> 00:26:09,940
there's a lot of cool work

1085
00:26:09,940 --> 00:26:11,750
on like leveraging pre-trained vlms and

1086
00:26:11,750 --> 00:26:13,560
you

1087
00:26:13,560 --> 00:26:14,920
freeze it and then you apply it oh

1088
00:26:14,920 --> 00:26:15,720
yeah and then you're

1089
00:26:15,720 --> 00:26:17,260
recording on top of that like sort of

1090
00:26:17,260 --> 00:26:19,110
experts to output actions um also like

1091
00:26:19,110 --> 00:26:20,960
systems

1092
00:26:20,960 --> 00:26:22,400
for doing like

1093
00:26:22,400 --> 00:26:24,224
hierarchical planning maybe outputting

1094
00:26:24,224 --> 00:26:25,280
some higher level plan

1095
00:26:25,280 --> 00:26:26,860
that and this is like a larger network

1096
00:26:26,860 --> 00:26:29,260
that takes a long time to a little

1097
00:26:29,260 --> 00:26:30,840
longer to do inference and so it outputs

1098
00:26:30,840 --> 00:26:31,980
its plans with less

1099
00:26:31,980 --> 00:26:34,080
frequency like some sort of chunk and then

1100
00:26:34,080 --> 00:26:36,080
from there there's like some sort of uh

1101
00:26:36,080 --> 00:26:37,060
second system that

1102
00:26:37,060 --> 00:26:38,360
operates a bit more fast i think there's

1103
00:26:38,360 --> 00:26:39,220
quite a bit of interesting research in

1104
00:26:39,220 --> 00:26:40,080
that

1105
00:26:40,080 --> 00:26:40,660
direction so

1106
00:26:40,660 --> 00:26:42,260
that's what i'm looking forward to cool

1107
00:26:42,260 --> 00:26:43,860
final

1108
00:26:43,860 --> 00:26:45,210
question uh hardest question you were

1109
00:26:45,210 --> 00:26:46,560
asked at

1110
00:26:46,560 --> 00:26:47,770
the poster session or just favorite

1111
00:26:47,770 --> 00:26:48,980
encounter anyone

1112
00:26:48,980 --> 00:26:51,540
famous that you met so i actually haven't

1113
00:26:51,540 --> 00:26:51,900
gotten a

1114
00:26:51,900 --> 00:26:53,320
chance to go to the conference that much

1115
00:26:53,320 --> 00:26:55,100
i'm actually working full-time now so oh

1116
00:26:55,100 --> 00:26:57,580
damn yeah uh so

1117
00:26:57,580 --> 00:26:59,620
so far i i actually literally just got

1118
00:26:59,620 --> 00:27:02,040
my badge like a few moments before session

1119
00:27:02,040 --> 00:27:03,280
so i guess i wouldn't

1120
00:27:03,280 --> 00:27:05,100
be the best to answer that question no

1121
00:27:05,100 --> 00:27:06,840
no like you see you like people ask

1122
00:27:06,840 --> 00:27:08,000
you stuff right oh that

1123
00:27:08,000 --> 00:27:10,360
might i might close people asking you or

1124
00:27:10,360 --> 00:27:13,020
meeting you and like you know just just

1125
00:27:13,020 --> 00:27:13,760
give a vibe of

1126
00:27:13,760 --> 00:27:14,990
like what people are saying and yeah

1127
00:27:14,990 --> 00:27:16,220
people

1128
00:27:16,220 --> 00:27:18,920
were very i think it's sort of like

1129
00:27:18,920 --> 00:27:20,180
a very eye-opening

1130
00:27:20,180 --> 00:27:21,740
i think that the general question is that

1131
00:27:21,740 --> 00:27:23,660
people thought is a very eye-opening paper

1132
00:27:23,660 --> 00:27:24,220
because like

1133
00:27:24,220 --> 00:27:25,520
the objective is quite simple it's quite

1134
00:27:25,520 --> 00:27:26,820
elegant

1135
00:27:26,820 --> 00:27:29,180
and for us to be able to like

1136
00:27:29,180 --> 00:27:30,880
you know like i don't

1137
00:27:30,880 --> 00:27:32,540
want to say overturn but like sort of

1138
00:27:32,540 --> 00:27:33,850
challenge the conventional wisdom that

1139
00:27:33,850 --> 00:27:35,160
like rl is

1140
00:27:35,160 --> 00:27:35,540
not super

1141
00:27:35,540 --> 00:27:38,280
scalable and push it to such limits like

1142
00:27:38,280 --> 00:27:39,710
a thousand layers d and see continuing

1143
00:27:39,710 --> 00:27:41,140
improve

1144
00:27:41,140 --> 00:27:41,880
performance i

1145
00:27:41,880 --> 00:27:43,080
think the general impression that i've

1146
00:27:43,080 --> 00:27:44,280
gotten is

1147
00:27:44,280 --> 00:27:47,320
that you know this this could be like

1148
00:27:47,320 --> 00:27:47,600
a really

1149
00:27:47,600 --> 00:27:49,780
cool like if we can sort of build

1150
00:27:49,780 --> 00:27:52,180
along this direction and that like we can

1151
00:27:52,180 --> 00:27:52,800
really scale along

1152
00:27:52,800 --> 00:27:53,660
all these different dimensions and push

1153
00:27:53,660 --> 00:27:54,520
the frontier

1154
00:27:54,520 --> 00:27:56,560
of the ability for rl i'm very curious

1155
00:27:56,560 --> 00:27:57,060
to see how that

1156
00:27:57,060 --> 00:27:58,600
goes all right well thank you so much

1157
00:27:58,600 --> 00:28:00,620
for dropping by uh congrats on the paper

1158
00:28:00,620 --> 00:28:00,840
again

1159
00:28:01,380 --> 00:28:03,060
and uh good luck in your future work

1160
00:28:03,060 --> 00:28:04,360
thank you thanks for having us yeah
