1
00:00:04,880 --> 00:00:08,160
그럼 더 지체하지 않고 시작하겠습니다. 오늘은,

2
00:00:08,220 --> 00:00:10,560
제가 로봇에 관한 연구를 말씀드리겠습니다.

3
00:00:10,560 --> 00:00:13,720
물리 기반 PDE 사전지식을 활용한 모션 러닝에 대해 말씀드리겠습니다.

4
00:00:15,600 --> 00:00:18,360
그래서 먼저, 그러니까, 무엇이

5
00:00:18,360 --> 00:00:21,400
모션 플래닝의 문제인지 소개드리겠습니다. 그리고 모션 플래닝은

6
00:00:21,400 --> 00:00:23,760
에이전트가 조정하는 방법입니다.

7
00:00:23,760 --> 00:00:26,320
주어진 시작점에서

8
00:00:26,320 --> 00:00:28,820
주어진 목표까지 모든 원하는 제약을 만족시키면서 행동을 조정하는 것입니다.

9
00:00:28,820 --> 00:00:32,740
그래서 동적 시스템이나 로봇이

10
00:00:32,740 --> 00:00:35,440
움직일 필요가 있을 때마다, 이 문제를 풀어야 합니다.

11
00:00:35,440 --> 00:00:37,700
즉, 이 모션 플래닝 문제를요, 알겠죠?

12
00:00:38,080 --> 00:00:40,340
그리고 이것에는 응용이 있는데, 또는 이것들이

13
00:00:40,340 --> 00:00:42,840
제 연구실이 집중하는 응용들인데요, 예를 들면

14
00:00:42,840 --> 00:00:45,180
전신 모션 플래닝부터 시작합니다.

15
00:00:45,400 --> 00:00:48,500
여기에는 이동 로봇이 있는데, 이동 로봇, 일반적인

16
00:00:48,500 --> 00:00:51,000
로봇입니다. 어떤 물체를

17
00:00:51,000 --> 00:00:52,560
주어진 시작점에서 주어진 목표로 옮기고 있습니다.

18
00:00:52,560 --> 00:00:55,840
그다음, 우리는 이 문제를 동적인

19
00:00:55,840 --> 00:00:59,600
환경에서도 푸는데, 로봇 앞에 장애물을 동적으로

20
00:00:59,600 --> 00:01:00,560
배치합니다.

21
00:01:01,000 --> 00:01:04,240
또 다른 분야로, 우리가 매우 기대하고 관심을 갖고

22
00:01:04,240 --> 00:01:06,380
있는 것은 반응형 조작입니다.

23
00:01:06,620 --> 00:01:09,260
로봇이 제약 하에서 조작 플래닝 문제를 풀 때,

24
00:01:09,260 --> 00:01:12,000
어떤 교란을 경험할 수 있습니다.

25
00:01:12,000 --> 00:01:15,920
로봇이 그 교란에 어떻게 동적으로 적응하고

26
00:01:15,920 --> 00:01:17,760
할당된 과제를 계속 수행할 수 있을까요?

27
00:01:19,840 --> 00:01:22,540
마지막으로, 우리는 또한 탐구하고 있습니다.

28
00:01:22,560 --> 00:01:26,340
이 모션 플래닝 도구들을 반응형 다중

29
00:01:26,340 --> 00:01:28,300
에이전트 플래닝에 적용하는 것으로, 많은 수의

30
00:01:28,300 --> 00:01:28,700
에이전트가 있고,

31
00:01:28,720 --> 00:01:31,940
그들이 할당된 과제를 수행하다가, 그리고 나서

32
00:01:31,940 --> 00:01:34,840
서로 충돌하는 것을 반응적으로 피해야 합니다.

33
00:01:34,840 --> 00:01:35,280
서로요.

34
00:01:35,980 --> 00:01:40,620
그래서 이런 모든 문제들 전반에 걸쳐, 핵심

35
00:01:40,620 --> 00:01:43,460
쟁점은 이 모션 플래닝

36
00:01:43,460 --> 00:01:43,780
문제들을 어떻게 푸느냐입니다.

37
00:01:43,880 --> 00:01:45,840
어떻게 도구를 만들어낼 것이냐,

38
00:01:45,840 --> 00:01:51,040
효율적인 도구를요. 그리고 제 연구실의 초점은

39
00:01:51,040 --> 00:01:53,820
이런 도구들을 실제로 사용할 수 있게 개발하는 것입니다.

40
00:01:53,820 --> 00:01:54,120
실시간으로

41
00:01:54,120 --> 00:01:57,420
로봇 움직임의 실시간 조정을 가능하게 하는

42
00:01:57,420 --> 00:02:01,200
비정형적이고 제약된 환경에서 최소한의 사전 훈련

43
00:02:01,200 --> 00:02:02,000
또는 시행착오로 작동하는 것입니다.

44
00:02:02,460 --> 00:02:05,680
저는 이 목표를 해결하거나 달성하기 위해

45
00:02:05,680 --> 00:02:07,280
꽤 오랜 시간 동안 노력해 왔습니다.

46
00:02:07,520 --> 00:02:10,220
학부 시절, 저는 샘플링 기반

47
00:02:10,220 --> 00:02:13,620
모션 계획 방법으로 시작했고, 제 초점은

48
00:02:13,620 --> 00:02:16,140
적응형 샘플링 기법을 개발하는 것이었습니다.

49
00:02:16,140 --> 00:02:18,960
이 샘플링 기반 방법들이

50
00:02:18,960 --> 00:02:20,700
트리를 가능한 빨리 구성할 수 있도록 하기 위해서입니다.

51
00:02:20,700 --> 00:02:22,800
그러면 그 트리의 한 가지가 여러분의

52
00:02:22,800 --> 00:02:24,560
시작점과 목표점 사이의 경로 해가 됩니다.

53
00:02:25,320 --> 00:02:29,420
그리고 박사 과정 동안 저는

54
00:02:29,420 --> 00:02:33,220
이 방법들을 데이터 기반 접근법으로 확장하는 방향으로 나아갔습니다.

55
00:02:33,460 --> 00:02:36,140
그러니까, 고전적인 샘플링 기반 방법의 문제는

56
00:02:36,140 --> 00:02:39,160
그 기법들이 계산적으로 매우 느렸다는 점이었습니다.

57
00:02:39,320 --> 00:02:41,680
로봇의 차원이 증가하면, 예를 들어

58
00:02:41,680 --> 00:02:44,560
사람의, 음, 동작을 계획한다든지

59
00:02:44,560 --> 00:02:45,000
인간형 로봇의 경우,

60
00:02:45,160 --> 00:02:46,880
자유도가 엄청나게 큽니다.

61
00:02:46,880 --> 00:02:49,720
그런 종류의 시스템에서는 계산 시간이

62
00:02:49,720 --> 00:02:50,940
매우 느려졌습니다.

63
00:02:51,040 --> 00:02:53,460
그래서 실시간으로 할 수 있는

64
00:02:53,460 --> 00:02:56,600
응용을 많이 할 수가 없었습니다. 느린 계산 속도 때문에

65
00:02:56,600 --> 00:02:57,060
그런 방법들 말입니다.

66
00:02:57,180 --> 00:03:00,560
그다음 저는 데이터 기반 접근으로 옮겨갔습니다.

67
00:03:02,020 --> 00:03:05,120
2018년이나 2019년쯤에는, 그 이전에는, 많은

68
00:03:05,120 --> 00:03:07,340
사람들이 모션 계획 문제를 풀려고 했는데

69
00:03:07,340 --> 00:03:08,940
이런, 음, 신경망으로 말입니다,

70
00:03:08,980 --> 00:03:12,000
하지만 한 가지 핵심 요소가 빠져 있었습니다.

71
00:03:12,000 --> 00:03:14,300
그리고 이, 이 논문들에서 저는

72
00:03:14,300 --> 00:03:18,080
신경망만 갖고서는 충분하지 않다는

73
00:03:18,080 --> 00:03:18,540
처방을 제시했습니다.

74
00:03:18,540 --> 00:03:21,240
예를 들어 모방 학습을 해도,

75
00:03:21,240 --> 00:03:21,940
여전히 학습이 되지 않습니다.

76
00:03:22,100 --> 00:03:24,720
이런 모델에는 확률성이 필요합니다.

77
00:03:24,780 --> 00:03:27,600
그리고 확률성은 여러 방식으로 포함할 수 있습니다.

78
00:03:27,900 --> 00:03:29,920
저희 연구에서는 드롭아웃을 사용했습니다.

79
00:03:30,360 --> 00:03:33,220
그다음 사람들은 그 아이디어를 받아들여, 옮겨가서

80
00:03:33,220 --> 00:03:35,000
변분 오토인코더를 사용하는 쪽으로 갔습니다.

81
00:03:35,060 --> 00:03:36,800
이제는 확산 모델을 사용합니다.

82
00:03:36,980 --> 00:03:38,960
그것들은, 그것들은 어떤 형태로든 확률성이 있습니다.

83
00:03:38,960 --> 00:03:42,120
어떤 노이즈에서 시작해서 그다음 디노이즈를

84
00:03:42,120 --> 00:03:42,760
시간에 따라 수행합니다.

85
00:03:42,920 --> 00:03:45,820
그래서 어떤 형태의 무작위성이 포함되어

86
00:03:45,820 --> 00:03:48,940
과정에 포함되어 이 모델들이 계획하게

87
00:03:48,940 --> 00:03:49,980
동작을 계획하도록 도와줍니다.

88
00:03:50,340 --> 00:03:53,100
그래서 핵심 처방은, 신경망이 필요하다는 것입니다.

89
00:03:53,100 --> 00:03:53,440
그리고요.

90
00:03:53,620 --> 00:03:55,820
또 어떤 형태의, 음,

91
00:03:56,000 --> 00:03:58,780
과정에 확률성이 필요합니다. 그래서, 음,

92
00:03:59,260 --> 00:04:01,860
신경망은, 해를 근사하고 있기 때문에,

93
00:04:01,860 --> 00:04:03,980
국소 최소값에 빠질 수 있습니다.

94
00:04:03,980 --> 00:04:06,820
노이즈 제거 과정이 있거나 여러 개의

95
00:04:06,820 --> 00:04:08,700
음, 시드로 시작하게 됩니다.

96
00:04:09,000 --> 00:04:11,580
기본적으로 이 신경망들이

97
00:04:11,580 --> 00:04:12,780
국소 최솟값에서 벗어나도록 돕습니다.

98
00:04:12,980 --> 00:04:13,520
알겠습니까?

99
00:04:13,920 --> 00:04:16,720
하지만 이 모델들에 대한 제 주요 문제는

100
00:04:16,900 --> 00:04:20,120
훈련 비용이었습니다.

101
00:04:20,400 --> 00:04:23,260
오프라인에서, 우리는 이런

102
00:04:23,260 --> 00:04:25,480
고전적인 방법들을 실행해서 모든 데이터를 수집해야 했습니다.

103
00:04:25,480 --> 00:04:28,560
그리고 그 데이터로 이 모델들을

104
00:04:28,560 --> 00:04:29,460
훈련시켰습니다.

105
00:04:29,700 --> 00:04:32,080
그리고 네, 추론 시에는 빠른

106
00:04:32,080 --> 00:04:32,440
추론을 제공했습니다.

107
00:04:33,160 --> 00:04:35,660
하지만 생각해보면, 이것들이 정말로

108
00:04:35,660 --> 00:04:39,180
이 방법들의, 음, 느린 계산 속도를

109
00:04:39,560 --> 00:04:42,480
극복하고 있었을까요?

110
00:04:42,580 --> 00:04:44,520
훈련 시간을 더하면

111
00:04:44,520 --> 00:04:47,160
또는 데이터를 얻는 데 소요된 시간을

112
00:04:47,160 --> 00:04:48,980
추론 시간과 합치면

113
00:04:48,980 --> 00:04:51,120
실제로 이득이 되지 않는다고 생각할 수 있습니다.

114
00:04:51,120 --> 00:04:53,680
그래서 제가 퍼듀 대학교에서 교수로

115
00:04:53,680 --> 00:04:56,360
시작했을 때, 저는 전문가 시연 없이

116
00:04:56,360 --> 00:04:59,920
이 신경망들을 훈련시키는 다른 방법을 찾기 시작했습니다.

117
00:05:00,820 --> 00:05:03,460
그리고 제 연구실이 집중하는

118
00:05:03,460 --> 00:05:04,060
세 가지 특징이 있습니다.

119
00:05:04,240 --> 00:05:05,600
예를 들어, 하나는 추론 효율성입니다.

120
00:05:05,880 --> 00:05:08,240
저는 즉시 실행이 가능하도록 하는 방법을 개발하고 싶습니다.

121
00:05:08,240 --> 00:05:10,760
즉, 계획을 가능한 한 빠르게 추론하도록 하는 것입니다.

122
00:05:11,620 --> 00:05:13,900
그다음 우리가 찾는 두 번째 특성은

123
00:05:13,900 --> 00:05:14,680
학습 효율성입니다.

124
00:05:14,680 --> 00:05:17,700
모델 학습 비용을 줄이고 싶습니다.

125
00:05:17,840 --> 00:05:20,080
전문가, 즉 전문가의 필요성을 없애고 싶습니다.

126
00:05:20,080 --> 00:05:20,820
주석 작업을 말입니다.

127
00:05:21,440 --> 00:05:23,560
그리고 이는 셀룰러 전이 가능성입니다.

128
00:05:23,820 --> 00:05:26,940
예를 들어, 지금은 누군가와 이야기할 때마다,

129
00:05:26,940 --> 00:05:28,700
모방 학습을 하거나

130
00:05:28,700 --> 00:05:30,560
아주 큰 모델을 학습시키는 사람에게는 두 번째 질문이

131
00:05:30,560 --> 00:05:30,980
늘 나오는데요,

132
00:05:31,280 --> 00:05:33,820
다른 환경이나

133
00:05:33,820 --> 00:05:34,660
처음 보는 환경으로 전이할 수 있느냐는 것입니다.

134
00:05:34,720 --> 00:05:36,840
왜냐하면 문제는, 다른 환경으로 옮기면

135
00:05:36,840 --> 00:05:37,800
환경이 달라질 때

136
00:05:37,800 --> 00:05:40,120
모든 데이터를 다시

137
00:05:40,120 --> 00:05:40,380
수집해야 할 수도 있기 때문입니다.

138
00:05:40,380 --> 00:05:43,240
하지만 학습 비용이 매우 낮은

139
00:05:43,240 --> 00:05:44,440
모델이 있고,

140
00:05:44,620 --> 00:05:47,340
전문가 시연의 필요를 없애 준다면,

141
00:05:47,740 --> 00:05:50,060
그 모델을 매우 쉽게 전이해서

142
00:05:50,060 --> 00:05:51,720
새로운 도메인으로 가져갈 수 있습니다.

143
00:05:52,740 --> 00:05:54,540
세 번째로, 매우 중요한 특성이 있습니다.

144
00:05:54,740 --> 00:05:56,400
예를 들어, 우리는 이런 모델을

145
00:05:56,400 --> 00:05:59,160
높은 자유도에 적응하도록 설계하고 싶고,

146
00:05:59,340 --> 00:06:03,020
매우 복잡한 구조, 미지의 환경, 또는, 음, 매우

147
00:06:03,020 --> 00:06:03,860
복잡한 제약조건에도요.

148
00:06:03,900 --> 00:06:06,580
예컨대, 음, 조작 문제에 수반되는 제약조건 같은 것들입니다.

149
00:06:06,580 --> 00:06:08,940
또한 아주 많은 수의 에이전트로도 확장되어야 해서

150
00:06:08,940 --> 00:06:10,800
여러 대의 로봇을 배치해

151
00:06:10,800 --> 00:06:11,860
운용할 수 있어야 합니다.

152
00:06:11,860 --> 00:06:13,660
음, 주어진 환경에서 말입니다.

153
00:06:13,800 --> 00:06:15,740
그렇게 하면 어떤 작업을

154
00:06:15,740 --> 00:06:19,000
협조적으로, 또는 분산 방식으로 해결할 수 있습니다.

155
00:06:20,700 --> 00:06:23,640
그래서 개괄적으로 말씀드리면, 이런 특성들의 관점에서

156
00:06:23,640 --> 00:06:25,520
현재 우리가 어디에 서 있는지 보면요.

157
00:06:25,880 --> 00:06:28,320
우리는 최적화 기반 기법을 가지고 있습니다.

158
00:06:29,300 --> 00:06:33,700
그리고 이 기법들은 높은 추론 효율성을 가지고 있습니다.

159
00:06:33,920 --> 00:06:36,220
우리는 아주 좋은 최적화 기반 도구들을 가지고 있어서

160
00:06:36,220 --> 00:06:39,520
궤적을 매우 효율적으로 찾을 수 있습니다.

161
00:06:39,520 --> 00:06:42,480
매우 높은, 음, 훈련 효율성을 가지고 있습니다.

162
00:06:42,480 --> 00:06:44,800
최적화기를 다시, 사전 훈련할

163
00:06:44,800 --> 00:06:45,500
필요가 없습니다.

164
00:06:45,700 --> 00:06:48,200
하지만 매우 복잡한 시나리오에는

165
00:06:48,200 --> 00:06:48,480
적응하지 못합니다.

166
00:06:48,600 --> 00:06:51,180
국소 최솟값에 빠지기 쉽고

167
00:06:51,180 --> 00:06:52,460
자주 거기에 갇힐 수 있습니다.

168
00:06:52,460 --> 00:06:56,600
그 다음으로, 로봇 계획 및 제어 문제를 해결하기 위한

169
00:06:56,600 --> 00:06:59,040
고전적인 도구들이 있습니다.

170
00:06:59,240 --> 00:07:02,820
이산화 기반 접근법이 있습니다.

171
00:07:03,000 --> 00:07:04,560
A*를 아실 것입니다. 아셔야 합니다.

172
00:07:04,720 --> 00:07:07,280
그리고 샘플링 기반 접근법도 있습니다.

173
00:07:07,580 --> 00:07:10,380
제어 분야에는 다른 고전적 도구들도 있습니다, 예를 들어

174
00:07:10,380 --> 00:07:13,240
랜덤 슈팅 방법과 그런 기법들이 있습니다.

175
00:07:13,240 --> 00:07:18,580
이 방법들은 낮은 훈련 효율성을, 음,

176
00:07:18,780 --> 00:07:20,460
높은 훈련 효율성을 가지고 있습니다.

177
00:07:20,640 --> 00:07:21,940
훈련할 필요가 없습니다.

178
00:07:22,440 --> 00:07:24,800
하지만 추론 효율성은 매우 느립니다.

179
00:07:25,060 --> 00:07:27,960
이 방법들은 전체 공간을 이산화하거나

180
00:07:27,960 --> 00:07:30,080
그래프를 구성해야 하기 때문입니다.

181
00:07:30,220 --> 00:07:32,440
그리고 그것을 통해 경로를 찾아야 합니다.

182
00:07:32,880 --> 00:07:35,740
최근의 발전으로, 제가 인정해야 할 점은 사람들이

183
00:07:35,740 --> 00:07:37,980
이런 방법들에서 대규모

184
00:07:37,980 --> 00:07:41,000
병렬화를 어떻게 활용해 속도를 높일 수 있는지

185
00:07:41,000 --> 00:07:41,460
탐구하고 있다는 것입니다.

186
00:07:41,460 --> 00:07:43,580
하지만 여전히 우리는

187
00:07:43,580 --> 00:07:47,780
이런 방법들이 복잡한, 어, 운동학적

188
00:07:47,780 --> 00:07:48,240
제약을 어떻게 다루는지 지켜봐야 합니다.

189
00:07:49,220 --> 00:07:51,780
그다음에는 모방학습 같은 데이터 기반 접근이나

190
00:07:51,780 --> 00:07:53,900
강화학습이 있습니다.

191
00:07:54,200 --> 00:07:56,600
이들은, 어, 학습 효율이

192
00:07:56,600 --> 00:07:57,840
매우 낮다는 것을 알고 있습니다.

193
00:07:58,160 --> 00:08:01,000
하지만 한 번 학습되면 추론 효율은 매우

194
00:08:01,000 --> 00:08:01,260
높습니다.

195
00:08:01,380 --> 00:08:04,000
그리고 매우 복잡한 시나리오에도 적응합니다.

196
00:08:04,440 --> 00:08:07,360
그런데 우리가 하나의 방법을 가질 수 있을까요, 그러니까 저는

197
00:08:07,360 --> 00:08:09,440
이 세 가지 범주를 모두 언급했습니다.

198
00:08:09,440 --> 00:08:13,260
그 범주들은 그 특징들 중 하나 이상이

199
00:08:13,260 --> 00:08:13,860
빠져 있었습니다.

200
00:08:15,000 --> 00:08:17,800
최적화 기반은 복잡성에 적응하지 못했습니다.

201
00:08:18,040 --> 00:08:20,840
샘플링이나 고전적 방법은, 어,

202
00:08:21,120 --> 00:08:22,860
추론 효율이, 어,

203
00:08:23,060 --> 00:08:24,620
낮았고, 데이터 기반은 매우

204
00:08:24,620 --> 00:08:25,860
낮은 학습 효율을 보였습니다.

205
00:08:25,960 --> 00:08:28,140
이 세 가지 특징을 모두 갖는 방법이

206
00:08:28,140 --> 00:08:28,780
가능할까요?

207
00:08:29,320 --> 00:08:30,760
그래서 저희 답은 예입니다.

208
00:08:30,760 --> 00:08:34,540
즉, 물리에서 얻을 수 있는 어떤, 어, 사전지식을

209
00:08:34,540 --> 00:08:36,880
도입하면 이 세 가지 특징을 모두

210
00:08:36,880 --> 00:08:38,640
달성할 수 있습니다.

211
00:08:38,640 --> 00:08:41,000
그리고 이것이 제 발표의 남은

212
00:08:41,000 --> 00:08:42,820
부분에서의 핵심 주제가 될 것입니다.

213
00:08:43,160 --> 00:08:45,900
그래서 제가 물리 사전지식을 사용한다고 말할 때

214
00:08:45,900 --> 00:08:47,940
자주 나오는 질문이 있습니다.

215
00:08:48,060 --> 00:08:49,720
어떤 물리 사전지식을 쓰느냐는 것입니다.

216
00:08:50,500 --> 00:08:50,980
알겠습니까?

217
00:08:51,340 --> 00:08:53,840
사람들은 종종 이 물리 사전지식을

218
00:08:53,840 --> 00:08:55,620
물리 엔진이나 시뮬레이터와 혼동합니다.

219
00:08:55,620 --> 00:08:58,800
하지만 제 발표에서는 PDE 사전지식을 쓴다고

220
00:08:58,800 --> 00:09:00,600
아주 명확히 말씀드렸습니다.

221
00:09:00,600 --> 00:09:02,840
왜냐하면 이 질문을 계속해서

222
00:09:02,840 --> 00:09:03,040
반복해서 받았기 때문입니다.

223
00:09:03,140 --> 00:09:04,520
그럼 여기서 시뮬레이션은 어디에 있느냐는 질문입니다.

224
00:09:04,660 --> 00:09:06,240
즉, 이것은 물리 시뮬레이션이 아닙니다.

225
00:09:06,240 --> 00:09:09,060
우리가 사전지식으로 사용하는 것은

226
00:09:09,060 --> 00:09:09,260
PDE입니다.

227
00:09:09,600 --> 00:09:12,360
그래서 PDE를 이야기할 때, 여러분이 들어보셨을

228
00:09:12,360 --> 00:09:15,040
PDE 중 하나가, 어,

229
00:09:15,040 --> 00:09:15,740
HJB PDE입니다.

230
00:09:16,020 --> 00:09:18,160
해밀토니언-자코비-벨만 방정식입니다.

231
00:09:18,540 --> 00:09:21,180
그리고 이는 일부 동적

232
00:09:21,180 --> 00:09:21,520
시스템의 운동을 지배합니다.

233
00:09:21,800 --> 00:09:23,700
하지만 그 PDE는 풀기가 매우 어렵습니다.

234
00:09:23,700 --> 00:09:26,940
특이점과 다봉성(multimodal) 문제가 있습니다.

235
00:09:27,000 --> 00:09:29,920
그리고 운동학적 제약과

236
00:09:29,920 --> 00:09:33,100
동역학적 제약을 모두 적용하면, 어,

237
00:09:33,100 --> 00:09:33,280
풀기가 더욱 어려워집니다.

238
00:09:33,560 --> 00:09:35,920
그래서 저희는 그 방정식을 분석하고,

239
00:09:35,940 --> 00:09:38,520
동역학을 단순하게 가정하면,

240
00:09:38,800 --> 00:09:42,340
그 식이 이 에이코널(Eikonal) PDE로 환원된다고 말했습니다.

241
00:09:42,340 --> 00:09:46,100
이 에이코널 PDE에는 두 함수가 있습니다.

242
00:09:46,580 --> 00:09:48,020
하나는 T입니다.

243
00:09:48,340 --> 00:09:49,940
다른 하나는 S입니다.

244
00:09:50,760 --> 00:09:52,740
T는 미지의 함수입니다.

245
00:09:52,940 --> 00:09:54,660
그리고 이는 가치 함수입니다.

246
00:09:54,660 --> 00:09:56,760
이 PDE를 풀어서

247
00:09:56,760 --> 00:09:57,980
이 미지의 함수 T를 찾고자 합니다.

248
00:09:58,700 --> 00:10:00,260
S는 알려진 함수입니다.

249
00:10:00,480 --> 00:10:03,180
그리고 기본적으로 제약을 정의합니다.

250
00:10:03,400 --> 00:10:05,740
예를 들어 장애물까지의 거리일 수 있습니다.

251
00:10:06,060 --> 00:10:08,820
로봇의 구성이 주어졌다고 하면,

252
00:10:08,820 --> 00:10:09,440
예를 들어 QS라고 하겠습니다.

253
00:10:09,440 --> 00:10:12,220
이 S는

254
00:10:12,220 --> 00:10:15,060
로봇이 가장 가까운 장애물로부터 얼마나 떨어져 있는지 알려줍니다.

255
00:10:15,500 --> 00:10:16,360
이해되셨습니까?

256
00:10:16,860 --> 00:10:18,960
또는 그 제약을

257
00:10:18,960 --> 00:10:19,500
다른 것으로 바꿀 수도 있습니다.

258
00:10:19,660 --> 00:10:22,220
예를 들어 장애물까지의 거리를 두는 대신,

259
00:10:22,300 --> 00:10:24,380
가장 가까운 매니폴드까지의 거리일 수도 있습니다.

260
00:10:24,920 --> 00:10:25,460
괜찮습니까?

261
00:10:25,820 --> 00:10:27,840
조작 매니폴드 같은 것일 수도 있습니다.

262
00:10:28,080 --> 00:10:29,740
그러면 이 S 함수는 알려져 있습니다.

263
00:10:30,080 --> 00:10:32,320
저는 이 PDE를 풀어서

264
00:10:32,320 --> 00:10:34,840
가치 함수인 T를 찾고 싶습니다.

265
00:10:34,840 --> 00:10:37,500
그리고 이 방정식을 풀면

266
00:10:37,500 --> 00:10:41,700
이러한 파면(wavefront)이 나오고, 이를 이용해 로봇이

267
00:10:41,700 --> 00:10:43,540
한 지점에서 다른 지점으로 이동할 수 있습니다.

268
00:10:43,800 --> 00:10:46,460
따라서 출발점을 주면, 이

269
00:10:46,460 --> 00:10:49,120
즉 PDE를 풀면, 이동 시간

270
00:10:49,120 --> 00:10:51,540
함수인 T를 얻게 됩니다.

271
00:10:51,880 --> 00:10:53,880
이 이동 시간 함수도 가치

272
00:10:53,880 --> 00:10:54,200
함수입니다.

273
00:10:54,400 --> 00:10:56,500
따라서 이 T의 그래디언트를 따라가기만 하면,

274
00:10:56,640 --> 00:10:59,280
환경 내 어떤 지점으로든

275
00:10:59,280 --> 00:10:59,980
이동할 수 있습니다.

276
00:10:59,980 --> 00:11:03,640
수치적으로는 FFM이라고 불리는 방법이 있는데,

277
00:11:03,720 --> 00:11:04,800
패스트 마칭 메서드(fast marching method)입니다.

278
00:11:04,940 --> 00:11:07,560
이 방법은 이 PDE를 풀지만, 수치적

279
00:11:07,560 --> 00:11:08,020
접근법입니다.

280
00:11:08,120 --> 00:11:10,340
3차원을 넘어서는 스케일링이 어렵습니다.

281
00:11:10,380 --> 00:11:12,660
그래서, 어, 우리가 이 PDE를 시작했을 때, 저희의

282
00:11:12,720 --> 00:11:15,340
주된 관심사는 이것을

283
00:11:15,340 --> 00:11:18,740
3차원을 넘어 실제로 복잡한 로봇 시스템까지 확장할 수 있는가였습니다.

284
00:11:20,040 --> 00:11:22,780
그래서 신경망이, 어,

285
00:11:22,780 --> 00:11:24,900
가능한 방법을 제시해 주었는데, 수치 해석기를 쓰는 대신

286
00:11:24,900 --> 00:11:27,400
신경망을 사용해서

287
00:11:27,400 --> 00:11:29,800
고차원에서 이 PDE를 풀 수 있다는 것이었습니다.

288
00:11:30,160 --> 00:11:33,120
그래서 우리가 처음으로 한 일은,

289
00:11:33,120 --> 00:11:36,540
이것을 풀려고 하는 신경망을 두는 것이었습니다.

290
00:11:36,540 --> 00:11:38,040
이 에이코널(Eikonal) PDE를 말입니다.

291
00:11:38,040 --> 00:11:40,700
그러면 이 신경망의 입력은, 그러니까

292
00:11:40,700 --> 00:11:43,200
이 신경망은 블랙박스이지만,

293
00:11:43,200 --> 00:11:45,480
이 PDE의 성질을

294
00:11:45,480 --> 00:11:48,820
존중하도록 신경망에 구조를 넣었습니다.

295
00:11:48,880 --> 00:11:50,740
이에 대해서는 간단히만 설명하겠지만, 관심이 있으시면

296
00:11:50,740 --> 00:11:53,480
논문을 찾아보셔서

297
00:11:53,480 --> 00:11:55,300
이 신경망의 구조를 이해해 보시기 바랍니다.

298
00:11:55,380 --> 00:11:57,420
하지만 단순화를 위해, 이것을

299
00:11:57,420 --> 00:12:01,340
로봇의

300
00:12:01,340 --> 00:12:05,600
입력은 로봇 시작 QS, 로봇 목표 QG, 그리고 환경 인지입니다.

301
00:12:05,600 --> 00:12:06,300
이를 입력으로 사용합니다.

302
00:12:06,300 --> 00:12:09,080
출력은 이 이동 시간 함수입니다.

303
00:12:09,600 --> 00:12:11,280
따라서 이는 미지의 함수입니다.

304
00:12:11,320 --> 00:12:13,280
그래서 저는 이 신경망을 학습시키고자 합니다.

305
00:12:13,280 --> 00:12:15,840
이 PDE를 풀도록 해서, 한 번 학습되면

306
00:12:16,000 --> 00:12:19,220
정확하거나 근사적으로 정확한 이동

307
00:12:19,220 --> 00:12:19,780
시간 함수를 갖게 됩니다.

308
00:12:20,120 --> 00:12:23,500
이제 이 모델을 학습시키는 방법은 다음과 같습니다.

309
00:12:23,500 --> 00:12:26,860
이 입력을 주면, 어, 신경망이 이를 처리합니다.

310
00:12:26,860 --> 00:12:27,680
이 이동 시간을 예측합니다.

311
00:12:27,680 --> 00:12:30,920
그다음 아이코널(Eikonal) PDE는 다음과 같이 말합니다.

312
00:12:30,920 --> 00:12:33,700
이 이동 시간을 입력에 대해 미분해 그래디언트를 구합니다.

313
00:12:33,700 --> 00:12:37,060
이 경우 입력은 QS이며, 그 결과는

314
00:12:37,060 --> 00:12:38,860
제약 함수의 역수와 같습니다.

315
00:12:39,440 --> 00:12:41,860
따라서 이 PDE는 우리에게 어떤 구조를 제공합니다.

316
00:12:42,380 --> 00:12:44,340
그래서 학습이 매우 단순해집니다.

317
00:12:44,580 --> 00:12:47,320
그래서 여러분은 로봇을 많이 샘플링하고

318
00:12:47,320 --> 00:12:48,600
시작 및 목표 구성(configuration)을 뽑습니다.

319
00:12:48,600 --> 00:12:50,600
환경 인지가 있습니다.

320
00:12:50,880 --> 00:12:51,460
환경 인지가 있습니다.

321
00:12:51,460 --> 00:12:52,980
이러한 각 쌍은

322
00:12:52,980 --> 00:12:53,600
신경망을 통과합니다.

323
00:12:53,780 --> 00:12:55,020
그들의 이동 시간을 예측합니다.

324
00:12:55,220 --> 00:12:59,700
그다음 역전파로 이 근사된 제약

325
00:12:59,700 --> 00:13:02,120
함수를 아이코널(Eikonal) PDE에 따라 계산합니다.

326
00:13:02,320 --> 00:13:03,280
그러면 끝입니다.

327
00:13:03,420 --> 00:13:03,600
네?

328
00:13:04,920 --> 00:13:05,540
질문이 있습니다.

329
00:13:05,960 --> 00:13:07,920
그러면 문제를 어떻게 인코딩하시나요?

330
00:13:07,920 --> 00:13:08,860
여러분의 공식에서는 어떻게 표현하시나요?

331
00:13:09,120 --> 00:13:13,440
음, SQ를 쓰는 건가요, 아니면요?

332
00:13:14,020 --> 00:13:14,900
좋은 질문입니다.

333
00:13:15,080 --> 00:13:16,020
그 부분은 곧 말씀드리겠습니다.

334
00:13:16,140 --> 00:13:16,420
알겠습니다.

335
00:13:16,580 --> 00:13:17,100
잠시 후에요.

336
00:13:17,380 --> 00:13:20,120
그래서 이 경우 QS는,

337
00:13:20,120 --> 00:13:22,640
모션 플래닝 문제는 로봇의

338
00:13:22,640 --> 00:13:24,900
시작, 목표, 그리고 환경 인지로 정의됩니다.

339
00:13:25,680 --> 00:13:26,120
맞습니까?

340
00:13:26,280 --> 00:13:28,340
표준 모션 플래닝 문제에서는 이를

341
00:13:28,340 --> 00:13:30,220
고전적 방법이나 옵티마이저에 넣습니다.

342
00:13:30,400 --> 00:13:32,240
그것은 시작과 목표, 그리고 여러분의

343
00:13:32,240 --> 00:13:35,140
환경, 예를 들면 장애물까지의 거리 같은 관측이나

344
00:13:35,140 --> 00:13:35,920
사용 가능한 어떤 관측이든을 받습니다.

345
00:13:36,300 --> 00:13:36,800
맞습니까?

346
00:13:37,020 --> 00:13:38,400
그래서 그것이 문제를 정의합니다.

347
00:13:39,120 --> 00:13:41,560
그리고 이 경우에는 우리는 오직

348
00:13:41,560 --> 00:13:42,500
이 문제 세트만을 제공합니다.

349
00:13:42,500 --> 00:13:45,780
즉 QS와 QG, 그리고 환경 인지가

350
00:13:45,780 --> 00:13:47,420
신경망을 통과하고 이것을 예측합니다,

351
00:13:47,420 --> 00:13:47,620
T입니다.

352
00:13:48,160 --> 00:13:51,140
그다음 이 손실을 학습시키는 방법은, 음,

353
00:13:51,340 --> 00:13:53,800
이 신경망을 그래디언트 매칭 손실

354
00:13:53,800 --> 00:13:54,160
함수로 학습시키는 것입니다.

355
00:13:54,440 --> 00:13:57,620
그래서 우리는 역전파로 이 그래디언트를

356
00:13:57,620 --> 00:13:58,400
QS에 대해 계산합니다.

357
00:13:58,680 --> 00:14:01,580
그리고 아이코널(Eikonal) PDE에 따르면 이것의

358
00:14:01,580 --> 00:14:05,820
그래디언트 노름의 역수가 기본적으로 제약 함수입니다.

359
00:14:07,140 --> 00:14:07,780
알겠습니까?

360
00:14:07,780 --> 00:14:11,020
그래서 이 제약 함수를 근사하고 이를 비교합니다,

361
00:14:11,020 --> 00:14:13,280
정답(ground truth) 제약 함수와요.

362
00:14:13,420 --> 00:14:15,440
예를 들어 정답(ground truth) 장애물 거리 같은 것입니다.

363
00:14:16,500 --> 00:14:17,460
이해되셨습니까?

364
00:14:17,620 --> 00:14:20,740
즉 손실 함수는 그래디언트에 대해 정의됩니다,

365
00:14:20,740 --> 00:14:21,520
신경망의요.

366
00:14:21,640 --> 00:14:23,380
순전파 경로에 두었다가

367
00:14:23,380 --> 00:14:25,760
어떤 출력을 계산한 다음 정의하는 것이 아닙니다.

368
00:14:25,760 --> 00:14:29,500
손실 함수는

369
00:14:29,500 --> 00:14:30,820
신경망의 그래디언트에 있습니다.

370
00:14:31,260 --> 00:14:31,740
네.

371
00:14:31,980 --> 00:14:32,200
좋습니다.

372
00:14:32,380 --> 00:14:32,700
질문입니다.

373
00:14:33,240 --> 00:14:34,920
신경망의 출력에 대해

374
00:14:34,920 --> 00:14:36,280
그리고 그 출력을 어떻게 사용해서

375
00:14:36,280 --> 00:14:36,700
그 출력을 통해

376
00:14:36,700 --> 00:14:39,080
원하는, 어, 로봇의 포즈를

377
00:14:39,080 --> 00:14:39,440
얻는지 설명해 주실 수 있습니까?

378
00:14:39,740 --> 00:14:42,060
네, 그 부분도 곧 말씀드리겠지만,

379
00:14:42,100 --> 00:14:43,460
아주 좋은 질문입니다.

380
00:14:43,460 --> 00:14:46,560
그래서 이 경우 출력은

381
00:14:46,560 --> 00:14:48,960
이동 시간 T입니다.

382
00:14:49,120 --> 00:14:50,260
이는 값 함수(value function)의 값입니다.

383
00:14:50,560 --> 00:14:52,200
그래서 그래디언트를 얻으려면,

384
00:14:52,200 --> 00:14:54,780
그 값 함수의 그래디언트를 따라가기만 하면 되고

385
00:14:54,780 --> 00:14:55,360
그 결과로

386
00:14:55,360 --> 00:14:56,700
궤적이 나옵니다.

387
00:14:57,200 --> 00:14:57,560
알겠습니까?

388
00:14:58,320 --> 00:15:00,820
그래서 우리는 이 단순한 그래디언트 매칭 손실

389
00:15:00,820 --> 00:15:01,100
함수를 사용합니다.

390
00:15:01,140 --> 00:15:02,880
그러니 생각해 보면 제가 필요한 유일한 데이터는

391
00:15:02,880 --> 00:15:04,960
무작위로 샘플링한 간단한 시작과

392
00:15:04,960 --> 00:15:05,200
목표,

393
00:15:06,640 --> 00:15:08,380
그리고 장애물까지의 거리입니다.

394
00:15:09,200 --> 00:15:09,840
그게 전부입니다.

395
00:15:09,840 --> 00:15:12,360
그래서 이것이 표준적인 모션 플래닝 도구인데,

396
00:15:12,360 --> 00:15:14,740
예를 들어 샘플링

397
00:15:14,740 --> 00:15:16,860
기반 방법인 RRT나 RRT*를 사용하면,

398
00:15:17,100 --> 00:15:19,640
충돌 검사기와

399
00:15:19,640 --> 00:15:21,120
구성을 무작위로 샘플링하는 방법이 필요합니다.

400
00:15:21,120 --> 00:15:23,560
그리고 나서

401
00:15:23,560 --> 00:15:24,740
궤적을 찾기 위한 메커니즘이 있지요, 맞습니까?

402
00:15:25,220 --> 00:15:26,200
우리의 데이터도 비슷합니다.

403
00:15:26,380 --> 00:15:30,660
우리는 로봇 구성을 무작위로 샘플링한 것과

404
00:15:30,660 --> 00:15:33,380
이 경우 제약조건 또는 충돌까지의 거리를

405
00:15:33,380 --> 00:15:33,600
필요로 합니다.

406
00:15:33,740 --> 00:15:36,740
그리고 그 제약 함수를

407
00:15:36,740 --> 00:15:39,280
전문가로 삼아 모델을 학습시킵니다.

408
00:15:39,280 --> 00:15:44,220
왜냐하면 신경망의 그래디언트가

409
00:15:44,220 --> 00:15:45,200
제약 함수를 근사하게 만들기 때문입니다.

410
00:15:45,360 --> 00:15:47,760
그리고 이렇게 근사된 제약 함수를

411
00:15:47,760 --> 00:15:48,320
실제 값과 비교합니다.

412
00:15:48,880 --> 00:15:49,480
답이 되었습니까?

413
00:15:49,780 --> 00:15:54,200
그래서 이론적으로는 모두 단순해 보이지만,

414
00:15:54,200 --> 00:15:55,260
실제로 적용해 보니,

415
00:15:55,400 --> 00:15:57,720
이 방법을 4

416
00:15:57,720 --> 00:15:58,060
차원을 넘어 확장할 수 없었습니다.

417
00:15:58,800 --> 00:16:02,440
수치 솔버는 3

418
00:16:02,440 --> 00:16:02,740
차원에서는 이를 풀고 있었습니다.

419
00:16:02,740 --> 00:16:04,860
우리는 '좋습니다, 4

420
00:16:04,860 --> 00:16:07,520
…차원이지만 수치적 방법도 4

421
00:16:07,520 --> 00:16:07,800
차원도 할 수 있었습니다.

422
00:16:07,960 --> 00:16:09,240
그저 조금 더 빠를 뿐이었습니다.

423
00:16:09,640 --> 00:16:12,680
그리고 나서 '무슨 문제가 있는지'를

424
00:16:12,680 --> 00:16:13,040
조사하기 시작했습니다.

425
00:16:13,540 --> 00:16:15,220
그 결과,

426
00:16:15,220 --> 00:16:16,860
두 가지 주요 한계가 있음을 알아냈습니다.

427
00:16:17,160 --> 00:16:20,720
첫째로, 아이코널 PDE는 해가 여러 개일 수 있고,

428
00:16:20,720 --> 00:16:23,060
우리의 신경망은 그 여러 해를

429
00:16:23,060 --> 00:16:24,380
포착하지 못했습니다.

430
00:16:24,380 --> 00:16:27,820
그래서 이 모델은

431
00:16:27,820 --> 00:16:29,400
그 다중 모드 문제로 어려움을 겪었습니다.

432
00:16:29,960 --> 00:16:33,320
둘째로, 저는 이 모델을

433
00:16:33,320 --> 00:16:35,020
무작위로 샘플링한 시작점과 목표점으로 학습시키기 때문에,

434
00:16:35,940 --> 00:16:40,260
연속된 구성들 사이의 그래디언트가 제어되지 않습니다.

435
00:16:40,360 --> 00:16:43,400
하지만 궤적을 생각해 보면,

436
00:16:43,400 --> 00:16:45,280
시작점에서 출발해

437
00:16:45,280 --> 00:16:46,140
다음 구성으로 이동하고,

438
00:16:46,360 --> 00:16:48,020
또 다음 구성으로 이동하면서

439
00:16:48,020 --> 00:16:48,760
하나의 궤적이 만들어집니다.

440
00:16:48,760 --> 00:16:52,000
따라서 연속된 구성들 사이의 그래디언트가

441
00:16:52,000 --> 00:16:54,800
제어되지 않으면 오류로 이어질 수 있습니다.

442
00:16:54,900 --> 00:16:56,800
그리고 잠시 후에

443
00:16:56,800 --> 00:16:57,680
그 모습이 어떤지 보여드리겠습니다.

444
00:16:58,520 --> 00:17:00,740
그래서 이것이 우리가 발견한 두 가지 한계였습니다.

445
00:17:00,740 --> 00:17:04,380
그리고 우리는 이것이 우리 모델을 본질적으로 제한한다고 생각했고,

446
00:17:04,380 --> 00:17:04,580
즉,

447
00:17:04,720 --> 00:17:07,100
4차원을 넘어서 확장하지 못하게 했습니다.

448
00:17:07,840 --> 00:17:10,720
그래서 우리가 처음으로 탐색한 해결책은,

449
00:17:10,720 --> 00:17:13,840
점성 아이코널 PDE를 사용할 수 있느냐는 것이었습니다.

450
00:17:14,040 --> 00:17:16,780
제가 강조한 문제 중 하나는 아이코널

451
00:17:16,780 --> 00:17:18,080
PDE가 해가 여러 개라는 점이었습니다.

452
00:17:18,080 --> 00:17:21,460
그런데 여기에 라플라시안을 추가하면,

453
00:17:21,460 --> 00:17:23,100
이 아이코널 PDE는 유일한 해를 갖게 됩니다.

454
00:17:24,980 --> 00:17:28,080
그리고 그것이

455
00:17:28,080 --> 00:17:28,580
훨씬 더 나은 성능으로 이어졌습니다.

456
00:17:28,860 --> 00:17:31,120
이 모델로 우리는

457
00:17:31,120 --> 00:17:33,120
최대 약 6자유도

458
00:17:33,120 --> 00:17:33,860
로봇 팔까지 확장할 수 있었습니다.

459
00:17:34,120 --> 00:17:36,440
또한 아주

460
00:17:36,440 --> 00:17:37,060
좁은 통로에서의 모션에도 적용할 수 있었습니다.

461
00:17:37,340 --> 00:17:41,180
하지만 음, 이 점성 아이코널 PDE에는

462
00:17:41,180 --> 00:17:41,600
비용이 따랐습니다.

463
00:17:41,780 --> 00:17:45,060
그래서 이제 저는 신경망을

464
00:17:45,060 --> 00:17:46,300
…에 대해 이차 미분한 항을 갖게 되고,

465
00:17:46,300 --> 00:17:47,900
즉 라플라시안이 생깁니다.

466
00:17:48,080 --> 00:17:51,200
그래서 이를 계산하는 데 계산 비용이 매우 컸습니다.

467
00:17:51,580 --> 00:17:53,580
그리고 학습 비용이 매우 높아졌습니다.

468
00:17:53,780 --> 00:17:56,540
그래서 그것은 다시 제가 원했던 목표와는 달랐습니다.

469
00:17:56,540 --> 00:17:59,040
저는 매우 빠르게 학습되는

470
00:17:59,040 --> 00:17:59,480
모델을 원했기 때문입니다.

471
00:17:59,700 --> 00:18:03,600
하지만 이 점성 아이코널 PDE는

472
00:18:03,600 --> 00:18:04,620
더 나은 성능으로 이어지긴 했습니다.

473
00:18:04,980 --> 00:18:08,640
하지만 그럼에도 우리는 여전히

474
00:18:08,640 --> 00:18:12,100
연속된 구성들 사이의 그래디언트를 조절하기 위한

475
00:18:12,100 --> 00:18:14,600
아무 조치도 하지 않았고, 무작위로 샘플링한

476
00:18:14,600 --> 00:18:15,760
시작점과 목표점으로 이 모델을 계속 학습시켰습니다.

477
00:18:16,520 --> 00:18:19,240
또 다른 기법도 진행 과정에서

478
00:18:19,240 --> 00:18:21,940
우리가 제안했는데, 이 라플라시안을

479
00:18:21,940 --> 00:18:23,920
그리슐리 에너지 최소화로 근사할 수 있습니다.

480
00:18:23,920 --> 00:18:26,000
자세한 내용은 다루지 않겠지만,

481
00:18:26,000 --> 00:18:28,800
관심이 있으시면 이 논문을 참고하시기 바랍니다.

482
00:18:29,060 --> 00:18:31,360
최근 IROS에 게재된 논문입니다.

483
00:18:31,700 --> 00:18:34,780
그래서 이 논문은, 예를 들어, 어떻게

484
00:18:34,780 --> 00:18:38,940
그리슐리 에너지 최소화를 사용해서 라플라시안을 근사하면서

485
00:18:38,940 --> 00:18:39,760
모델을 학습할 수 있는지 논의합니다.

486
00:18:40,400 --> 00:18:44,780
이는 라플라시안을 더 저렴하게 근사하는 방법입니다.

487
00:18:44,780 --> 00:18:46,920
그럼에도 여전히 많은 반복이 필요합니다.

488
00:18:46,920 --> 00:18:49,940
시간이 지나면서 이 라플라시안을

489
00:18:49,940 --> 00:18:50,420
근사하기 때문입니다.

490
00:18:50,460 --> 00:18:52,240
그래서 처음에는 모델이

491
00:18:52,240 --> 00:18:52,700
매우 나쁘게 동작할 것입니다.

492
00:18:54,200 --> 00:18:58,880
그래서 그 성질들로 다시 돌아가 보면,

493
00:18:58,880 --> 00:18:59,960
즉 해가 여러 개라는 점이 있고,

494
00:19:01,340 --> 00:19:04,320
또한 음,

495
00:19:04,320 --> 00:19:07,720
연속된 구성들 사이의 그래디언트가 제어되지 않는다는 점이 있습니다.

496
00:19:07,720 --> 00:19:10,780
우리는 이러한 문제들의 해법이

497
00:19:10,780 --> 00:19:14,480
아이코널 PDE의 성질들 안에 존재한다는 것을 알게 되었습니다.

498
00:19:14,740 --> 00:19:17,740
아이코널 PDE는 지오데식

499
00:19:17,740 --> 00:19:18,080
거리입니다.

500
00:19:18,220 --> 00:19:21,180
아이코널 PDE의 해는 지오데식

501
00:19:21,180 --> 00:19:21,780
함수입니다.

502
00:19:22,400 --> 00:19:25,300
따라서 지오데식 거리의 성질을

503
00:19:25,300 --> 00:19:27,360
예를 들어 삼각 부등식 같은 성질을

504
00:19:27,360 --> 00:19:29,880
비롯하여 여러 지오데식 거리

505
00:19:29,880 --> 00:19:32,360
성질들을 만족해야 합니다.

506
00:19:32,880 --> 00:19:36,140
또 지오데식 거리도 해가 여러

507
00:19:36,140 --> 00:19:36,620
개일 수 있습니다.

508
00:19:36,620 --> 00:19:40,500
즉 여러 지오데식 경로를 포착할 수 있는데,

509
00:19:40,600 --> 00:19:41,980
예를 들어 시작점과 목표점이 있으면,

510
00:19:42,160 --> 00:19:43,760
저는 이쪽으로 갈 수도 있고,

511
00:19:43,760 --> 00:19:44,800
저쪽으로도 갈 수 있습니다.

512
00:19:44,920 --> 00:19:48,940
그래서 가능한 해가 여러 개이고,

513
00:19:48,940 --> 00:19:51,140
그 해들 모두가 더 짧은 경로이거나

514
00:19:51,380 --> 00:19:53,440
이동 시간 측면에서의 해일 수도 있습니다.

515
00:19:53,680 --> 00:19:56,540
즉 구성 공간에 어떤 대칭성이

516
00:19:56,540 --> 00:19:58,000
있을 수도 있고,

517
00:19:58,120 --> 00:19:59,260
그래서 해가 여러 개가 될 수 있습니다.

518
00:19:59,680 --> 00:20:02,440
그 모두가 똑같이 중요할 수 있습니다.

519
00:20:02,440 --> 00:20:06,380
두 번째 성질은 에이코널

520
00:20:06,380 --> 00:20:07,920
PDE의 해가 가치 함수라는 점입니다.

521
00:20:08,340 --> 00:20:11,540
따라서 벨만 최적성 원리를 만족해야 합니다.

522
00:20:11,540 --> 00:20:13,100
이는 잠시 후에 설명드리겠습니다.

523
00:20:13,300 --> 00:20:15,560
그럼, 음, 첫 번째 성질부터 시작하겠습니다.

524
00:20:15,800 --> 00:20:18,140
에이코널 PDE의 해는 지오데식

525
00:20:18,140 --> 00:20:18,520
거리입니다.

526
00:20:20,400 --> 00:20:24,100
그래서, 그래서 메트릭 학습을 사용해야 합니다.

527
00:20:24,100 --> 00:20:28,100
예측이 유효한 메트릭 공간이 되도록 강제해야 하고

528
00:20:28,100 --> 00:20:28,520
즉,

529
00:20:28,520 --> 00:20:31,280
지오데식 거리의 모든 성질을

530
00:20:31,280 --> 00:20:32,180
존중할 수 있어야 합니다.

531
00:20:32,180 --> 00:20:35,380
따라서 지오데식 거리와의 일관성을 보장해야 합니다.

532
00:20:36,000 --> 00:20:38,660
예를 들어, 음, 대칭성 성질이 있습니다.

533
00:20:38,980 --> 00:20:41,060
음, 삼각 부등식도 있습니다.

534
00:20:41,520 --> 00:20:45,620
예를 들어, 시작과 목표 사이의

535
00:20:45,620 --> 00:20:46,340
직선 경로와,

536
00:20:46,540 --> 00:20:50,220
어떤 중간 지점을 거치는 경로에서

537
00:20:50,220 --> 00:20:51,900
그 경로의 길이는

538
00:20:51,900 --> 00:20:52,540
직선보다 더 길어야 합니다.

539
00:20:53,020 --> 00:20:53,540
알겠습니까?

540
00:20:53,580 --> 00:20:54,980
이것이 삼각 부등식입니다.

541
00:20:55,260 --> 00:20:58,660
따라서 이런 지오데식 거리에서는 신경망도

542
00:20:58,660 --> 00:20:59,960
그 성질을 만족해야 합니다.

543
00:20:59,960 --> 00:21:04,200
그래서 이를 위해, 음, 이런 구조를

544
00:21:04,200 --> 00:21:04,880
신경망에 제안합니다.

545
00:21:05,180 --> 00:21:07,780
이 구조가 무엇을 하는지 말씀드리면, 이전에는

546
00:21:07,780 --> 00:21:08,240
우리는 이렇게 하고 있었습니다.

547
00:21:08,340 --> 00:21:09,620
그러니 이 문제를 생각해 보겠습니다.

548
00:21:09,760 --> 00:21:11,440
A에서 B로 가는 두 경로가 있다고 하겠습니다.

549
00:21:13,020 --> 00:21:15,400
만약 손실 함수에서 L2 노름만 사용하면,

550
00:21:15,400 --> 00:21:17,500
그것이 이 두 경로를

551
00:21:17,500 --> 00:21:18,000
이 직선으로 압축해 버립니다.

552
00:21:19,300 --> 00:21:21,680
하지만 우리가 제안한 이 손실 함수를 쓰면,

553
00:21:21,680 --> 00:21:22,800
즉 우리가 가진 이 구조를 쓰면,

554
00:21:22,940 --> 00:21:26,560
QS를 받아서 이를

555
00:21:26,560 --> 00:21:27,480
어떤 잠재 상태로 보내고,

556
00:21:27,480 --> 00:21:29,760
그다음 그 위에 맥스 풀링을

557
00:21:29,760 --> 00:21:30,100
적용합니다.

558
00:21:30,680 --> 00:21:33,080
그러면 이것이 하는 일은 해를 구간별로

559
00:21:33,080 --> 00:21:34,480
근사하는 것입니다.

560
00:21:34,640 --> 00:21:36,660
그래서 보시면, 이런 마름모꼴 형태의

561
00:21:36,660 --> 00:21:38,700
경로 근사를 얻게 됩니다.

562
00:21:38,800 --> 00:21:41,640
이 원들 대신, 여러분은 구간별로 근사해서

563
00:21:41,640 --> 00:21:43,100
그 여러 해들을 모두 근사합니다.

564
00:21:43,260 --> 00:21:46,020
그래서 여기의 메트릭 학습은

565
00:21:46,020 --> 00:21:47,280
QS와 QG를 받아서,

566
00:21:47,500 --> 00:21:49,380
어떤 신경망을 통과시키고,

567
00:21:49,380 --> 00:21:53,500
그리고 나서 QS와 QG의 잠재

568
00:21:53,500 --> 00:21:55,840
특징에 대해 어떤 연산을 합니다,

569
00:21:55,840 --> 00:21:57,940
예를 들어 맥스 연산을 적용하고, 그다음

570
00:21:57,940 --> 00:22:01,140
음, 무한 노름을

571
00:22:01,340 --> 00:22:02,600
그 잠재 표현에 적용합니다.

572
00:22:02,720 --> 00:22:05,200
그러니 생각해 보시면, 우리는

573
00:22:05,200 --> 00:22:07,400
시작점과 목표점을 어떤 잠재 공간으로 옮긴 다음,

574
00:22:07,400 --> 00:22:10,100
그 잠재 공간에서 지오데식 거리를

575
00:22:10,100 --> 00:22:10,860
학습합니다.

576
00:22:11,880 --> 00:22:12,780
이해되십니까?

577
00:22:14,840 --> 00:22:15,360
좋습니다.

578
00:22:15,480 --> 00:22:17,040
그래서 이것이 우리가 한 첫 번째 변경입니다.

579
00:22:17,700 --> 00:22:20,640
둘째로, 이 에이코널 PDE의 해는

580
00:22:20,860 --> 00:22:21,900
가치 함수입니다.

581
00:22:21,900 --> 00:22:24,620
따라서 벨만 최적성 원리를 따라야 합니다.

582
00:22:25,200 --> 00:22:27,680
그리고 우리는 이것이 실제로

583
00:22:27,680 --> 00:22:28,180
해결해 주고 있다는 것을 알게 되었는데,

584
00:22:28,400 --> 00:22:31,080
연속된 점들 사이에서 기울기가

585
00:22:31,080 --> 00:22:33,860
통제되지 않는 문제를 해결할 수 있었습니다.

586
00:22:34,060 --> 00:22:36,540
그러니 이 간단한 에이코널 PDE를 생각해 보십시오.

587
00:22:36,860 --> 00:22:39,200
이는 절댓값 함수입니다.

588
00:22:40,080 --> 00:22:42,200
그러니 제가 이 절댓값 함수를 가지고 있고,

589
00:22:42,200 --> 00:22:43,940
제가 신경망을 학습시키는 방식은

590
00:22:43,940 --> 00:22:46,660
저 초록색 점들을 무작위로 샘플링하는 것입니다.

591
00:22:47,440 --> 00:22:50,280
그래서 제가 그 점들로 신경망을 학습시키면

592
00:22:50,280 --> 00:22:50,820
그 점들에서

593
00:22:50,960 --> 00:22:53,200
손실 함수는 오직 그

594
00:22:53,200 --> 00:22:53,640
점들에서만 최소화됩니다.

595
00:22:53,880 --> 00:22:56,600
그 구간들에서는 아무것도 하지 않습니다.

596
00:22:57,140 --> 00:23:01,300
따라서 그 신경망 입장에서는 두 함수가 모두

597
00:23:01,300 --> 00:23:01,720
정확합니다,

598
00:23:03,240 --> 00:23:05,820
왜냐하면 그 초록색 점들에서의 오차가

599
00:23:05,820 --> 00:23:06,100
0이기 때문입니다.

600
00:23:06,280 --> 00:23:08,420
하지만 그 사이에 작은 점프가

601
00:23:08,420 --> 00:23:09,340
있다면,

602
00:23:09,480 --> 00:23:11,540
제 신경망은 그것을 알지 못합니다,

603
00:23:11,540 --> 00:23:14,480
왜냐하면 제가 조절하기 위해 아무것도 하지 않았고

604
00:23:14,480 --> 00:23:17,460
그 연속된 점들 사이의 기울기를 규제하지 않았기 때문입니다.

605
00:23:17,580 --> 00:23:19,780
하지만 로봇 궤적을 생각해 보면,

606
00:23:20,100 --> 00:23:22,400
한 점에서 시작해서 그리고 여러분은,

607
00:23:22,400 --> 00:23:23,360
계속 진행합니다.

608
00:23:23,640 --> 00:23:25,820
그 사이에 이런 작은 오차들이 있으면,

609
00:23:26,220 --> 00:23:29,000
궤적이 원하는

610
00:23:29,000 --> 00:23:29,240
목표에서 벗어날 수 있습니다.

611
00:23:29,440 --> 00:23:30,440
이해되십니까?

612
00:23:32,080 --> 00:23:34,600
그래서 우리는 이 기울기를 규제해서

613
00:23:34,600 --> 00:23:37,360
여러분이 보고 있는 이런 범프들이

614
00:23:37,380 --> 00:23:39,880
이 절댓값 함수를 포착하도록 해야 했습니다.

615
00:23:41,360 --> 00:23:43,740
그래서 이 문제의 해결책은,

616
00:23:43,880 --> 00:23:47,320
음, 에이코널 PDE의 해를 최적

617
00:23:47,320 --> 00:23:48,480
가치 함수의 값으로 취급하는 것이고,

618
00:23:48,680 --> 00:23:52,060
또한 벨만 최적성 원리를 만족해야 합니다.

619
00:23:53,060 --> 00:23:56,200
따라서 벨만

620
00:23:56,200 --> 00:23:57,360
최적성 원리를 만족해야 한다고 할 때,

621
00:23:57,560 --> 00:23:59,760
우리는 시간차 학습을 해야 합니다.

622
00:24:02,500 --> 00:24:05,460
표준적인

623
00:24:05,460 --> 00:24:07,600
Q-러닝, 딥 Q-러닝을 알고 계신 분은 몇 분이나 계십니까?

624
00:24:09,300 --> 00:24:11,100
일부이긴 하지만, 대부분은 알고 계시는군요.

625
00:24:11,280 --> 00:24:13,080
그래서 Q-러닝에서는 보상

626
00:24:13,080 --> 00:24:13,480
함수가 있고,

627
00:24:14,540 --> 00:24:17,680
다음 상태의 다음 행동에서의 Q 함수에서,

628
00:24:17,980 --> 00:24:20,620
현재 상태의 현재

629
00:24:20,620 --> 00:24:21,300
행동에서의 Q 함수를 빼는 것이지요?

630
00:24:21,300 --> 00:24:22,940
이것이 Q-러닝입니다.

631
00:24:24,120 --> 00:24:27,380
그리고 이것은 시간차(Temporal Difference) 값에서 나왔는데,

632
00:24:27,540 --> 00:24:30,100
가치 함수로 보면, 다음 상태에서의 보상 값이

633
00:24:30,100 --> 00:24:30,540
있고,

634
00:24:30,540 --> 00:24:33,260
현재 상태의 가치를 빼는 것과 비슷하지요?

635
00:24:33,580 --> 00:24:35,300
그래서 이 식은 유사합니다.

636
00:24:35,600 --> 00:24:38,700
저희는 이 식에 도달하는 완전한 유도 과정을

637
00:24:38,700 --> 00:24:39,680
보여드립니다.

638
00:24:39,680 --> 00:24:42,720
하지만 핵심은 이것이

639
00:24:42,720 --> 00:24:43,400
가치 함수라는 점입니다.

640
00:24:43,640 --> 00:24:45,260
즉 Q들이 있고,

641
00:24:45,640 --> 00:24:48,600
그 Q들을 어떤 방향으로 교란합니다.

642
00:24:48,600 --> 00:24:50,860
그러면 다음 상태에서의 값이 있고,

643
00:24:52,440 --> 00:24:54,440
여기에 어떤 보상이 더해지고,

644
00:24:54,660 --> 00:24:58,200
그리고 다음이 아니라, 음, 이전 또는

645
00:24:58,200 --> 00:24:59,160
현재 상태의 값이 있습니다.

646
00:24:59,160 --> 00:25:01,060
따라서 T는 가치 함수입니다.

647
00:25:01,980 --> 00:25:05,180
저희는 어떤 방향으로 Q들을 교란하여

648
00:25:05,180 --> 00:25:06,220
다음 상태를 얻습니다.

649
00:25:06,560 --> 00:25:08,280
그래서 이것은 다음 상태에서의 값입니다.

650
00:25:08,960 --> 00:25:10,780
저것은 현재 상태에서의 값입니다.

651
00:25:10,900 --> 00:25:13,100
그리고 이것은 비용 함수이고,

652
00:25:13,220 --> 00:25:15,800
이는, 음, 제약 조건의 역수로,

653
00:25:15,900 --> 00:25:17,540
여러분이 주는 값이 예를 들어 거리 같은 것입니다.

654
00:25:17,780 --> 00:25:20,740
그래서 제가 최소화하거나 최대화하고 싶다면

655
00:25:20,740 --> 00:25:21,900
장애물까지의 거리를,

656
00:25:22,120 --> 00:25:24,120
그것이 보상 함수이거나 비용

657
00:25:24,120 --> 00:25:24,420
함수입니다.

658
00:25:26,740 --> 00:25:30,880
그래서 이 시간차 학습을 결합하면,

659
00:25:30,980 --> 00:25:33,480
즉 저희 논문에서는, 여러분이

660
00:25:33,480 --> 00:25:35,400
이 U를 해석적으로 계산할 수 있음을 보여드립니다.

661
00:25:35,640 --> 00:25:38,180
그렇다면 이 Q들을 어떻게 교란해서

662
00:25:38,180 --> 00:25:38,720
어떤 방향으로

663
00:25:38,720 --> 00:25:41,320
다음의 보존적인 상태를 얻을 수 있을까요?

664
00:25:41,520 --> 00:25:44,440
그래서 기본적으로 이 U는, 음, 짧게 말하면,

665
00:25:44,820 --> 00:25:47,660
그냥 이동 시간의 기울기입니다.

666
00:25:47,700 --> 00:25:51,540
그래서 여러분은 작은 한 걸음을

667
00:25:51,540 --> 00:25:54,280
음, 이동 시간 기울기의 방향으로

668
00:25:54,280 --> 00:25:55,120
이동합니다.

669
00:25:55,220 --> 00:25:56,160
그래서 그것이 다음 상태입니다.

670
00:25:56,920 --> 00:25:57,440
이해되시지요?

671
00:25:57,760 --> 00:26:00,160
그리고 그 기울기는 해석적으로 계산할 수 있습니다.

672
00:26:01,540 --> 00:26:04,740
그래서 그다음 저희는 기울기 매칭 손실을

673
00:26:04,740 --> 00:26:06,000
이 3D 손실과 결합했고,

674
00:26:06,740 --> 00:26:10,640
그러고 나서 이 모델은

675
00:26:10,640 --> 00:26:11,300
매우 잘 수행할 수 있었습니다.

676
00:26:11,380 --> 00:26:13,060
우리는 이를 아주

677
00:26:13,060 --> 00:26:14,120
고차원 문제로 확장할 수 있었고

678
00:26:14,120 --> 00:26:15,660
곧 제가 보여드리겠습니다.

679
00:26:15,980 --> 00:26:19,040
그래서 먼저, 아주 단순한 문제에서,

680
00:26:19,580 --> 00:26:21,340
이는 미로 같은 환경입니다.

681
00:26:21,340 --> 00:26:24,460
만약 여러분이, 음,

682
00:26:24,620 --> 00:26:27,120
기울기 매칭 손실 같은 접근을 3D

683
00:26:27,120 --> 00:26:27,420
학습과 결합하면,

684
00:26:27,740 --> 00:26:29,320
이런 종류의 등고선을 얻고,

685
00:26:29,380 --> 00:26:31,800
이것이 이 전문가 결과와 일치하는 것을 보실 수 있습니다.

686
00:26:31,800 --> 00:26:35,580
즉 전문가, 음, FFM과의 일치입니다.

687
00:26:35,680 --> 00:26:36,960
이는 수치적 접근법입니다.

688
00:26:37,060 --> 00:26:38,940
3차원까지만 가능하고,

689
00:26:39,120 --> 00:26:40,640
아주 좋은 컴퓨터가 있다면,

690
00:26:40,760 --> 00:26:41,640
4차원까지도 할 수 있습니다.

691
00:26:41,940 --> 00:26:44,360
하지만 시간 등고선이 거의

692
00:26:44,360 --> 00:26:44,800
비슷합니다.

693
00:26:45,400 --> 00:26:49,120
그래서 이런, 음, 신경망

694
00:26:49,120 --> 00:26:49,780
접근법을 쓰면,

695
00:26:49,900 --> 00:26:51,160
비슷한 해를 포착할 수 있습니다.

696
00:26:51,320 --> 00:26:53,100
하지만 여기에는 두 가지 방법이 있습니다.

697
00:26:53,460 --> 00:26:54,780
첫 번째는 저희의 첫 번째 방법입니다.

698
00:26:54,980 --> 00:26:57,280
이는 3D 학습이나 메트릭

699
00:26:57,280 --> 00:26:57,620
학습 없이 수행한 것입니다.

700
00:26:57,620 --> 00:27:01,100
처음에는 소스에서의 기울기가

701
00:27:01,100 --> 00:27:01,840
초기에는

702
00:27:01,840 --> 00:27:02,520
아주 좋다는 것을 볼 수 있지만,

703
00:27:02,540 --> 00:27:04,820
소스 지점에서 멀어질수록

704
00:27:04,820 --> 00:27:05,100
즉 멀리 갈수록,

705
00:27:05,240 --> 00:27:07,120
기울기가 매우 나빠지기 시작합니다.

706
00:27:07,500 --> 00:27:10,120
그래서 보존적 상태들 사이의 오차가

707
00:27:10,120 --> 00:27:12,860
전파되면서 모든 것이 잘못됩니다.

708
00:27:13,660 --> 00:27:16,260
그리고 이것이 점성 에이코널 PDE입니다.

709
00:27:16,500 --> 00:27:19,340
이는 이것보다는 더 나은 해를 갖지만,

710
00:27:19,500 --> 00:27:21,020
그래도 여전히,

711
00:27:21,140 --> 00:27:22,940
음, 정규화하지는 못했고,

712
00:27:24,060 --> 00:27:26,020
보존적 상태들 사이의 기울기를요.

713
00:27:26,200 --> 00:27:29,380
그래서 그 플롯에서 보시는 것처럼

714
00:27:29,380 --> 00:27:30,360
여전히 몇 가지 인공물이 있습니다.

715
00:27:30,720 --> 00:27:34,420
그래서 기울기 매칭을 TD 학습과 결합하면

716
00:27:34,420 --> 00:27:34,600
즉,

717
00:27:34,660 --> 00:27:36,220
훨씬 더 좋은 결과로 이어집니다.

718
00:27:37,380 --> 00:27:39,780
그래서 이제 하나씩 논의하겠습니다.

719
00:27:39,780 --> 00:27:41,580
추론 효율성,

720
00:27:41,580 --> 00:27:42,540
학습 효율성,

721
00:27:42,900 --> 00:27:44,500
그리고 이 모델의 적응성입니다.

722
00:27:46,580 --> 00:27:48,640
그럼 추론 효율성부터 시작하겠습니다.

723
00:27:50,280 --> 00:27:52,060
이는 간단한, 음,

724
00:27:52,240 --> 00:27:54,080
7자유도(DOF) 설정입니다.

725
00:27:54,400 --> 00:27:55,700
보시다시피 여기에서,

726
00:27:55,760 --> 00:27:59,060
그 계획을 찾는 데 0.07초가 걸리고,

727
00:27:59,060 --> 00:27:59,660
바로 그 계획입니다.

728
00:28:00,320 --> 00:28:03,280
그리고, 음, MpyNet은 음,

729
00:28:03,280 --> 00:28:03,840
NVIDIA에서 나온 것입니다.

730
00:28:03,840 --> 00:28:06,660
방대한 양의 데이터로 학습되었습니다.

731
00:28:06,820 --> 00:28:07,480
제가 보여드리겠습니다.

732
00:28:07,660 --> 00:28:09,700
그 데이터를 수집하는 데는 몇 주가 걸렸고요,

733
00:28:09,700 --> 00:28:09,980
데이터를요,

734
00:28:10,100 --> 00:28:11,720
그 모델을 학습시키는 데는 1주일이 걸렸습니다.

735
00:28:11,720 --> 00:28:12,260
그 모델입니다.

736
00:28:12,900 --> 00:28:14,920
반면 저희 모델은,

737
00:28:15,580 --> 00:28:17,440
제가 보여드리겠지만,

738
00:28:17,680 --> 00:28:17,800
음,

739
00:28:19,500 --> 00:28:20,860
데이터 수집에 1분도 채 걸리지 않았고,

740
00:28:21,520 --> 00:28:23,920
학습에도 1시간도 채 걸리지 않았습니다, 그러니까 음,

741
00:28:24,140 --> 00:28:26,260
300개 환경에서 이 모델을 학습시킨 것입니다.

742
00:28:26,260 --> 00:28:29,300
즉 하나의 모델이 300개의

743
00:28:29,300 --> 00:28:31,040
서로 다른 환경으로 일반화됩니다.

744
00:28:31,400 --> 00:28:33,500
그럼에도 MpyNet과, 음,

745
00:28:33,660 --> 00:28:35,480
전체적으로 보시면 성공률이 매우

746
00:28:35,480 --> 00:28:35,900
비슷합니다.

747
00:28:36,600 --> 00:28:40,000
그리고 계획 시간은, 음, 저희 경우가

748
00:28:40,000 --> 00:28:40,560
훨씬 더 빠릅니다.

749
00:28:40,840 --> 00:28:45,180
또 lazy PRM의 경우, 이 값은

750
00:28:45,180 --> 00:28:46,500
그래프 질의 시간만을 보여줍니다.

751
00:28:46,820 --> 00:28:50,020
그래프 구축 시간은 포함하지 않았습니다.

752
00:28:50,680 --> 00:28:52,780
그래서 그래프를 미리 구축해 두고

753
00:28:52,780 --> 00:28:54,960
그 그래프를 사용해, 음, 시간을 질의했고

754
00:28:54,960 --> 00:28:55,380
그 시간을 측정했습니다.

755
00:28:58,960 --> 00:29:01,120
그다음에는, 그러니까,

756
00:29:01,240 --> 00:29:03,680
매우 복잡한, 음, 실내 환경으로도 확장되었습니다.

757
00:29:03,700 --> 00:29:05,460
즉 다층 구조의 환경입니다.

758
00:29:05,740 --> 00:29:08,440
그리고 성공률도 다시 보시다시피

759
00:29:08,440 --> 00:29:09,020
매우 높습니다.

760
00:29:09,460 --> 00:29:11,280
그리고 계획 시간도 매우 낮고

761
00:29:11,280 --> 00:29:13,440
다른 접근법과 비교하면, 그러니까 음,

762
00:29:13,780 --> 00:29:15,260
그래서 차이가 있는데요,

763
00:29:15,400 --> 00:29:16,680
이것이 가치함수이기 때문입니다.

764
00:29:16,980 --> 00:29:20,260
이 가치함수를 MPC에 통합할 수 있고

765
00:29:20,260 --> 00:29:21,180
또는 MPPI에도 통합할 수 있습니다.

766
00:29:21,180 --> 00:29:25,520
따라서 음, 이용 가능한 어떤 도구든

767
00:29:25,520 --> 00:29:26,600
가치함수를 사용할 수 있다면요.

768
00:29:26,780 --> 00:29:29,160
궤적 최적화에도 통합할 수 있습니다.

769
00:29:29,500 --> 00:29:31,540
그래서 저희는 이것을

770
00:29:31,540 --> 00:29:32,180
MPPI와 결합하면,

771
00:29:32,460 --> 00:29:35,120
훨씬 더 빠르게 동작함을 보였는데,

772
00:29:35,120 --> 00:29:37,520
그때는

773
00:29:37,520 --> 00:29:37,800
그래디언트를 계산할 필요가 없기 때문입니다.

774
00:29:37,960 --> 00:29:40,660
그냥 음, 예측을 그대로 사용하시면 되고,

775
00:29:40,940 --> 00:29:43,260
cost-to-go 또는 가치함수

776
00:29:43,260 --> 00:29:44,120
출력값을요.

777
00:29:44,480 --> 00:29:47,160
하지만 이 그래디언트는, 또한

778
00:29:47,160 --> 00:29:47,480
이 그래디언트를

779
00:29:47,480 --> 00:29:49,280
이 이동 시간의 그래디언트로 궤적을

780
00:29:49,280 --> 00:29:49,700
여전히 찾아내는 데도 사용할 수 있습니다.

781
00:29:50,220 --> 00:29:54,040
음, 음, 계산 시간도

782
00:29:54,040 --> 00:29:54,480
크게 증가하지 않고,

783
00:29:55,000 --> 00:29:56,480
성공률도 높습니다.

784
00:29:57,940 --> 00:29:59,980
그리고 나서 저희는 이를

785
00:29:59,980 --> 00:30:01,060
트라이얼 DOF까지 확장할 수 있었습니다.

786
00:30:01,200 --> 00:30:03,440
이는 이러한 모델들에게 매우 복잡한 환경입니다,

787
00:30:03,440 --> 00:30:03,720
이 모델들에겐요,

788
00:30:03,720 --> 00:30:05,640
아주 얇은 장애물이 있기 때문이고,

789
00:30:05,860 --> 00:30:07,620
이러한 PDE 해법은 어려움을 겪습니다

790
00:30:07,620 --> 00:30:09,220
이런 얇은 장애물이 있을 때입니다.

791
00:30:09,240 --> 00:30:11,060
그래서 저희는 이 환경을 특별히 선택했습니다

792
00:30:11,060 --> 00:30:14,580
이런 얇은 장애물을

793
00:30:14,580 --> 00:30:14,920
잘 포착할 수 있음을 보여드리기 위해서입니다.

794
00:30:14,920 --> 00:30:16,560
그리고도, 음, 계획을 수행할 수 있습니다.

795
00:30:17,560 --> 00:30:19,140
이는 15 DOF입니다,

796
00:30:20,980 --> 00:30:24,760
그리고 이 환경은 특히 이

797
00:30:24,760 --> 00:30:24,960
로봇에게 도전적입니다.

798
00:30:25,060 --> 00:30:26,260
이 로봇은 매우 크고,

799
00:30:26,280 --> 00:30:27,660
이 방은 매우 작았으며,

800
00:30:27,680 --> 00:30:29,380
그래서 매우 협소한 환경이고

801
00:30:29,380 --> 00:30:31,280
이 로봇이 움직일 수 있어야

802
00:30:31,280 --> 00:30:33,000
한 지점에서 다른 지점으로 이동할 수 있습니다.

803
00:30:34,020 --> 00:30:36,460
지금까지가, 그러니까, 추론 효율성에 관한 내용이었습니다.

804
00:30:36,720 --> 00:30:38,640
제가, 제가 이런 모델들을 보여드렸는데,

805
00:30:38,760 --> 00:30:40,100
차원이 높아질수록,

806
00:30:40,280 --> 00:30:42,220
계산상의 이점을 유지한다는 점입니다.

807
00:30:42,860 --> 00:30:44,160
이제 제가 가장 좋아하는 슬라이드인데요,

808
00:30:44,240 --> 00:30:46,080
제가, 제가 학습 효율성에 대해 말씀드리겠습니다.

809
00:30:50,300 --> 00:30:52,340
여기에는 서로 다른 환경들이 있고,

810
00:30:52,480 --> 00:30:54,260
예를 들어 이 7 DOF(자유도)

811
00:30:54,260 --> 00:30:54,920
환경만 선택한다고 하면,

812
00:30:56,460 --> 00:30:57,940
그 환경에서는,

813
00:30:58,200 --> 00:31:00,100
데이터 수집에, 그러니까, 음,

814
00:31:00,100 --> 00:31:02,460
어, 저희는 50

815
00:31:02,460 --> 00:31:02,880
분이 걸렸습니다,

816
00:31:02,880 --> 00:31:04,120
데이터를 수집하는 데요.

817
00:31:04,360 --> 00:31:05,600
즉 환경 하나당,

818
00:31:05,600 --> 00:31:06,980
몇 초씩 걸렸고,

819
00:31:07,180 --> 00:31:08,640
그래서 50을 곱하면,

820
00:31:08,780 --> 00:31:10,040
대략 50분이 걸렸습니다.

821
00:31:10,460 --> 00:31:13,260
학습에는, 어, 46분이 걸렸고,

822
00:31:13,320 --> 00:31:15,220
EmpireNet과 비교했는데,

823
00:31:15,380 --> 00:31:17,720
이 수치들은 그들의 논문에서 가져온 것입니다.

824
00:31:17,900 --> 00:31:19,520
그들은 많은 양의 데이터를

825
00:31:19,520 --> 00:31:19,880
수집하는 데

826
00:31:20,040 --> 00:31:21,460
몇 주가 걸렸고,

827
00:31:21,480 --> 00:31:22,900
그리고 학습에는 1주일이 걸렸습니다.

828
00:31:23,180 --> 00:31:25,640
저희 모델은 GPU 한 대로 학습시켰고,

829
00:31:26,340 --> 00:31:27,340
표준적인 3090이었습니다,

830
00:31:28,000 --> 00:31:30,080
반면 EmpireNet은 학습되었는데,

831
00:31:30,100 --> 00:31:32,800
음, NVIDIA Tesla GPU 8대로 학습되었습니다.

832
00:31:32,960 --> 00:31:34,860
그래서 상당한, 음,

833
00:31:34,940 --> 00:31:36,300
막대한 컴퓨팅 자원이

834
00:31:36,300 --> 00:31:37,280
그 모델을 학습시키는 데 필요했습니다.

835
00:31:38,540 --> 00:31:39,460
그리고, 음,

836
00:31:39,780 --> 00:31:41,020
이 Gibson 환경에서는,

837
00:31:41,260 --> 00:31:42,080
보시다시피,

838
00:31:42,400 --> 00:31:44,380
음, 데이터 수집에 24초가

839
00:31:44,380 --> 00:31:45,160
걸렸고,

840
00:31:45,260 --> 00:31:46,120
학습에는 9분이 걸렸습니다.

841
00:31:46,260 --> 00:31:46,960
그래서 제가,

842
00:31:46,980 --> 00:31:48,200
여기서 강조하고 싶은 것은,

843
00:31:48,340 --> 00:31:50,540
이런 PDE 프라이어를 사용하면,

844
00:31:52,380 --> 00:31:53,400
그러니까,

845
00:31:53,480 --> 00:31:55,100
학습 효율이 훨씬 높아진다는 점입니다.

846
00:31:55,840 --> 00:31:57,700
제 관점에서는,

847
00:31:58,660 --> 00:32:00,780
물리학자들이 이런 물리 모델을 제공해 주었고,

848
00:32:01,320 --> 00:32:03,380
그것이 어떤 동역학 시스템을 지배합니다.

849
00:32:03,680 --> 00:32:06,160
이제 궤적들을 다시 모으고 있습니다,

850
00:32:06,340 --> 00:32:07,400
이 모델들은 무시한 채로,

851
00:32:07,620 --> 00:32:09,640
그저 이 모델들을 재현하기 위해서입니다.

852
00:32:10,460 --> 00:32:10,980
알겠습니까?

853
00:32:11,160 --> 00:32:12,640
그렇다면 왜 그냥 이를 사용하지 않습니까?

854
00:32:12,740 --> 00:32:13,260
이미 존재합니다.

855
00:32:13,300 --> 00:32:14,360
그리고 이를 사용하면,

856
00:32:14,520 --> 00:32:16,880
학습 효율이 매우 높아지고,

857
00:32:16,940 --> 00:32:19,040
추론 효율도 매우 높아지며,

858
00:32:19,040 --> 00:32:20,860
또한 이 모델들은 여전히

859
00:32:20,860 --> 00:32:22,440
매우 복잡한 환경에도 적응할 수 있습니다.

860
00:32:22,640 --> 00:32:24,360
그리고 이 모델들은 전이 가능합니다.

861
00:32:24,820 --> 00:32:27,100
그래서 제가 이 연구를 시작했을 때,

862
00:32:28,320 --> 00:32:29,880
저희 모델은 학습에

863
00:32:29,900 --> 00:32:31,560
대략 이틀이 걸렸습니다.

864
00:32:32,220 --> 00:32:33,960
그러니까 어, 학습이 이틀씩 걸렸는데,

865
00:32:34,000 --> 00:32:36,460
저희가 제대로 만족시키지 못했기 때문입니다.

866
00:32:36,460 --> 00:32:37,880
이러한 PDE들의 성질을 말입니다.

867
00:32:37,940 --> 00:32:39,780
그런데 이런 성질들을 만족시키고 나니,

868
00:32:40,040 --> 00:32:42,160
결국, 한

869
00:32:42,160 --> 00:32:42,300
시간도

870
00:32:42,300 --> 00:32:44,320
채 안 걸릴 정도가 되었고, 제 연구실은 지금도 더 앞으로 나아가려고

871
00:32:45,020 --> 00:32:46,260
그 이상을 계속 밀어붙이고 있습니다.

872
00:32:46,800 --> 00:32:49,580
그러니까 어, 저희가, 저희는,

873
00:32:49,580 --> 00:32:51,600
논문을 아직 준비 중입니다.

874
00:32:51,800 --> 00:32:53,420
제 생각에 저희 최신 모델은,

875
00:32:53,620 --> 00:32:55,360
음, 12 자유도에서,

876
00:32:55,540 --> 00:32:57,720
학습에 5분도

877
00:32:57,720 --> 00:32:57,960
채 걸리지 않습니다.

878
00:32:58,860 --> 00:33:00,120
그래서 훨씬 빠릅니다.

879
00:33:00,280 --> 00:33:00,500
예?

880
00:33:01,460 --> 00:33:03,680
그러면, 예를 들어,

881
00:33:03,800 --> 00:33:06,260
S, 음, S 함수 안의 제약은

882
00:33:06,440 --> 00:33:07,940
그게 단지 이동 시간만을 의미하나요?

883
00:33:08,120 --> 00:33:09,220
아니면 다른 제약도 있나요?

884
00:33:09,220 --> 00:33:11,640
아니요, S가 제약 함수입니다.

885
00:33:11,820 --> 00:33:12,960
충돌까지의 거리입니다.

886
00:33:13,200 --> 00:33:13,680
예.

887
00:33:14,160 --> 00:33:15,240
그래서 이동 시간이 아닙니다.

888
00:33:15,360 --> 00:33:17,360
이동 시간은 미지 함수이고,

889
00:33:17,600 --> 00:33:19,200
이 제약 함수가 주어졌을 때 정의되는 것입니다.

890
00:33:19,340 --> 00:33:21,680
그래서 그 PDE를 풀어서

891
00:33:21,680 --> 00:33:22,200
이 이동 시간을 구했습니다.

892
00:33:23,680 --> 00:33:25,440
또, 모델 아키텍처는

893
00:33:25,740 --> 00:33:28,080
그러니까, 예를 들어,

894
00:33:28,200 --> 00:33:31,480
MPI와 여러분 논문 사이에서,

895
00:33:31,600 --> 00:33:33,060
모델 아키텍처가 얼마나 다른가요?

896
00:33:33,180 --> 00:33:34,780
저희 모델 아키텍처는 매우 다릅니다.

897
00:33:35,040 --> 00:33:37,420
저희 모델 아키텍처는

898
00:33:37,420 --> 00:33:39,760
아이코널 PDE의 몇 가지 성질을 따르도록 설계되었습니다.

899
00:33:39,780 --> 00:33:41,420
예를 들어, 한 가지 성질을 말씀드리면,

900
00:33:42,160 --> 00:33:44,160
대칭성이라는 성질이 있습니다.

901
00:33:44,360 --> 00:33:46,660
그래서 시작점에서 목표점까지의 이동 시간과

902
00:33:46,660 --> 00:33:48,400
목표점에서 시작점까지의 이동 시간은

903
00:33:48,400 --> 00:33:49,160
같아야 합니다.

904
00:33:49,540 --> 00:33:50,920
따라서 대칭성을 갖습니다.

905
00:33:51,200 --> 00:33:53,100
그리고 저희 아키텍처는

906
00:33:53,100 --> 00:33:54,480
그 대칭성을 존중하도록 설계되었습니다.

907
00:33:54,740 --> 00:33:57,620
이 가치 함수가 그 대칭성을 만족하도록 강제합니다.

908
00:33:57,840 --> 00:33:59,580
그리고 제가 말씀드린 다른 점은,

909
00:33:59,700 --> 00:34:01,660
저희 가치 함수가 측지 거리라는 것입니다.

910
00:34:02,020 --> 00:34:03,660
그래서 아키텍처도

911
00:34:03,660 --> 00:34:05,520
그런 삼각, 어,

912
00:34:05,680 --> 00:34:06,960
삼각 부등식 같은 것들을 만족하도록 설계되었습니다.

913
00:34:06,960 --> 00:34:09,680
반면 MPI Net은 표준적인,

914
00:34:10,020 --> 00:34:14,140
우리가 보통 사용하는 표준 아키텍처로,

915
00:34:14,140 --> 00:34:15,300
대규모 모델에 사용합니다.

916
00:34:17,140 --> 00:34:17,720
예.

917
00:34:18,360 --> 00:34:20,420
음, 그래서 매우 좋은

918
00:34:20,420 --> 00:34:20,660
아이디어라고 생각합니다.

919
00:34:20,660 --> 00:34:23,940
매니퓰레이터 과제에 아이코널 PDE를 적용하는 것은요,

920
00:34:24,120 --> 00:34:26,440
왜냐하면 아이코널 PDE에서는

921
00:34:26,440 --> 00:34:29,680
동역학 제약이, 어,

922
00:34:29,680 --> 00:34:29,860
들어 있지 않기 때문입니다.

923
00:34:29,860 --> 00:34:33,100
따라서 모든 상태에서 아마도

924
00:34:33,100 --> 00:34:33,820
매니퓰레이터가

925
00:34:33,820 --> 00:34:35,540
어느 방향으로든 잘 움직일 수 있다고 가정하실 텐데요.

926
00:34:35,940 --> 00:34:38,760
하지만 실제로는 여전히

927
00:34:38,760 --> 00:34:41,560
동역학 제약이 있고, 또 특이점이

928
00:34:41,560 --> 00:34:42,500
매니퓰레이터에 존재합니다.

929
00:34:42,800 --> 00:34:45,160
그래서 그런 문제를 겪어보신 적이 있습니까?

930
00:34:45,160 --> 00:34:45,260
아이코널 PDE에서 말입니다.

931
00:34:45,380 --> 00:34:46,780
아주 좋은 질문입니다.

932
00:34:46,940 --> 00:34:49,400
현재로서는, 어, 이 연구에서는

933
00:34:49,400 --> 00:34:50,540
운동학만 고려합니다.

934
00:34:50,700 --> 00:34:53,240
올해 뉴립스에 채택된 논문이

935
00:34:53,240 --> 00:34:53,500
있는데요,

936
00:34:53,500 --> 00:34:56,100
그 논문에서 운동역학 문제를

937
00:34:56,100 --> 00:34:57,320
이 아이코널 PDE로 다뤘습니다.

938
00:34:57,320 --> 00:35:01,080
그리고, 어, 저희는 기본적으로 이 아이코널

939
00:35:01,080 --> 00:35:01,700
PDE가

940
00:35:01,700 --> 00:35:04,700
여전히 해를 정규화할 수 있다는 것을

941
00:35:04,700 --> 00:35:05,840
HJB PDE의 해에 대해 보여드렸습니다.

942
00:35:06,420 --> 00:35:08,660
그래서 이를 사전조건으로

943
00:35:08,660 --> 00:35:10,920
어떤 HJB PDE 솔버를 웜 스타트하기 위한 것으로

944
00:35:10,920 --> 00:35:13,720
생각하실 수 있고, 또 몇 가지 운동역학적 예도

945
00:35:13,720 --> 00:35:16,520
그러니까 어, 휴머노이드 과제에서,

946
00:35:16,600 --> 00:35:19,840
접촉이 많은, 어, 어,

947
00:35:20,060 --> 00:35:20,620
보행을 어떻게 하는지 보여드렸습니다.

948
00:35:20,780 --> 00:35:22,900
그리고 기억이 맞다면 조작도

949
00:35:22,900 --> 00:35:23,300
함께 보여드렸습니다.

950
00:35:23,300 --> 00:35:27,300
하지만 두 번째 포인트에서 여러분이 지적하신

951
00:35:27,300 --> 00:35:27,960
다른 부분도

952
00:35:27,960 --> 00:35:29,540
있었는데요, 그러니까 어, 조작에서는,

953
00:35:29,620 --> 00:35:31,100
조작 이야기는 곧 하겠습니다.

954
00:35:31,840 --> 00:35:32,280
예.

955
00:35:32,660 --> 00:35:32,900
예.

956
00:35:33,540 --> 00:35:34,500
그냥 궁금한데요,

957
00:35:34,660 --> 00:35:36,380
데이터 생성도 해야 하고

958
00:35:36,380 --> 00:35:36,920
학습도 해야 하지 않나요?

959
00:35:37,180 --> 00:35:39,800
어, 그러니까, 어, 채널에서,

960
00:35:40,700 --> 00:35:41,520
한 위치에서,

961
00:35:41,720 --> 00:35:42,360
우리가 완고한 것입니까?

962
00:35:43,940 --> 00:35:45,360
그래서 이 경우에는 그렇습니다.

963
00:35:45,560 --> 00:35:48,240
그래서 저희는, 어, 어, 제가

964
00:35:48,240 --> 00:35:49,040
작업 하나를 더 논의하겠습니다.

965
00:35:49,200 --> 00:35:52,040
예를 들어, 신경망을 선택적으로 재학습할 수 있습니다.

966
00:35:52,040 --> 00:35:53,280
예를 들어, 제가 어떻게 하면

967
00:35:53,280 --> 00:35:54,100
재학습해야 하는 파라미터를

968
00:35:54,100 --> 00:35:55,460
선택할 수 있는지

969
00:35:55,460 --> 00:35:57,060
환경의 일부가 바뀌었을 때입니다.

970
00:35:57,160 --> 00:35:59,160
전부를 재학습할 필요는 없습니다.

971
00:35:59,400 --> 00:35:59,860
좋습니다, 좋습니다.

972
00:36:00,020 --> 00:36:02,300
그리고 강력한 학습은, 저는...

973
00:36:02,300 --> 00:36:04,640
네트워크에

974
00:36:04,640 --> 00:36:06,760
임의의 시작점과 기본 위치를 설정할 수 있습니다.

975
00:36:06,920 --> 00:36:07,060
네, 맞습니다.

976
00:36:07,180 --> 00:36:07,580
좋습니다.

977
00:36:07,700 --> 00:36:07,880
네.

978
00:36:09,500 --> 00:36:12,100
자, 지금까지는 학습 효율성을 논의했고,

979
00:36:12,240 --> 00:36:13,520
복잡성에 대한 적응성도 논의했습니다.

980
00:36:13,700 --> 00:36:15,480
제가 이것이 매우 높은 차원까지 확장된다는 것을 보여드렸지만,

981
00:36:15,480 --> 00:36:17,100
이제 조작 문제로 어떻게 유도할 수 있는지,

982
00:36:17,100 --> 00:36:17,820
그러니까,

983
00:36:17,880 --> 00:36:19,800
조작으로도

984
00:36:19,800 --> 00:36:21,880
아주 큰 환경으로도 어떻게 유도할 수 있는지 논의하고자 합니다.

985
00:36:22,020 --> 00:36:23,340
신경망에는 이런 문제가 있습니다.

986
00:36:23,480 --> 00:36:25,120
아주 큰 환경이 있으면,

987
00:36:25,420 --> 00:36:26,420
잘 처리하지 못합니다.

988
00:36:26,580 --> 00:36:28,860
즉, 신경망은 스펙트럼 편향 문제를

989
00:36:28,860 --> 00:36:30,800
어, 겪습니다.

990
00:36:31,140 --> 00:36:34,300
그래서 먼저 이를 매니폴드

991
00:36:34,300 --> 00:36:35,580
또는 조작 문제로 확장하려면,

992
00:36:36,020 --> 00:36:38,380
바꿔야 할 것은 전문가

993
00:36:38,380 --> 00:36:39,360
속도 함수뿐입니다.

994
00:36:39,640 --> 00:36:41,120
장애물까지의 거리 대신,

995
00:36:41,520 --> 00:36:43,340
제약 매니폴드까지의 거리로 바꾸십시오.

996
00:36:43,340 --> 00:36:45,160
예를 들어 문

997
00:36:45,160 --> 00:36:45,840
여는 작업을 한다면,

998
00:36:46,000 --> 00:36:46,880
그것이 하나의 매니폴드입니다.

999
00:36:47,320 --> 00:36:49,600
그리고 만약

1000
00:36:49,600 --> 00:36:50,400
로봇 구성에서

1001
00:36:50,400 --> 00:36:51,340
그 매니폴드까지의 거리를 계산할 수 있다면,

1002
00:36:51,540 --> 00:36:54,080
그것을, 어, 학습

1003
00:36:54,080 --> 00:36:54,500
함수로 사용할 수 있습니다.

1004
00:36:55,120 --> 00:36:58,020
그리고 우리는 이것을 해결할 수 있었는데요,

1005
00:36:58,180 --> 00:36:58,640
그러니까, 어,

1006
00:36:58,900 --> 00:37:01,300
이 모델은, 그러니까, 문을 열고,

1007
00:37:01,320 --> 00:37:03,000
그다음 이 컵을

1008
00:37:03,000 --> 00:37:05,520
기울이지 않고 한 지점에서 다른 지점으로 옮기는데,

1009
00:37:05,520 --> 00:37:07,200
이 제한된 공간에서 말입니다.

1010
00:37:08,280 --> 00:37:10,060
그래서 다시, 어,

1011
00:37:10,200 --> 00:37:11,960
계산 시간은 매우 짧습니다.

1012
00:37:12,120 --> 00:37:13,400
성공률은 매우 높습니다.

1013
00:37:13,660 --> 00:37:16,020
그리고 우리는 이를 C-by-RRT와 비교했는데,

1014
00:37:16,140 --> 00:37:17,420
이는 샘플링 기반 접근법입니다.

1015
00:37:17,900 --> 00:37:19,940
CompNet X는 데이터 기반, 그러니까,

1016
00:37:20,100 --> 00:37:21,760
모방 학습 기반 접근법입니다.

1017
00:37:22,100 --> 00:37:24,640
제가 박사 과정 동안 제안한 CompNet X는

1018
00:37:24,640 --> 00:37:25,100
박사 과정에서,

1019
00:37:25,400 --> 00:37:27,700
이런, 어, 조작 문제를 풀기 위한 것이었습니다.

1020
00:37:27,820 --> 00:37:30,520
하지만 어떤 C-by-RRT 같은

1021
00:37:30,520 --> 00:37:31,460
또는 고전적 플래너로 데이터를 수집해

1022
00:37:31,460 --> 00:37:33,320
그 모델을 학습시켜야 합니다.

1023
00:37:35,060 --> 00:37:36,940
그리고 나서 이런 질문이 나옵니다,

1024
00:37:37,080 --> 00:37:38,980
어떻게 다중 모달 문제로 확장할 수 있습니까?

1025
00:37:39,140 --> 00:37:40,920
예를 들어 이 컵이 있고,

1026
00:37:40,960 --> 00:37:42,820
이것을 움직이면서 똑바로 세운 채로 유지하다가,

1027
00:37:43,000 --> 00:37:43,680
그다음에는 기울입니다.

1028
00:37:43,800 --> 00:37:44,920
이는 서로 다른 두 가지 제약입니다.

1029
00:37:45,120 --> 00:37:46,980
그리고 이 로봇은 또한 열어야 합니다,

1030
00:37:46,980 --> 00:37:47,440
캐비닛을,

1031
00:37:47,600 --> 00:37:48,240
무언가를 꺼내야 합니다.

1032
00:37:48,700 --> 00:37:50,300
그리고 조향을 한다면,

1033
00:37:50,760 --> 00:37:53,180
그러면 스푼 동작은 하나의 매니폴드이고,

1034
00:37:53,400 --> 00:37:54,820
그다음 조향은 또 다른 매니폴드입니다.

1035
00:37:55,040 --> 00:37:56,500
망치질을 하거나

1036
00:37:56,500 --> 00:37:58,080
또는 모든 도구 조작을 한다면,

1037
00:37:58,300 --> 00:38:02,480
그것들은 다중 모달, 어, 어, 제약 문제입니다.

1038
00:38:02,480 --> 00:38:05,440
그렇다면 이런 방법들을 어떻게 확장하여

1039
00:38:05,440 --> 00:38:07,440
그런 다중 모달 제약을 풀 수 있습니까?

1040
00:38:07,780 --> 00:38:11,560
그래서 그, 그 해법은, 어, 다음에 달려 있습니다,

1041
00:38:11,780 --> 00:38:14,440
고전 PDE 문헌에,

1042
00:38:14,660 --> 00:38:16,840
즉, 고전 PDE 문헌을 공부해 보면,

1043
00:38:17,080 --> 00:38:19,960
그들은 PDE 해를

1044
00:38:19,960 --> 00:38:22,260
유한한 기저 함수들의 합으로

1045
00:38:23,360 --> 00:38:23,690
표현하곤 했습니다, 아시겠습니까?

1046
00:38:24,260 --> 00:38:26,540
그래서 이런 각 기저 함수가

1047
00:38:26,540 --> 00:38:28,340
하나의 매니폴드를 나타낸다면,

1048
00:38:28,660 --> 00:38:30,820
그러면 그것들을 결합해서

1049
00:38:30,820 --> 00:38:31,760
전역 궤적을 얻을 수 있습니다.

1050
00:38:31,760 --> 00:38:33,000
그래서 우리가 하는 것이 이것입니다.

1051
00:38:33,200 --> 00:38:36,240
즉, 문제를 다음과 같이 분해합니다,

1052
00:38:36,520 --> 00:38:38,280
이는 단지 설명을 위한 것입니다.

1053
00:38:38,720 --> 00:38:40,080
이 2차원 환경을 생각해 보십시오.

1054
00:38:40,320 --> 00:38:43,000
이를 여러 조각으로 나눌 수 있습니다.

1055
00:38:43,660 --> 00:38:45,360
그리고 각 조각마다,

1056
00:38:45,440 --> 00:38:47,160
몇몇 기저 함수를 학습할 수 있습니다.

1057
00:38:48,120 --> 00:38:50,520
그다음 기저 함수들을 결합해서

1058
00:38:50,520 --> 00:38:52,860
전역 가치 함수를 얻습니다.

1059
00:38:53,040 --> 00:38:55,560
하지만 거기에는, 거기에는 어려움이 있었습니다.

1060
00:38:55,560 --> 00:38:58,420
저는 가치 함수가

1061
00:38:58,420 --> 00:38:58,880
연속적이기를 원합니다,

1062
00:38:58,900 --> 00:39:00,460
즉, 공간적으로 완전히 연결되도록 말입니다,

1063
00:39:00,520 --> 00:39:01,820
왜냐하면 궤적에서는,

1064
00:39:02,000 --> 00:39:03,560
이렇게 분해를 하면,

1065
00:39:04,160 --> 00:39:07,260
이 모든 분할 경계에서

1066
00:39:07,260 --> 00:39:07,840
특이점이 생길 수 있기 때문입니다.

1067
00:39:08,500 --> 00:39:10,940
그래서 이 전역 가치 함수를 어떻게 얻어서

1068
00:39:10,940 --> 00:39:11,260
가치 함수가

1069
00:39:11,260 --> 00:39:13,060
어느 지점에서든 어느 지점으로

1070
00:39:13,060 --> 00:39:13,780
갈 수 있게 하고,

1071
00:39:13,900 --> 00:39:15,720
이 구간에서 저 구간으로,

1072
00:39:15,860 --> 00:39:19,820
불연속이나 국소 최소에 빠지지 않고 갈 수 있겠습니까?

1073
00:39:20,280 --> 00:39:21,540
그래서 그것이 도전 과제였습니다.

1074
00:39:21,740 --> 00:39:23,620
그리고 우리는 몇 가지 아키텍처를 제안했습니다.

1075
00:39:24,100 --> 00:39:24,960
관심이 있으시면,

1076
00:39:24,980 --> 00:39:26,960
자세한 내용은 거기서 보실 수 있습니다.

1077
00:39:27,400 --> 00:39:29,520
하지만, 어, 이 아키텍처는 기본적으로,

1078
00:39:30,220 --> 00:39:32,220
문제를 부분 영역들로 분해한 다음

1079
00:39:32,220 --> 00:39:34,620
이 전역 함수를 학습합니다.

1080
00:39:34,780 --> 00:39:36,540
그래서 그것이 질문에 대한 답이 됩니다,

1081
00:39:36,640 --> 00:39:38,420
예를 들어 환경의 어떤 부분이 바뀌면,

1082
00:39:39,080 --> 00:39:40,780
모든 것을 다시 학습할 필요가 없습니다.

1083
00:39:40,920 --> 00:39:42,800
그 기저 함수만 다시 학습하면 됩니다.

1084
00:39:42,800 --> 00:39:44,180
그리고 그것은 매우 빠릅니다.

1085
00:39:46,500 --> 00:39:48,180
그래서 우리는 이를

1086
00:39:48,180 --> 00:39:49,280
매우 큰 환경까지 확장할 수 있었습니다.

1087
00:39:50,160 --> 00:39:53,960
어, 그리고, 어, 여전히 아주

1088
00:39:53,960 --> 00:39:54,720
높은 성공률을 유지합니다.

1089
00:39:54,840 --> 00:39:56,520
이 분해의 또 다른 장점은,

1090
00:39:56,880 --> 00:40:01,500
엔드투엔드 함수에 비해

1091
00:40:01,500 --> 00:40:02,120
더 적은 파라미터가 필요하다는 것입니다.

1092
00:40:02,280 --> 00:40:03,440
그래서 그렇게 확인했습니다.

1093
00:40:03,860 --> 00:40:06,200
그리고 이것은 레일 위에 있고,

1094
00:40:06,200 --> 00:40:07,620
캠퍼스 위를 지나갑니다.

1095
00:40:07,660 --> 00:40:10,520
즉, 이 로봇은 여러 개의 좁은

1096
00:40:10,520 --> 00:40:11,000
통로들을 이동하며

1097
00:40:11,000 --> 00:40:12,200
한 지점에서 다른 지점으로 이동하고,

1098
00:40:12,200 --> 00:40:15,660
이 전체 환경은, 어,

1099
00:40:16,000 --> 00:40:17,140
여러 개의 기저 함수로 분해되었습니다.

1100
00:40:19,320 --> 00:40:22,000
그리고 같은 아이디어가 여러분의

1101
00:40:22,000 --> 00:40:23,080
다중 모달 조작에도 적용됩니다.

1102
00:40:23,280 --> 00:40:25,980
다중 모달로 여러 개의 매니폴드를 갖습니다.

1103
00:40:26,200 --> 00:40:28,840
이를 기저 함수로 표현할 수 있습니다.

1104
00:40:29,060 --> 00:40:31,200
그것들을 결합해 궤적을 얻을 수 있습니다.

1105
00:40:31,280 --> 00:40:34,300
그래서 이것은 하나의 신경 모델이, 어,

1106
00:40:34,600 --> 00:40:35,160
문을 여는 작업을 수행하고,

1107
00:40:35,180 --> 00:40:37,400
그다음 이 컵을 집어 올리는데,

1108
00:40:37,400 --> 00:40:37,920
기울이지 않습니다.

1109
00:40:37,960 --> 00:40:40,580
그리고 나서 붓기 작업을 수행합니다.

1110
00:40:48,220 --> 00:40:50,640
최근에는 휴머노이드 로봇으로 옮겼습니다.

1111
00:40:50,820 --> 00:40:52,760
그래서 이 모든 작업은

1112
00:40:52,760 --> 00:40:53,220
휴머노이드가 해결하고 있습니다.

1113
00:40:53,380 --> 00:40:57,460
따라서 이런 분해는 공간에서도 이뤄집니다.

1114
00:40:57,680 --> 00:40:59,500
그래서 로봇은

1115
00:40:59,500 --> 00:40:59,860
환경 어디로든 이동할 수 있습니다.

1116
00:40:59,860 --> 00:41:02,640
그래서, 어, 이것이 평면에서의 분해입니다.

1117
00:41:02,820 --> 00:41:04,840
그다음에는 매니폴드에서의 분해가 있습니다.

1118
00:41:05,220 --> 00:41:08,160
그래서 이런 모든, 그러니까, 다중 모달 문제는

1119
00:41:08,160 --> 00:41:08,860
해결되고 있습니다.

1120
00:41:09,020 --> 00:41:11,400
그리고 이 논문은, 잘되면, 곧

1121
00:41:11,400 --> 00:41:13,660
학회에 제출할 계획입니다.

1122
00:41:13,660 --> 00:41:16,920
어, 하지만 여러분이 보시는 다중 모달 조작

1123
00:41:16,920 --> 00:41:18,080
문제는 해결할 수 있습니다.

1124
00:41:18,540 --> 00:41:21,520
요즘 모방 학습이나 데이터 기반

1125
00:41:21,520 --> 00:41:22,340
접근법에서 매우 인기가 있습니다.

1126
00:41:22,340 --> 00:41:25,020
그 모든 문제들을, 어, 저희는

1127
00:41:25,020 --> 00:41:25,600
이 모델들로 해결할 수 있습니다.

1128
00:41:26,760 --> 00:41:29,660
또 저희가 확장한 다른 영역은, 그러니까, 미지의

1129
00:41:29,660 --> 00:41:30,140
환경입니다.

1130
00:41:30,320 --> 00:41:32,620
지금은 우리가 환경을 안다고 가정하고,

1131
00:41:32,640 --> 00:41:34,040
그리고 그 환경으로부터,

1132
00:41:34,040 --> 00:41:36,440
어, 장애물까지의 거리를 계산했습니다.

1133
00:41:36,620 --> 00:41:38,100
조금 속도를 내겠습니다.

1134
00:41:38,600 --> 00:41:42,700
그래서 저희가 생각하기에

1135
00:41:42,700 --> 00:41:44,900
모션 플래닝 문제가 계산적으로 비싼 이유는

1136
00:41:44,900 --> 00:41:48,440
매핑 특징이

1137
00:41:48,440 --> 00:41:49,200
모션 플래닝에 적합하지 않기 때문입니다.

1138
00:41:49,360 --> 00:41:52,220
매핑 특징과

1139
00:41:52,220 --> 00:41:53,760
모션 플래너 사이에는 큰 간극이 있습니다.

1140
00:41:53,920 --> 00:41:55,740
예를 들어 부호 거리장이나

1141
00:41:55,740 --> 00:41:59,780
점유 지도 같은 것이 있어서

1142
00:41:59,780 --> 00:42:03,360
모션 플래닝을 위해 계산 비용이 큰 도구들을 만들게 됩니다.

1143
00:42:03,640 --> 00:42:06,400
예를 들어 그런 지도를

1144
00:42:06,400 --> 00:42:06,820
C-공간으로 변환하려면,

1145
00:42:07,000 --> 00:42:08,780
샘플링 기반 기법을 쓰거나

1146
00:42:08,780 --> 00:42:09,520
최적화를 해야 합니다.

1147
00:42:09,760 --> 00:42:12,440
그다음 C

1148
00:42:12,440 --> 00:42:12,680
-공간에서 궤적을 찾습니다.

1149
00:42:12,680 --> 00:42:15,320
그러면 더 나은 매핑을

1150
00:42:15,320 --> 00:42:18,600
이런, 어, 매우 복잡한

1151
00:42:19,020 --> 00:42:19,280
도구 없이 만들 수 있을까요?

1152
00:42:19,440 --> 00:42:20,640
답은 그렇습니다.

1153
00:42:20,840 --> 00:42:25,200
만약 지도를, 어, 이동 시간 함수로

1154
00:42:25,200 --> 00:42:27,660
학습할 수 있다면, 어떤

1155
00:42:27,660 --> 00:42:27,920
플래너도 필요 없습니다.

1156
00:42:27,980 --> 00:42:30,120
그 이동 시간의 기울기를 따라가기만 하면,

1157
00:42:30,120 --> 00:42:32,320
그것이 궤적을 제공합니다.

1158
00:42:32,600 --> 00:42:36,120
그래서 저희는 그것을 조사했습니다.

1159
00:42:36,120 --> 00:42:38,340
로봇이 환경을 탐색하면서

1160
00:42:38,340 --> 00:42:39,620
깊이 인식을 얻고,

1161
00:42:39,620 --> 00:42:43,060
제약 함수를 로컬하게 근사할 수 있습니다.

1162
00:42:43,640 --> 00:42:46,640
그래서 데이터가 들어오는 대로

1163
00:42:46,640 --> 00:42:47,520
이 모델을 학습시킬 수 있습니다.

1164
00:42:47,680 --> 00:42:49,340
그래서, 어, 여기 영상은

1165
00:42:49,340 --> 00:42:51,160
로봇이 환경을 탐색하는 모습을 보여줍니다.

1166
00:42:52,240 --> 00:42:54,280
인식 정보가 스트림으로 들어오고,

1167
00:42:54,280 --> 00:42:56,580
이 도달 시간 필드 지도를 구축하고 있습니다.

1168
00:42:59,380 --> 00:43:01,200
그리고 이 지도가 있으면,

1169
00:43:01,200 --> 00:43:02,500
어떤 모션 플래닝 도구도 필요 없습니다.

1170
00:43:02,600 --> 00:43:04,720
그냥 이 지도의 기울기를 따라가면 됩니다.

1171
00:43:04,880 --> 00:43:09,100
이 지도에는 로봇의 환경 탐색에 도움이 되는 기하학적 표현이

1172
00:43:09,100 --> 00:43:10,580
통합되어 있습니다.

1173
00:43:12,320 --> 00:43:16,800
그리고 이 논문에서는 매핑 시간이

1174
00:43:16,800 --> 00:43:20,700
표준 매핑, 예를 들어 점유

1175
00:43:20,700 --> 00:43:21,040
지도보다 두 배였습니다.

1176
00:43:21,240 --> 00:43:23,200
하지만 최신 모델에서는

1177
00:43:23,200 --> 00:43:25,620
이 시간을 약 40% 줄일 수 있었습니다.

1178
00:43:25,620 --> 00:43:29,020
그래서, 어, 매번, 즉 각 프레임마다,

1179
00:43:29,200 --> 00:43:31,680
이 경우 신경망은 학습에 2초가

1180
00:43:31,680 --> 00:43:32,300
걸립니다.

1181
00:43:32,300 --> 00:43:34,360
하지만 최신 모델은, 제 생각에,

1182
00:43:34,360 --> 00:43:36,220
로봇이 탐색하는 동안 학습하는 데

1183
00:43:36,220 --> 00:43:37,240
1초도 채 걸리지 않습니다.

1184
00:43:37,740 --> 00:43:41,920
그리고 이것은, 그러니까, 어, 어,

1185
00:43:43,080 --> 00:43:45,780
그래서 이것은 RARE 로봇이 이

1186
00:43:45,780 --> 00:43:46,260
환경을 매핑하는 모습입니다.

1187
00:43:51,460 --> 00:43:53,380
탐색하는 동안을 보시면,

1188
00:43:53,420 --> 00:43:55,460
즉석에서 모델을 학습하면서,

1189
00:43:55,480 --> 00:43:58,340
어, 그 이동 시간을 얻고 있습니다.

1190
00:43:58,340 --> 00:44:00,180
그래서 이런 지도를

1191
00:44:00,180 --> 00:44:02,980
점유 지도와 거의 같은 시간 안에

1192
00:44:02,980 --> 00:44:03,260
얻을 수 있다면,

1193
00:44:03,460 --> 00:44:05,200
그러면 모션 플래너가 전혀 필요 없습니다.

1194
00:44:05,420 --> 00:44:08,320
그냥 로봇을

1195
00:44:08,320 --> 00:44:09,140
미지의 환경 어디에든 배치할 수 있고,

1196
00:44:09,200 --> 00:44:10,900
로봇이 어떻게 움직여야 하는지

1197
00:44:10,900 --> 00:44:13,640
이러한 플러그앤플레이

1198
00:44:13,640 --> 00:44:14,160
모델로 그 환경에서 스스로 알아냅니다.

1199
00:44:16,000 --> 00:44:18,320
그리고 또 다른 장점으로는, 그러니까, 여러분이

1200
00:44:18,320 --> 00:44:19,700
이것으로 조작까지 할 수 있습니다.

1201
00:44:19,800 --> 00:44:21,940
로봇에 손안(인핸드)

1202
00:44:21,940 --> 00:44:22,320
카메라를

1203
00:44:22,340 --> 00:44:24,800
장착할 수 있고, 로봇이 탐색하면서 이러한 도달

1204
00:44:24,800 --> 00:44:26,400
시간 필드 맵을 C-공간에서 구축합니다.

1205
00:44:27,200 --> 00:44:29,720
그리고 여전히, 어, 또 여전히, 그러니까,

1206
00:44:29,800 --> 00:44:30,180
내비게이션도 합니다.

1207
00:44:30,440 --> 00:44:32,880
그래서 저희는 이를, 어, TRO

1208
00:44:32,880 --> 00:44:33,240
논문에서 보여드립니다.

1209
00:44:33,580 --> 00:44:35,580
관심이 있으시면 이것을 확인해 보십시오.

1210
00:44:38,000 --> 00:44:38,480
네.

1211
00:44:39,780 --> 00:44:42,140
그래서 이것이, 그러니까, 로봇 팔 셋업

1212
00:44:42,140 --> 00:44:42,560
구성이었습니다.

1213
00:44:42,660 --> 00:44:44,400
이것은 이 인핸드 카메라를 사용해서

1214
00:44:44,400 --> 00:44:46,840
6차원 C-공간에서 이 도달 시간 필드 맵을

1215
00:44:46,840 --> 00:44:47,800
구축하는 것이었습니다.

1216
00:44:49,900 --> 00:44:51,960
또 우리가 관심 있는 다른 분야는, 그러니까, 어떻게

1217
00:44:51,960 --> 00:44:54,220
이러한 PDE들을, 어,

1218
00:44:55,560 --> 00:44:56,620
어, 다중 에이전트 설정에서 풀 수 있는가입니다.

1219
00:44:56,900 --> 00:44:59,820
그래서 기본적으로, 예를 들어 HGBPD 대신 기본적으로

1220
00:44:59,820 --> 00:45:01,200
HGR을 풉니다.

1221
00:45:01,200 --> 00:45:04,200
여기서 Sumel Benson의 연구를 아실 수도 있습니다.

1222
00:45:04,200 --> 00:45:07,780
그리고, 어, 그의 연구와 다른 점은

1223
00:45:07,780 --> 00:45:09,280
우리가, 그러니까, 이 방법들의

1224
00:45:09,280 --> 00:45:10,480
확장성을 더 밀어붙이려는 것입니다.

1225
00:45:10,480 --> 00:45:13,920
매우 복잡한 로봇 매니퓰레이터와 매우 복잡한

1226
00:45:13,920 --> 00:45:15,420
장애물의 기하 구조까지요.

1227
00:45:16,620 --> 00:45:18,880
그리고 이것이 저희 작업 중 하나인데요,

1228
00:45:18,960 --> 00:45:22,300
어, 어, 여기에서 로봇들이 도달하려고 합니다

1229
00:45:22,300 --> 00:45:22,840
자신들의 목표에

1230
00:45:22,840 --> 00:45:24,440
저희가 적극적으로 충돌을 회피하면서요.

1231
00:45:24,720 --> 00:45:28,200
또한 이를, 어, 조립

1232
00:45:28,200 --> 00:45:30,740
라인 작업으로 확장했는데, 여러 로봇이 자신의,

1233
00:45:30,900 --> 00:45:31,180
어,

1234
00:45:32,320 --> 00:45:34,020
어, 작업을 수행하면서 충돌을 회피하도록 했습니다.

1235
00:45:34,160 --> 00:45:36,260
그래서, 어, 관심이 있으시면, 그러니까, 확인해

1236
00:45:36,260 --> 00:45:37,060
보시면 됩니다.

1237
00:45:37,060 --> 00:45:40,280
그래서 결론적으로, 물리 기반 사전지식을 사용해야 합니다,

1238
00:45:40,280 --> 00:45:42,840
그 이유는 이 세 가지 특징을 제공하기 때문입니다.

1239
00:45:42,840 --> 00:45:43,300
즉,

1240
00:45:43,480 --> 00:45:46,400
추론 효율성, 학습 효율성, 적응성입니다.

1241
00:45:46,700 --> 00:45:50,080
향후 연구 방향으로는, 우선, 우리는

1242
00:45:50,080 --> 00:45:52,060
학습 효율성을 개선하고자 합니다.

1243
00:45:52,320 --> 00:45:54,140
현재 모델은, 그러니까, 5분 정도가 걸립니다.

1244
00:45:54,160 --> 00:45:56,300
저는, 그러니까, 실시간 학습을 달성하고 싶습니다.

1245
00:45:56,540 --> 00:45:59,000
그래서, 그래서 이 신경망이

1246
00:45:59,000 --> 00:46:00,200
플러그앤플레이 모델이 되도록 하려는 것입니다.

1247
00:46:00,760 --> 00:46:01,620
그것이 제 비전입니다.

1248
00:46:01,820 --> 00:46:03,280
즉, 이런 플러그앤플레이

1249
00:46:03,280 --> 00:46:06,500
모델이 있으면, 새로운 환경으로의 전이성과 일반화는

1250
00:46:06,500 --> 00:46:08,920
문제가 되지 않습니다, 로봇이

1251
00:46:08,920 --> 00:46:10,600
그냥 그곳에 가서, 가서

1252
00:46:10,600 --> 00:46:10,860
즉석에서

1253
00:46:10,860 --> 00:46:13,700
모든 조작 모션 플래닝 문제를 수행할 수 있기 때문입니다.

1254
00:46:14,480 --> 00:46:16,860
그리고 우리가 관심 있는 다른 분야는

1255
00:46:16,860 --> 00:46:19,760
반응형인데, 이런 방법들이 매우 빠르기 때문입니다.

1256
00:46:20,220 --> 00:46:23,180
따라서 매우 다른

1257
00:46:23,180 --> 00:46:26,220
설정으로 일반화할 수 있다면, 환경을 교란하더라도,

1258
00:46:26,440 --> 00:46:28,300
아주 빠르게 회복할 수 있습니다.

1259
00:46:30,480 --> 00:46:33,400
그리고 이것은, 그러니까, 멀티모달 문제인데요,

1260
00:46:33,440 --> 00:46:36,240
로봇이 파지와 조작을 위한 계획을 함께 세우는 경우입니다.

1261
00:46:36,240 --> 00:46:38,420
그래서 이것도 우리가

1262
00:46:38,420 --> 00:46:38,860
확장하고 있는 또 다른 분야입니다.

1263
00:46:39,640 --> 00:46:42,800
이것은 제가 최근에 관심을 갖게 된

1264
00:46:42,800 --> 00:46:43,580
새로운 문제입니다.

1265
00:46:43,800 --> 00:46:48,520
이는 보조 조작(assistive manipulation)이라고 하며, 로봇이

1266
00:46:48,520 --> 00:46:51,440
여러 작업을 하기 위해 사람의 몸을 조작하는 것입니다.

1267
00:46:51,560 --> 00:46:54,900
예를 들어 보조 착의는 조작

1268
00:46:54,900 --> 00:46:55,320
문제입니다.

1269
00:46:55,320 --> 00:46:58,500
사람의 몸을 움직이기 위해서는 생체역학적 제약을

1270
00:46:58,500 --> 00:46:59,360
존중해야 합니다.

1271
00:46:59,360 --> 00:47:01,400
저는 이것이 연구하기에 매우 좋은 모션 플래닝

1272
00:47:01,400 --> 00:47:04,080
문제라고 생각하며, 우리는 이러한

1273
00:47:04,080 --> 00:47:04,440
방법들을

1274
00:47:04,440 --> 00:47:05,880
그러한 도메인으로도 확장하고 있습니다.

1275
00:47:06,100 --> 00:47:09,140
그래서 이것은 시뮬레이션에서의 침대 닦기 작업이고,

1276
00:47:09,140 --> 00:47:09,760
시뮬레이션에서 수행됩니다.

1277
00:47:10,000 --> 00:47:12,060
그러니까 로봇이 사람의 팔다리를

1278
00:47:12,060 --> 00:47:14,140
한 지점에서 다른 지점으로 옮기고, 다른 로봇이

1279
00:47:14,140 --> 00:47:15,780
몸을 닦고 있습니다.

1280
00:47:16,340 --> 00:47:18,140
이로써 발표를 마치겠습니다.

1281
00:47:18,140 --> 00:47:19,320
시간도 거의 맞췄습니다.

1282
00:47:19,580 --> 00:47:21,260
그리고 이것이 제 연구실입니다.

1283
00:47:21,440 --> 00:47:23,360
이분들이 없었다면 어떤 것도 가능하지 않았을 것입니다.

1284
00:47:23,360 --> 00:47:25,060
관심이 있으시면 저희

1285
00:47:25,060 --> 00:47:25,720
연구실 웹사이트를 확인해 보십시오.

1286
00:47:27,860 --> 00:47:29,500
이상으로 마치겠습니다.

1287
00:47:29,580 --> 00:47:29,940
감사합니다.

1288
00:47:30,220 --> 00:47:30,820
질문 있으신가요?

1289
00:47:35,760 --> 00:47:36,280
네.

1290
00:47:37,720 --> 00:47:38,880
훌륭한 발표 감사합니다.

1291
00:47:39,200 --> 00:47:41,700
한 가지 여쭙고 싶은데요, 어떻게

1292
00:47:41,700 --> 00:47:43,520
선생님 방법을, 예를 들면, 어떤 다른

1293
00:47:43,520 --> 00:47:43,920
방법들과

1294
00:47:44,740 --> 00:47:47,480
최근의 모션 플래닝 방법들, 예를 들어 기하학적

1295
00:47:47,480 --> 00:47:48,920
패턴, 또는 Drop from All

1296
00:47:48,920 --> 00:47:51,700
Except 같은 것들과 비교하실 수 있을까요, 가능하시다면, 그리고

1297
00:47:51,700 --> 00:47:53,040
그리고 결과적으로, 예를 들어, 그러니까….

1298
00:47:57,020 --> 00:48:00,240
그래서, 그래서, 개념적으로 보면, 저는

1299
00:48:00,240 --> 00:48:03,280
이런 방법들이 몇 가지 가정을 두고 수행된다고 생각합니다.

1300
00:48:03,280 --> 00:48:07,300
예를 들어 볼록한 장애물이 있다는 가정 같은 것입니다.

1301
00:48:07,580 --> 00:48:09,560
그래서 그것이 하나의 가정입니다.

1302
00:48:09,820 --> 00:48:13,060
예를 들어, 보시면, 그게

1303
00:48:13,060 --> 00:48:13,740
이름이 무엇이었습니까?

1304
00:48:14,180 --> 00:48:15,580
쿠로보(Kurobo)라는 것이 있습니다.

1305
00:48:16,020 --> 00:48:18,840
Kurobo가 어떻게 동작하는지 보면, 그들은

1306
00:48:18,840 --> 00:48:23,020
로봇 환경을 여러 개의 구로 표현한다고 가정합니다.

1307
00:48:23,380 --> 00:48:26,220
그다음 모션 플래닝 문제를 매우 빠르게 풉니다.

1308
00:48:26,560 --> 00:48:26,920
알겠습니까?

1309
00:48:27,300 --> 00:48:30,280
그리고 병렬화에 관한 그들의 테스트 작업도

1310
00:48:30,280 --> 00:48:33,260
이 병렬화에 크게 의존합니다.

1311
00:48:33,260 --> 00:48:36,060
즉, 로봇과

1312
00:48:36,060 --> 00:48:36,540
환경의 구형 표현에 의존합니다.

1313
00:48:36,840 --> 00:48:39,620
그리고 이 모션 플래닝 문제를

1314
00:48:39,620 --> 00:48:40,540
기본적으로 작업공간에서 풉니다.

1315
00:48:41,260 --> 00:48:43,120
하지만 거기에는 본질적인 문제가 있습니다.

1316
00:48:43,300 --> 00:48:45,320
예를 들어, 이 문제를

1317
00:48:45,320 --> 00:48:49,140
작업공간에서 풀면, 많은 최신 연구가

1318
00:48:49,220 --> 00:48:50,040
예를 들어 모방학습에서도

1319
00:48:50,160 --> 00:48:51,780
이 문제를 작업공간에서 풀고 있습니다.

1320
00:48:52,420 --> 00:48:54,880
하지만 이 궤적을

1321
00:48:54,880 --> 00:48:56,760
C-공간 궤적으로 매핑할 수 있다는 보장은 없습니다.

1322
00:48:56,980 --> 00:48:58,820
로봇이 국소 최솟값에 빠질 수도 있습니다.

1323
00:48:59,360 --> 00:49:01,960
종종 매우 복잡한

1324
00:49:01,960 --> 00:49:05,200
작업을 수행할 때, 작업공간 궤적에서

1325
00:49:05,200 --> 00:49:07,660
C-공간으로의 직접적인 매핑이

1326
00:49:07,660 --> 00:49:07,880
없다는 것을 보게 됩니다.

1327
00:49:08,100 --> 00:49:11,280
그리고 모션 플래닝을 정말로 깊게 공부하고

1328
00:49:11,280 --> 00:49:16,060
1990년대 초반의 문헌을 살펴보면, 그때는

1329
00:49:16,060 --> 00:49:17,540
작업공간에서 모션 플래닝을 하고 있었습니다.

1330
00:49:17,540 --> 00:49:19,320
그다음 이것이 문제라는 것을 알아냈고,

1331
00:49:19,360 --> 00:49:21,240
그 후 C-공간 플래닝으로 옮겨갔습니다.

1332
00:49:21,820 --> 00:49:24,640
안타깝게도 우리는 다시 작업공간 플래닝으로 돌아가고 있습니다.

1333
00:49:25,040 --> 00:49:27,900
그래서 최신 방법들은 매우 빠르지만,

1334
00:49:28,100 --> 00:49:31,880
이런 가정들에 의존하며, 그것은 결국

1335
00:49:31,880 --> 00:49:33,860
C-공간 궤적으로의 매핑 문제로

1336
00:49:33,860 --> 00:49:34,660
이어질 것입니다.

1337
00:49:34,660 --> 00:49:37,080
네, 저도 그렇게 보지는 않으며, 저는

1338
00:49:37,080 --> 00:49:37,760
그 부분을 잘 접해보지 못했습니다.

1339
00:49:38,800 --> 00:49:41,060
그래서 geometric fabric은 저는 잘 알지 못합니다.

1340
00:49:41,200 --> 00:49:42,640
하지만 말씀하신 다른 논문은

1341
00:49:42,800 --> 00:49:44,720
꽤 합리적인 구형 표현을 가정하는 것 아닙니까?

1342
00:49:45,460 --> 00:49:45,660
그렇지 않습니까?

1343
00:49:46,180 --> 00:49:48,500
아마 제가 다른 논문을 생각하고 있는 것일 수도 있습니다.

1344
00:49:49,040 --> 00:49:49,280
네.

1345
00:49:49,560 --> 00:49:51,080
그렇다면 저는 그것에 대해 잘 알지 못합니다.

1346
00:49:51,320 --> 00:49:55,460
제 생각에는, 기하학적 시스템에는

1347
00:49:55,460 --> 00:49:56,640
매우 흥미로울 수도 있습니다.

1348
00:49:56,880 --> 00:49:57,160
죄송합니다, 뭐라고 하셨습니까?

1349
00:49:57,160 --> 00:50:01,180
즉, 기본적으로 시스템을

1350
00:50:01,180 --> 00:50:03,560
동역학 시스템으로 모델링한 다음, 본질적으로

1351
00:50:03,560 --> 00:50:04,140
추출하여

1352
00:50:04,140 --> 00:50:05,540
흐름 벡터장을 얻으려는 것입니다.

1353
00:50:05,660 --> 00:50:05,980
네.

1354
00:50:06,080 --> 00:50:06,740
왔다 갔다 합니다.

1355
00:50:06,960 --> 00:50:08,800
그러니까 그런 의미에서는, 그것이

1356
00:50:08,800 --> 00:50:10,120
하나의 응용이 될 수도 있습니다.

1357
00:50:10,200 --> 00:50:10,680
네, 네.

1358
00:50:10,920 --> 00:50:11,300
그러면…

1359
00:50:11,300 --> 00:50:13,740
예를 들어 다른 함수를 취하려고 할 때, 그것의

1360
00:50:13,740 --> 00:50:14,880
수렴 특성은 어떻게 됩니까?

1361
00:50:15,180 --> 00:50:17,560
음, 매우 강한 관련성이 있다고 생각합니다.

1362
00:50:17,800 --> 00:50:21,240
예를 들어, 저희가 최근에

1363
00:50:21,240 --> 00:50:24,980
NeurIPS에 제출한 연구는 기본적으로 키노다이내믹 쪽으로, 즉 어떻게

1364
00:50:24,980 --> 00:50:27,400
이런 이동 시간

1365
00:50:27,400 --> 00:50:29,580
장을 동적 공간에서 만들 수 있는가에 관한 것입니다.

1366
00:50:30,360 --> 00:50:32,440
말씀하시는 연구는 제가 잘 알지 못해서

1367
00:50:32,500 --> 00:50:35,140
그 관련성에 대해서는 말씀드리기 어렵습니다.

1368
00:50:35,560 --> 00:50:38,460
하지만 일반적으로 저희는

1369
00:50:38,460 --> 00:50:38,980
벡터장으로 나아가고자 합니다.

1370
00:50:39,220 --> 00:50:41,500
다만 저희는 그것을

1371
00:50:41,500 --> 00:50:43,700
전문가 시연 없이, 전문가로부터

1372
00:50:43,700 --> 00:50:46,100
데이터를 학습하지 않고, 그 PDE들을 푸는 것만으로 이루고자 합니다.

1373
00:50:46,580 --> 00:50:47,080
알겠습니까?

1374
00:50:47,320 --> 00:50:49,220
어떤 연구를 말씀하시는지 잘 모르겠습니다.

1375
00:50:49,340 --> 00:50:50,880
공유해 주시면, 제가

1376
00:50:50,880 --> 00:50:52,820
확인한 뒤 다시 답변드리겠습니다.

1377
00:50:53,300 --> 00:50:53,520
네.

1378
00:50:53,520 --> 00:50:55,140
다른 질문 있으십니까?

1379
00:50:59,560 --> 00:51:00,760
질문이 있습니다.

1380
00:51:02,080 --> 00:51:07,060
제가 가진 한 가지 문제는

1381
00:51:07,060 --> 00:51:09,900
공이 제약에 가까우면

1382
00:51:09,900 --> 00:51:10,520
해가

1383
00:51:11,060 --> 00:51:12,060
상당히 불안정해질 수 있다는 점입니다.

1384
00:51:15,040 --> 00:51:18,060
푸는 과정에서, 그러니까

1385
00:51:18,360 --> 00:51:20,040
제약 주변의 힘이 문제입니다.

1386
00:51:20,780 --> 00:51:23,180
선생님 문제도 비슷한 문제가 있습니까?

1387
00:51:23,720 --> 00:51:24,300
네.

1388
00:51:24,500 --> 00:51:27,080
그래서 데이터를 샘플링하는 방식이 매우

1389
00:51:27,080 --> 00:51:27,560
중요합니다.

1390
00:51:28,540 --> 00:51:31,460
제 생각에는, 제 경험으로는, 저희 경험으로는

1391
00:51:31,460 --> 00:51:34,120
커리큘럼을 만드는 것이 도움이 되는데, 예를 들어

1392
00:51:34,120 --> 00:51:37,020
더 단순한 문제에서 시작해서 천천히

1393
00:51:37,020 --> 00:51:38,520
시작점과 목표점을 아주

1394
00:51:38,520 --> 00:51:40,000
멀게 설정해 가는 것이 도움이 됩니다.

1395
00:51:40,340 --> 00:51:43,080
또 다른 도움이 되는 점은

1396
00:51:43,080 --> 00:51:46,600
장애물 근처의 샘플을 더 많이 갖는 것인데, 왜냐하면

1397
00:51:46,600 --> 00:51:48,540
그런 구간에서 특징이 급격하게

1398
00:51:48,540 --> 00:51:50,480
변하고, 신경망이

1399
00:51:50,480 --> 00:51:51,240
그것을 더 잘 학습하기를 원하기 때문입니다.

1400
00:51:51,240 --> 00:51:54,700
그래서 네, 저희도

1401
00:51:54,700 --> 00:51:57,900
그런 문제를 겪고 있으며, 적응적 샘플링이 도움이 됩니다.

1402
00:51:57,900 --> 00:51:58,000
네.

1403
00:51:58,120 --> 00:51:58,720
알겠습니다.

1404
00:51:58,820 --> 00:52:02,040
하지만 학습을 성공적으로 마친 뒤에는, 그러면

1405
00:52:02,040 --> 00:52:04,160
즉, 그때도 똑같이

1406
00:52:04,760 --> 00:52:07,220
더 저렴하게, 음, 오프스토어에서

1407
00:52:07,220 --> 00:52:07,740
샘플을 뽑아 올릴 수 있다는 뜻인가요?

1408
00:52:09,560 --> 00:52:12,300
그래디언트를 따라가면, 그리고 이는

1409
00:52:12,300 --> 00:52:14,580
근사이기 때문에 로컬 최소값으로 갈 수 있습니다.

1410
00:52:14,700 --> 00:52:16,460
하지만 이 신경망을

1411
00:52:16,460 --> 00:52:19,060
가치 함수로 사용하고 어떤 MPPI에 통합하면,

1412
00:52:19,300 --> 00:52:20,220
그러면

1413
00:52:20,220 --> 00:52:23,020
그런 로컬 최소값 문제를 극복하는 데 도움이 되고

1414
00:52:23,020 --> 00:52:24,400
더 잘 작동합니다.

1415
00:52:24,560 --> 00:52:24,940
알겠습니다.

1416
00:52:25,240 --> 00:52:25,340
알겠습니다.

1417
00:52:25,580 --> 00:52:26,140
물론입니다.

1418
00:52:26,640 --> 00:52:26,760
네.

1419
00:52:26,880 --> 00:52:27,120
감사합니다.

1420
00:52:27,300 --> 00:52:29,700
그리고 또 하나는...

1421
00:52:29,700 --> 00:52:30,260
네.

1422
00:52:33,040 --> 00:52:35,320
음, 상용 시스템들이 어떤 방법을 쓰는지, 음,

1423
00:52:35,720 --> 00:52:36,440
상용...

1424
00:52:39,580 --> 00:52:41,600
아마, 음, 보통은,

1425
00:52:41,860 --> 00:52:44,280
무엇을 쓰는지는 모르겠지만,

1426
00:52:44,280 --> 00:52:44,520
보통은

1427
00:52:44,520 --> 00:52:47,460
어떤 고전적인 기법이거나, 또는,

1428
00:52:47,700 --> 00:52:50,140
고전적인 모션 플래너 같은 것이거나

1429
00:52:50,140 --> 00:52:50,500
그런 것일 것입니다.

1430
00:52:51,120 --> 00:52:52,960
저, 저는 그게

1431
00:52:52,960 --> 00:52:53,600
그럴 거라고 가정하지만, 저는...

1432
00:52:53,600 --> 00:52:54,020
저는...

1433
00:52:54,320 --> 00:52:55,020
저는...

1434
00:52:55,080 --> 00:52:55,780
저는...

1435
00:52:55,780 --> 00:52:56,300
저는...

1436
00:52:59,300 --> 00:53:00,700
저는...

1437
00:53:00,700 --> 00:53:00,780
저는...

1438
00:53:00,780 --> 00:53:00,960
저는...

