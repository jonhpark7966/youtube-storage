# Merge chunk outputs into final Markdown

You are given multiple Markdown chunks from a transcript.
Merge them into a single, well-structured document.

Context:
- Title: [NeurIPS Best Paper] 1000 Layer Networks for Self-Supervised RL — Kevin Wang et al, Princeton
- Description: From undergraduate research seminars at Princeton to winning *Best Paper award at NeurIPS 2025,* *Kevin Wang, Ishaan Javali, Michał Bortkiewicz, Tomasz Trzcinski, Benjamin Eysenbach* defied conventional wisdom by scaling reinforcement learning networks to *1,000 layers deep*—unlocking performance gains that the RL community thought impossible. We caught up with the team live at *NeurIPS* to dig into the story behind *RL1000:* why deep networks have worked in language and vision but failed in RL 
- Language: Korean

Rules:
- Output Markdown only.
- Use the same language as the chunks.
- Start with: `# [NeurIPS Best Paper] 1000 Layer Networks for Self-Supervised RL — Kevin Wang et al, Princeton`
- Then include exactly 3 summary lines as a numbered list (1. 2. 3.).
- After that, organize content into chapters:
  `## Chapter N: <Topic> (<MM:SS>-<MM:SS>)`
- Under each chapter, use bullet points with timestamp tags:
  `- [MM:SS] <Detailed note>`
- Preserve all blockquotes (key statements) from the chunks:
  > "[MM:SS] Quote" — Speaker
- Remove duplicate content from overlapping regions.
- Maintain chronological order by timestamps.
- Do not omit important information; keep notes detailed.
- Do not add sections beyond summary and chapters.

Chunk outputs:
<<<
## Introduction: Best Paper Award and NeurIPS Poster Experience

- [00:12] 진행자가 “레인 스페이스”에 오신 것을 환영하며, 현장에 오지 못한 뉴립스 시청자들을 위한 팟캐스트임을 소개함.
  - [00:17] “최대한 최적의 최고의” 팟캐스트를 지향한다고 강조함.
  - [00:17] 뉴립스 현장을 직접 못 온 분들을 위해 현장 분위기/내용을 전하겠다는 톤을 잡음.
- [00:19] 진행자가 논문 수상을 축하하며 소감을 질문함.
  - [00:22] 수상자가 “정말 흥분”되었다고 답함.
- [00:25] 수상자가 일정/상황을 공유함: 어제 포스터 발표, 오늘 구두 발표가 있다고 말함.
  - [00:27] 진행자가 “사람들에게 둘러싸이셨나요?”라고 포스터 세션 반응을 확인함.
- [00:30] 수상자가 포스터 발표 현장의 열기를 설명함.
  - [00:30] 사람이 정말 많았다고 말함.
  - [00:31] 거의 3시간 내내 “파도처럼” 사람들이 찾아와 계속 설명했다고 언급함.
- [00:36] 진행자가 베스트 페이퍼 수상 소식을 어떻게 알았는지(웹사이트/경로) 질문함.
  - [00:40] 수상자는 어느 날 일어나 이메일을 확인하다가 알게 됐다고 답함.
- [00:48] 진행자가 이메일에 “베스트 페이퍼를 수상하셨습니다”라고 왔던 것으로 확인하며, 리뷰로도 짐작했을 것 같다고 덧붙임.
  - [00:53] 수상자는 리뷰가 좋았던 건 알았지만 “좋은 평가”와 “베스트 페이퍼”는 다르다며, 실제로는 몰랐다고 말함.
- [01:03] 진행자가 진행을 정리하며, 이제 팀원들이 한 명씩 자기소개와 팀에서 맡은 역할을 말해달라고 요청함.
  - [01:10] 첫 화자가 “저는 케빈이고 프린스턴에서 학부를…”로 자기소개를 시작하나, 문장이 끝나기 전에 구간이 종료됨.

> "[00:31] 파도처럼 사람들이 찾아와서" — 수상자(발표자)  
> "[00:40] 어느 날 일어나서 이메일을 확인했는데" — 수상자(발표자)  
> "[00:59] 그냥 좋은 평가를 받은 것과 베스트 페이퍼를 받는 것은 다르니" — 수상자(발표자)

---

## Team Introductions and Princeton Research Origins

- [01:10] 케빈 소개: 프린스턴에서 학부를 다녔고 최근 졸업함.
  - [01:15] 프로젝트를 주도했고, 처음 시작한 사람이라고 밝힘.
  - [01:21] 이샨, 니콜, 벤과 함께 협업하게 되어 매우 기뻤다고 말함.
- [01:24] 인터뷰어 질문: 같은 연구 그룹이었는지, 어떻게 연결됐는지(맥락) 확인.
- [01:29] 케빈 설명: 팀원들이 모두 프린스턴 출신임.
  - [01:33] 섭외를 도와준 앨런에게 감사 인사.
  - [01:35] 프로젝트 시작 배경: IW 세미나에서 시작됨.
    - [01:39] 벤이 가르치던 독립 연구 세미나였다고 설명.
    - [01:41] 케빈에게는 머신러닝 연구의 첫 경험 중 하나였음.
    - [01:46] 그 경험을 얻은 것이 정말 소중했다고 강조.
  - [01:49] 이샨도 해당 세미나에 참여했고, 비슷한 주제를 하고 있어서 협업하게 됨.
    - [01:53] 세미나 기간 동안 많이 협업했고, 그 결과 프로젝트에서 흥미로운 결과가 나왔다고 언급.
  - [01:59] 이후 흐름: 비슷한 일을 하던 사람들도 프로젝트에 합류해 좋은 협업으로 이어졌다고 정리.
- [02:06] 인터뷰어 질문: 이 주제를 선택하게 된 다른 배경이 있는지, 다른 팀원이 덧붙일 내용이 있는지 요청.
- [02:15] 다른 발화자(연구실/지도자 관점) 설명: 연구실은 딥 강화학습을 하지만, 역사적으로 ‘딥’은 2~4개 층 정도를 의미해 왔다고 언급.
  - [02:21] 케빈과 이샨이 “정말 깊은 네트워크를 해보자”고 했을 때 성공 가능성에 회의적이었음을 솔직히 밝힘.
    - [02:29] 본인도 이전에 시도해 봤지만 잘 안 됐고, 기존 논문들도 시도했으나 잘 될 리 없었다고 봤다고 말함.
    - [02:33] 시작 시점의 선입견이 “매우 회의적”이었다고 재강조.
- [02:41] 인터뷰어 질문(멘토링/의사결정): 본인의 역할이 “심사”인지, “잘 안 될 것 같으니 다른 아이디어를 시도하라”는 조언인지, 혹은 “멍청해 보여도 계속 격려”하는 쪽인지 묻고, 결국 베팅을 고르는 문제라고 프레이밍함.
- [02:53] 발화자 답: “기꺼이 해볼 만한 베팅”이었다고 판단했다고 말함.
  - [02:55] 왜 베팅했는지: 비용이 비교적 낮아 보였다고 설명.
  - [03:00] 근거(인프라): 미할이 지난 1년 동안 인프라를 개발해, 이런 실험을 훨씬 쉽게 돌릴 수 있게 해줬다고 언급.
  - [03:08] 근거(신념/전례): 깊은 신경망이 더 잘해야 한다는 믿음이 있었고, 지난 10년 딥러닝 혁명이 해온 일이 그 방향이었다고 연결함.
  - [03:14] 문제의식: “왜 우리는 더 깊게 만들기를 멈췄을까?”를 제기하며, 강화학습이 유독 계속 얕은 네트워크를 써 왔다고 지적.
    - [03:22] 특히 자신들이 보던 설정에서는 더 그랬다고 말함.
    - [03:25] 그 설정의 특징: “처음부터 아무것도 없는 상태에서 시작”하는 상황이라고 설명.
- [03:27] 인터뷰 진행 조율: 다른 관점이 있으면 다른 사람들이 끼어들어 말해달라고 요청하거나, 아니면 본인이 프로젝트 개요를 이어서 설명하겠다고 제안.
  - [03:34] “알겠다”로 정리하며 다음 단계로 넘어갈 준비를 함.

> "[02:21] 케빈이 그리고 이샨이 정말 깊은 네트워크를 해 보자고 했을 때 저는 잘 될지 꽤 회의적이었습니다." — (화자 미상)

> "[02:53] 이건 제가 기꺼이 해볼 만한 베팅이었습니다" — (화자 미상)

> "[03:14] 그런데 왜 우리는 더 깊게 만들기를 멈췄을까요" — (화자 미상)

---

## The Deep Learning Anomaly: Why RL Stayed Shallow

- [03:34] 진행을 정리하며 사과하고, 다음 설명으로 넘어감.
- [03:36] 프로젝트를 바라보는 관점을 설명하겠다고 예고함.
- [03:38] 딥러닝의 “전반적인 지형”을 큰 축으로 나눠 보자고 제안함.
  - [03:41] 분야 축으로 NLP/언어, 비전, RL을 언급함.
- [03:44] 앞서 벤이 암시한 내용에 기대어, 언어/비전에서의 스케일링 흐름을 대비시킴.
  - [03:49] 언어와 비전은 “거대한 네트워크로 스케일링”하는 패러다임으로 수렴해 왔다고 말함.
  - [03:51] 규모의 예로 “수천억 개 파라미터”, “수조 개 파라미터”를 언급함.
  - [03:56] 그런 스케일링으로 딥러닝에서 많은 것을 얻었다고 강조함.
- [04:00] 반면 “딥러닝의 세 번째 가지”로서 딥 RL에서는 같은 일이 아직 일어나지 않았다고 대비함.
  - [04:05] 본인에게 이 점이 “정말 놀라웠다”고 감정을 표현함.
- [04:06] 벤의 수업/세미나에 들어갔을 때 본 강화학습 네트워크 설계가 의외였다고 회상함.
  - [04:08] 최전선 수준의 강화학습 알고리즘에서도 단순한 구조를 쓰는 것처럼 보였다고 말함.
  - [04:17] “2층 MLP 같은 것”만 쓰는 것 같아 보였다고 구체적으로 언급함.
- [04:20] 그래서 RL이 스케일하도록 하는 “레시피”를 설계/수집할 수 있을지 문제의식을 제기함.
  - [04:23] RL이 스케일하도록 하는 방법들을 “모아서” 만들겠다는 방향을 말함.
  - [04:27] 언어/비전이 스케일하는 방식과 “유사한 방식”으로 RL도 만들 수 있을지 질문함.
- [04:29] 자신들이 실제로 한 일을 소개하려고 전환함.
  - [04:32] “전통적인 RL”을 예로 들며 설명을 시작함.
  - [04:34] 가치 기반 RL을 언급하며 다음 내용으로 이어지지만, 문장이 중간에서 끊김.

> "[04:03] 아직 그런 일이 일어나지 않았습니다"  
> "[04:17] 2층 MLP 같은 것만 쓰고 계시지 않나"

---

## Self-Supervised RL: A Different Approach to Scaling

- [04:34] 가치 기반 RL(value-based RL)은 문헌에서 “스케일하지 않는다”는 점이 꽤 분명하다고 본다.
  - [04:36] 그래서 기존 가치 함수 학습 중심 접근과 다른 방향이 필요하다고 판단했다.
- [04:38] 대안으로 RL의 다른 접근을 시도했고, 이를 “자기지도 RL(self-supervised RL)”이라고 부른다.
- [04:41] 자기지도 RL에서는 가치 함수(value function)를 학습하는 대신, 상태(state)·행동(action)·미래 상태(future state)의 “표현(representation)”을 학습한다.
  - [04:47] 같은 궤적(trajectory)을 따라 나온 표현들은 서로 가깝게 밀어 넣는다(유사하게 만든다).
  - [04:49] 서로 다른 궤적의 표현들은 서로 멀어지게 밀어낸다(구분되게 만든다).
- [04:51] 이런 방식은 RL에 대한 또 다른 접근이며, “자기지도 방식”으로 학습할 수 있게 해준다.
- [04:58] 사람이 직접 만든 보상 신호(reward signal) 없이도 과제를 풀고 목표에 도달할 수 있다는 점이 핵심 동기다.
- [05:03] 자기지도 학습은 여러 영역에서 스케일된다는 것을 이미 알고 있고, 딥러닝에서도 마찬가지다.
  - [05:06] 그래서 “자기지도 RL도 비슷한 방식으로 스케일할 수 있는가?”라는 질문을 던진다.
- [05:09] 하지만 처음 시도했을 때는 잘 되지 않았다.
  - [05:12] 네트워크를 더 깊게 만들수록(깊이를 늘릴수록) 문제가 생겼다는 흐름으로 이어진다.

> "[05:06] 자기지도 RL도 비슷한 방식으로 스케일할 수 있겠습니까?" — (화자 미상)

---

## The Breakthrough Moment: Residual Connections and Critical Depth

- [05:12] 네트워크를 더 깊게 만들수록 성능이 “완전히 악화”되는 현상을 반복적으로 관찰함.
  - [05:14] 깊이 증가가 곧바로 성능 저하로 이어져, 단순 스케일링이 통하지 않는다는 신호였음.
- [05:22] 관련 문헌을 바탕으로 잔차(residual) 연결을 포함해 몇 가지 아키텍처 구성 요소를 함께 넣는 “레시피”를 구성함.
  - [05:26] 단일 기법이 아니라 여러 구성 요소의 조합을 시도하는 방향으로 전환함.
- [05:30] 특정 환경에서 실험을 돌리던 중 “갑자기” 성능이 튀는 전환점을 경험함.
  - [05:35] 깊이를 두 배로 늘렸을 때는 “사실 별 변화”가 없었음.
  - [05:39] 다시 깊이를 두 배로 늘리고 다른 구성 요소들과 함께 적용하자 성능이 “급상승”함.
- [05:44] 해당 환경에서 이 현상을 안정적으로 동작하게 만드는 것이 매우 어려웠다고 언급함.
  - [05:49] 일반적인 하이퍼파라미터 최적화는 A를 바꾸고 좋아지면 B를 바꾸는 식의 점진적 탐색임.
  - [05:57] 깊이만 키웠을 때 나빠지면 잔차 연결만 넣어보는 식의 단독 변경도 해봤지만 개선되지 않았음.
- [06:02] 결정적인 요인은 “조합”이었고, Kevin과 Sean이 그 조합을 찾아냈다고 강조함.
- [06:07] 깊이 외의 스케일링도 시도했음(다른 차원).
  - [06:09] 배치 크기를 키움.
  - [06:12] 네트워크 폭(은닉 레이어 크기)도 키움.
  - [06:15] 전반적으로는 “깊이를 순진하게 스케일링”하는 시도들과 유사한 맥락이었음.
- [06:20] 잔차 연결과 레이어 정규화 같은 특정 아키텍처 선택을 도입하자 큰 성능 점프가 나타났다고 정리함.
  - [06:27] “임계 깊이”에서 성능이 마치 곱해지듯(매우 큰 비율로) 증가하는 구간이 있었음.
  - [06:33] 그 지점에서 “의미 있는 성능 향상”이 열린다고 표현함.
- [06:36] 단순 스케일링도 어느 정도 성능 향상은 주지만, 임계 깊이에서의 급격한 도약과는 성격이 다름.
  - [06:42] 파라미터 수를 성장시키면 대략 이차적으로 증가한다고 언급하며(파라미터 증가 양상), 깊이 증가와 대비해 설명함.
  - [06:49] 어떤 의미에서는 파라미터 효율이 더 높다고 주장함.
  - [06:51] 실험에서는 샘플 효율도 더 높았다고 덧붙임.
- [06:56] 질문: “현실에서 관찰되는 현상을 아주 작은 모델에서 연구할 수 있게 재현한 것”으로 볼 수 있는지 확인 질문이 나옴.
- [07:04] 답변(덧붙임): 언어 모델/이미지 생성 모델에서는 더 크게/더 깊게 만들어 얻는 큰 성능 향상이 직관적으로 받아들여진다고 연결해 설명을 시작함.
  - [07:11] “더 크게 만들고 더 깊게”가 성능 향상으로 이어지는 패턴을 다른 도메인에서 이미 봐왔다는 맥락을 제시함.

> "[05:12] 저희가 네트워크를 더 깊게 만들수록 성능이 완전히 악화됐는데" — (발화자)

> "[05:39] 깊이를 또 두 배로 늘리니 이런 다른 구성 요소들과 함께하니까 갑자기 성능이 급상승했고" — (발화자)

> "[06:27] 이런 임계 깊이에서 성능이 정말 매우 큰 비율로 곱해지듯 늘었고" — (발화자)

---

## Architectural Choices: Borrowing from ResNets and Avoiding Vanishing Gradients

- [07:15] 연구가 기초 연구(예: 잔차 네트워크)에서 영감을 받는 이유를 설명함.
  - [07:18] 특히 잔차(Residual) 연결을 사용하는 핵심 동기가 “소실 그래디언트(vanishing gradients)”를 막기 위함이라고 강조함.
- [07:24] 잔차 연결의 효과를 논문 내 어블레이션(ablation) 결과로 보여준다고 언급함.
  - [07:27] 해당 어블레이션은 논문 아래쪽, 아마 부록(appendix)에 있을 것이라고 안내함.
  - [07:30] 잔차 연결 “없이” 수행한 실험을 포함해 비교했다는 점을 구체적으로 말함.
- [07:32] 다른 분야(이미 존재하던 개념)에서 검증된 아이디어를 가져와 RL 설정에 적용했고, 실제로 “동작한다”는 것을 보였다는 메시지를 전달함.
- [07:39] “Ben”이 곧 자리를 떠야 해서 마지막 발언을 Ben에게 넘기겠다고 전환함.
- [07:45] 이 작업으로부터 떠오르는 후속 연구/추가 작업이 무엇인지, 다음으로 어떤 방향을 밀고 가고 싶은지 질문을 던지며 마무리로 이어짐.

> "[07:20] 잔차 연결을 사용해 소실 그래디언트를 막고" — 발표자

> "[07:30] 이 잔차 연결 없이 한 실험을" — 발표자

> "[07:37] RL이라는 이 설정에 가져와서, 동작한다는 것을 보인 것입니다" — 발표자

---

## Clarifying the Paper: Not Just Big Networks, But Different Objectives

- [07:49] 다음 주제로 넘어가기 전에, 논문에 대해 한 가지를 먼저 정리하고 싶다고 말함.
  - [07:53] 논문 제목만 보고 “큰 네트워크를 쓰면 강화학습이 해결되는구나”라고 오해할 수 있는 지점을 짚음.
- [07:55] 흔한 오해(발표자가 예상하는 독자 반응)를 구체적으로 예시로 설명함.
  - [07:56] “와, 큰 네트워크라니”처럼 ‘규모’ 자체에만 주목하는 반응.
  - [08:01] “큰 네트워크를 가져다가 PPO에 붙이고, SAC에 붙이고, 좋아하는 강화학습 알고리즘에 붙이면 되지”라는 식의 적용 아이디어.
- [08:05] 위 방식이 논문의 핵심 결론은 아니라고 강조함.
  - [08:07] 핵심은 “큰 네트워크를 쓰려면” 단순히 크기만 키우는 게 아니라 추가적인 조건이 필요하다는 점이라고 정리함.
- [08:09] 큰 네트워크를 가능하게 하려면 ‘구조적 기법들’이 필요하다고 언급함.
  - [08:11] 단지 네트워크만 키우는 게 아니라, 특정한 구조적 테크닉이 전제되어야 한다는 뉘앙스.
- [08:13] (케빈이 앞서 말했듯이) 더 중요한 요소로 “다른 목표 함수”를 사용해야 한다고 강조함.
  - [08:16] 이 목표 함수는 실제로 “보상(reward)”을 포함하지 않는다고 명확히 말함.
- [08:22] 제목에 있는 ‘강화학습’이라는 단어 자체도 오해를 부를 수 있다고 지적함.
  - [08:25] 자신들은 보상을 직접 최대화하려는 것이 아니라고 설명함.
- [08:28] 구현 관점에서 “보상을 최대화하라”는 코드가 아예 없다고 단언함.
  - [08:31] 코드 한 줄도 없다는 표현으로, 접근법이 전통적 RL 최적화와 다름을 강하게 강조함.
- [08:35] 그래서 이것이 강화학습 방법이 “맞는지 잘 모르겠다”고까지 말함.
  - [08:39] 오히려 다른 분야의 “자기지도(self-supervised)” 방법들과 더 비슷해 보인다고 비교함.
- [08:42] 이 방법/연구가 서 있는 위치를 “흥미로운 교차점”으로 표현하며, 분야 경계에 걸친 성격을 예고함.
  - [08:45] 강화학습과 자기지도 학습 사이의 접점(교차점)이라는 framing을 제시하려는 흐름.

> "[08:25] 보상을 직접 최대화하려는 것이 아닙니다." — 발표자

> "[08:30] ‘보상을 최대화하라’라는 코드 한 줄도 없습니다." — 발표자

> "[08:35] 이것이 강화학습 방법이 맞는지는 잘 모르겠습니다." — 발표자

---

## Blurring the Lines: RL Meets Self-Supervised Learning

- [08:45] 강화학습(RL)과 자기지도학습(자기지도/비지도 계열) 연구가 만나는 “흥미로운 교차점”을 강조함.
  - [08:52] 포스터 왼쪽 아래에 작은 그림을 넣었고, 이는 어떤 슬라이드의 스크린샷이라고 설명함.
  - [08:58] 그 스크린샷이 얀 르쿤(Yann LeCun)의 슬라이드이며, “지능적인 시스템을 어떻게 만들고 그것이 어떻게 이뤄지는가”를 다루고, 그 과정이 비지도학습/지도학습/강화학습 중 무엇인지의 경계를 질문하는 내용이라고 언급함.
- [09:06] 논문이 시사하는 핵심으로 “세 분야(RL/지도/비지도) 사이의 경계가 매우 흐릿하다”는 점을 제시함.
  - [09:10] 지능적인 시스템을 만드는 데 있어 핵심은 특정 패러다임 하나가 아니라, 세 분야 모두에서 얻은 통찰을 “활용하는 것”이라고 정리함.
- [09:16] 인터뷰 종료 맥락에서 감사 인사와 시간 제약(곧 가야 함)을 확인하며 마무리로 넘어감.
- [09:24] 진행자가 “경계를 흐리게 한다”는 통찰이 흥미롭다고 반응함.
- [09:27] 진행자가 앞서 언급된 “추상화 계층/표현 학습” 이야기를 다시 꺼내며, 그 관점이 떠올리게 하는 추가 생각이 있는지 질문함.
- [09:34] 진행자가 자기지도학습과 강화학습의 결합과 관련해,
  - [09:39] 근본적으로 발견한 것이 있는지,
  - [09:42] 혹은 사람들이 논문에서 잘 이해하지 못하는 부분이 무엇인지 묻기 시작함(질문 진행 중, 발화가 여기서 끊김).

> "[09:07] 정말로 시사하는 바는 그 경계가 이들 사이가 매우 흐릿하며, 어쩌면 핵심은 지능적인 시스템을 만드는 데 있어 결국 활용하는 것, 즉 세 분야 모두에서 얻은 통찰을 활용하는 것이라고 생각합니다." — (화자 미상)

---

`videos/25FsKN0f8gQ/chunks/chunk_009.md`에 chunk_009 구간을 규칙대로(한국어, 타임스탬프 포함 불릿, 핵심 문장 블록쿼트) Markdown 노트로 정리해 추가했습니다.

---

## Architecture Details: Building on Braw and SymbaFowl

- [11:05] 아키텍처 맥락을 먼저 설명: 현재 접근은 **다른 목표 함수**를 사용하며, 그로 인해 **대조 손실(contrastive loss)** 같은 요소의 영향도 함께 고려됨
  - [11:09] 목표 함수/학습 신호가 기존과 달라 아키텍처만 떼어 설명하기 어렵고, 무엇에 영향을 받았는지 “대조”가 필요하다는 전제 제시
- [11:14] 아키텍처 자체는 **이전 연구와 상당히 유사**하다고 강조
  - [11:17] 참고/기반이 된 이전 논문 예시로 **BRO**, **SIMBA v1/v2** 등을 언급
- [11:24] 기존 아키텍처를 **약간 수정**해 사용했으나, 완전히 새로 만든 것은 아님
  - [11:32] “바퀴를 새로 발명”한 것이 아니라는 표현으로, 혁신의 초점이 순수 아키텍처 신규성에만 있지 않음을 분명히 함
- [11:34] 핵심 주장: **아키텍처와 목표 함수의 결합**이 스케일을 올리고, 성능도 그 스케일을 따라가게 만든다고 봄
  - [11:39] 스케일이 “정말로 올라가게” 만들고
  - [11:43] 성능도 스케일 증가를 “따라가게” 만든다는 관점 제시
  - [11:44] 이 결합 메커니즘은 더 깊게 파고들 필요가 있다고 언급
- [11:50] 인터뷰어 질문: 어떤 도메인/산업에 적용했는지, 다양한 네트워크 유형·데이터셋에서 **특히 잘 맞는 영역**(비교적 쉽게 성과가 나는 영역)이 있는지 확인 요청
  - [11:55] “특별히 잘 맞는 것”과 “쉽게 성과가 나는 영역”을 구체적으로 묻는 형태로 적용 범위/전이 가능성에 초점
- [12:01] 답변 시작: “사실 저희 과제들을 많이 보면 특히…”로 이어지며, 이후 설명이 계속될 것으로 보이나 본 발췌는 여기서 종료됨

> "[11:32] 처음부터 바퀴를 새로 발명한 것은 아닙니다." — Speaker (미상)

---

## Robotics Applications: Goal-Conditioned RL Without Human Supervision

- [12:04] (질문) 본인 과제들을 보면 특히 로보틱스 과제들이 많다고 언급함
  - [12:07] 이런 작업/연구가 로보틱스 분야에 어떤 영향을 줄지 매우 궁금하다고 밝힘
  - [12:12] “이 연구가 로보틱스에 어떤 영향을 줄 수 있는가”를 본인의 로보틱스 이해를 바탕으로 질문하겠다고 함
- [12:16] (배경 정리) 요즘 로보틱스에는 서로 다른 몇 가지 접근법이 있다고 설명함
- [12:20] (접근 1: 모방학습) 로봇을 훈련할 때 모방 학습으로 훈련하자는 접근을 소개함
  - [12:24] 엄청난 양의 데이터를 모으려 하고
  - [12:26] 사람의 감독을 많이 투입해 데이터를 스케일업하려 하며
  - [12:29] 그렇게 모은 데이터로 모방 학습을 한다고 요약함
- [12:31] (접근 2: 목표 조건부 RL) 다른 한편으로 잠재적으로 다른 접근도 가능하다고 제안함
  - [12:36] 예시로 “목표 조건부 강화학습”을 듦
  - [12:38] 실제 로봇 에이전트를 RL 에이전트로 훈련해
  - [12:41] 의미 있는 과업을 사람의 감독 없이 전적으로 수행하게 하는 방향을 설명함
  - [12:43] 사람의 감독/시범 데이터 없이도 가능해 “훨씬 더 확장 가능”하다는 점을 강조함
- [12:47] (핵심 주장) 이런 목표 조건부 강화학습이 “대안적인 접근법”이 될 수 있다고 말함
- [12:49] (스케일링 관점 전환) 데이터를 스케일업한다는 것은 결국 수작업으로 사람의 감독을 늘리는 것인데, 그 방식은 그다지 확장 가능하지 않다고 지적함
  - [12:56] 반대로 목표 조건부 강화학습을 확장 가능하게 만들 수 있다면 “그냥 확장하면” 된다는 식으로 방향성을 제시함
- [13:01] (어떻게 확장하나) 확장 방법으로 아키텍처를 확장할 수도 있고, 다른 방식으로도 확장할 수 있다고 언급함
  - [13:02] 그 이유로 “목표에 집중하고 있기 때문”이라고 설명함
  - [13:04] 서로 다른 특정 목적(목표)들이 있다면 매우 흥미로울 수 있다고 덧붙임
  - [13:07] 이런 점이 로보틱스 같은 분야에 어떤 영향을 줄 수 있을지 예시로 연결함
- [13:09] (추가 질문 예고) 효율성에 대해 한 가지만 더 짚어보겠다며, 앞서 언급된 효율성 논의로 넘어가려 함
  - [13:14] “저는 이렇게 예상했습니다”라고 하며 다음 발언을 이어가려는 지점에서 끊김

> "[12:43] 사람의 감독도 시범 데이터도 없이 훨씬 더 확장 가능하다는 점입니다, 네." — 질문자

---

`videos/25FsKN0f8gQ/chunks/chunk_012.prompt.txt` 내용을 규칙에 맞춰 한국어 마크다운 노트로 정리해 `videos/25FsKN0f8gQ/chunks/chunk_012.md`에 생성했습니다.

---

## JAX and GPU-Accelerated Environments: The Data Infrastructure

- [15:47] RL에서는 병목이 네트워크 계산(여러 번의 forward pass)보다 데이터 수집일 수 있다는 문제의식 제기
  - [15:49] “데이터 수집이 병목일 수 있고”
  - [15:51] 네트워크에 “네 번 패스를 … 통과시키는 것”이 항상 병목은 아닐 수 있다고 설명
- [15:54] 연구에서 사용한 인프라: `JAX GCRL` 환경
  - [15:59] “JAX GCRL 환경을 사용했는데”
  - [16:00] JAX 기반의 GPU 가속 환경이라고 정의
- [16:03] GPU 가속 환경의 핵심 이점: 대규모 병렬 롤아웃/트래젝터리 수집
  - [16:05] 환경 트래젝터리를 “수천 개 동시에 병렬로 수집” 가능
  - [16:12] 기본 기능으로 내장되어 있어 설정 부담을 줄인다고 언급
  - [16:13] 대략 “약 천 개 정도의 트래젝터리(궤적)”를 여러 환경에서 동시에 수집 가능하다고 구체화
- [16:18] 결과적으로 학습에 충분한 데이터 확보를 가능하게 한다는 주장
  - [16:20] “학습이 충분히 이뤄질 만큼의 데이터를 충분히 확보”해 준다고 강조
- [16:26] 더 깊게 들어가면 전문용어/관련 개념(ZCRL 등)로 추가 설명 가능하다고 제안
  - [16:29] “전문용어로는 ZCRL 같은 것”
- [16:32] 프레임워크 친숙도 대비: PyTorch는 익숙하지만 JAX는 상대적으로 덜 익숙할 수 있다는 전제
  - [16:36] JAX가 “요즘 점점 주목”받고 있다고 언급
  - [16:39] 특히 강화학습 분야에서 그렇다고 맥락을 제한
- [16:40] 왜 RL에서 JAX가 주목받는가: 온라인 RL에서 데이터 양이 최우선 과제
  - [16:43] “가능한 한 많은 데이터를 모으는 것이 가장 중요”
  - [16:47] PyTorch 대응물도 있겠지만(대안 가능성 인정) 그래도 이 방향을 강조
- [16:52] “탐색하는 롤아웃”을 시도하는 사람들에게 팁 요청(진행자 질문)
- [16:54] 추천: 목표 조건(Goal-conditioned) 강화학습에 JAX를 권장
  - [16:58] “목표 조건 강화학습에는 올해 JAX를 추천”
  - [17:01] 멀티에이전트용 JAX 구현들도 존재한다고 언급
  - [17:04] 그 외 관련 생태계/도구들도 있다고 포괄적으로 덧붙임
- [17:05] 논문 결과로 돌아가서: 성능 향상은 데이터 규모가 충분히 커진 뒤에야 관측됨
  - [17:09] 큰 성능 향상은 “약 5천만 트랜지션을 넘길 때” 나타났다고 설명
  - [17:19] 결론적으로 “데이터가 핵심”이라는 입장 재강조
- [17:23] 다른 딥러닝 영역(특히 LLM)의 성공과의 비유로 데이터 패러다임을 설명
  - [17:29] LLM이 확장 가능했던 이유는 “인터넷 전체 규모의 데이터”를 활용하는 패러다임을 찾았기 때문이라고 주장
  - [17:40] RL에서는 전통적으로 데이터가 구하기 어려웠다고 대비
- [17:44] GPU 가속 환경이 만든 변화: 짧은 시간에 초대규모 데이터 수집 가능
  - [17:45] “수억 건의 데이터를”
  - [17:48] “몇 시간 만에 수집”할 수 있다고 설명
  - [17:50] 이 환경이 “정말 좋은 테스트베드”가 된다고 평가
  - [17:53] 네트워크 용량을 확장하고도 “비슷한 종류의 이득”을 얻는 길이 열릴 수 있다고 전망
- [17:58] 진행자 질문 시작: LLM에서의 사전학습을 “다르게” 할 것인지(혹은 그와 유사한 관점이 RL에도 있는지)로 화제가 넘어가려는 지점
  - [18:00] “대규모 언어 모델에서 사전학습을 … 다르게 … 하실 거라는 말씀이십니까”로 질문이 이어지다 중단됨

> "[17:09] 이런 큰 성능 향상이 약 5천만 트랜지션을 넘길 때 나타나는 것만 봤습니다." — 화자  
> "[17:19] 데이터가 핵심이라고 생각합니다." — 화자  
> "[17:45] 수억 건의 데이터를 단지 몇 시간 만에 수집할 수 있고" — 화자

---

## 월드 모델과 다음 상태 분류 (World Models and Next State Classification)

- [18:05] 질문: (언어 모델의) “다음 단어/토큰 예측” 패러다임과의 차이는 무엇이고, 지금 목표가 무엇인가?
  - [18:12] 전제: 다음 토큰 예측은 매우 견고한데, 이를 RL에 어떻게 “바꾸는지”가 쟁점으로 제기됨.

- [18:20] 답변: “바꾸겠다”기보다, 그 통찰을 활용해 다양한 것에 적용하고 싶다는 입장.
  - [18:25] 대화 흐름: 오히려 “반대로 가야 한다”는 문제의식에 대해 “아마도 그렇다”는 식으로 수긍하며 연구 방향으로 흥미롭다고 언급.

- [18:35] 핵심 관찰: RL의 목표는 정확히 다음 단어 예측은 아니지만, “다음 상태 예측”과 유사한 구조를 가진다.
  - [18:41] 설정: 현재 상태 + 현재 행동이 주어졌을 때,
  - [18:47] 목표: 특정 미래 상태가 “같은 궤적에서 나온 미래 상태인지” vs “다른 궤적에서 나온 상태인지”를 예측하려는 형태로 볼 수 있음.
  - [18:54] 결론: 이런 관점에서는 RL이 사실상 “암묵적인 월드 모델”을 하고 있는 셈이라고 설명.

- [18:59] 손실/목표의 평행선: 언어는 크로스 엔트로피로 다음 토큰 “분류”를 하는데, 여기서는 “이진 분류”로 다음 상태(혹은 후보 상태의 소속)를 분류하는 구조로 볼 수 있다고 비교.
  - [19:11] 제안: 이 평행선을 더 깊게 파서 “딥러닝이 확장될 수 있게 해주는 핵심”이 무엇인지 규명하고,
  - [19:21] 응용: 그 통찰을 언어/강화학습 등 여러 분야에 걸쳐 추출·적용하자는 문제의식.

- [19:31] 확인/연결: 인터뷰어도 포스터에서 (Eysenbach 교수가) 비슷한 설명을 하는 것을 들었다고 언급.
  - [19:40] 요지: 이는 “표현 학습”을 통해 의미 있는 표현을 학습하려는 것으로, 상태·행동·목표가 주어졌을 때 어떤 의미에서는 환경/세계의 모델을 학습하는 것처럼 볼 수 있음.
  - [19:52] 단, “다음 프레임 예측” 같은 저차원적 픽셀/프레임 예측은 고차원·복잡성이 커서 꼭 필요하지 않다고 언급.

- [19:56] 관점 제시: “다음 세계를 직접 예측”하기보다, 가능한 여러 “세계 후보”를 생성해 그것들을 “분류”하는 접근을 밀고 싶다고 설명.
  - [20:09] 응답: 상대도 “정확히 내가 하는 방식과 같다”고 동의.

- [20:14] 직관적 비유(포커): 상대 패를 “예측(생성)하는 것”이 아니라, 행동으로부터 가능한 패의 범위를 좁히며 “분류/추론”해 확신으로 수렴하는 과정으로 설명.
  - [20:31] 확장: 이것이 표현의 “궁극적인 관점”일 수 있다고 말하면서도,
  - [20:35] 질문: 그 자체가 하나의 “세계”라고 볼 때 너무 모호한지, 비디오 생성 등에서 말하는 더 구체적 월드 모델과는 어떻게 비교되는지 고민을 공유.

- [20:46] 비용/배포 이슈: 깊은 모델은 느리고 비싸며, 추론(배포)에서는 더 얕게 만드는 경향이 있다는 점을 짚음.
  - [20:58] 제안: “깊은 교사, 얕은 학생” 패러다임(깊이로 최전선 역량을 끌어올린 뒤 증류로 되돌려 배포)에 대한 가능성을 언급.

> "[20:58] 깊은 교사, 얕은 학생" — (화자 미상)

- [21:12] 응답: 그 아이디어가 좋고, 실제로 팀 웹사이트의 “미래 방향” 항목에도 적어둔 주제라고 확인.
  - [21:23] 목표: 더 효율적인 표준 네트워크 수준으로도 (어떤 방식으로든) 성능을 유지/재현할 수 있는지 보고 싶다고 언급.
  - [21:26] 성과: 잭슨 CRL에서 목표-조건 강화학습(goal-conditioned RL)에서 최첨단(SOTA) 성능을 큰 폭으로 달성했다고 말함.
  - [21:45] 논점: 훈련 시의 모델이 추론/배포 시의 모델과 같을 필요는 없으므로,
  - [21:53] 방향: 더 작은 모델로 증류하거나(pruning 포함) 성능을 유지할 수 있다면 흥미로운 연구라고 정리.

- [22:00] 다음 질문: 위 선택 외에도 다른 미래 방향이 무엇인지, 개인적으로 열정을 두는 분야가 무엇인지 요청.
  - [22:07] 답변: 현재 추구하는 방향은 “스티칭(stitching) 강화학습”.
  - [22:14] 목표: 더 짧은 하위 행동들로부터 일반화하고,
  - [22:18] 메커니즘: 테스트 시점에 그것들이 스티치되어 병합되도록 하는 접근을 설명.

---

## 네트워크 용량을 통한 배치 사이즈 스케일링 해금

- [22:36] (연구 방향) “끝까지 전진”시키는 것에 관심이 있다고 밝힘.
- [22:39] 논문은 **깊이(depth) 스케일링**에 초점을 맞추지만, **너비(width) 스케일링**도 성능을 높인다는 점을 관찰함.
  - [22:42] 깊이뿐 아니라 너비를 키우는 것도 실제 성능 향상으로 이어졌다고 언급.
- [22:46] 깊이를 스케일링하면 **배치 사이즈(batch size)까지 스케일링할 수 있는 능력**이 “열린다”고 설명.
  - [22:49] 깊이 스케일링과 배치 사이즈 스케일링이 함께 엮인(공선적인) 관계처럼 보인다고 함.
- [22:55] 전통적인 강화학습(특히 가치 기반 RL)에서는 **배치 사이즈를 키우는 것이 효과적이지 않은 경우가 많다**고 전제함.
  - [23:02] 딥러닝의 다른 분야(언어/비전 등)에서는 배치 사이즈 스케일링 연구가 존재한다고 연결.
- [23:04] 다른 분야의 결과를 인용하며, **배치 사이즈를 크게 하는 것이 가장 효과적인 조건**을 제시함.
  - [23:06] 조건: 증가한 배치 사이즈를 “활용할 수 있을 만큼” **충분히 큰 네트워크 용량(capacity)**이 필요하다고 설명.
  - [23:13] 이를 바탕으로 전통적 RL에서 배치 스케일링이 덜 효과적이었던 이유에 대한 가설을 제시함.
- [23:15] 가설: 전통적인 강화학습에서 계속 써온 **작은 네트워크들이 큰 배치의 이점을 포착하지 못했기 때문**일 수 있다고 주장.
  - [23:23] 자신들의 실험이 이 가설을 시험하는 역할을 한다고 설명.
  - [23:25] 전제: 이제는 **깊은 네트워크를 성공적으로 학습**할 수 있게 되었고, 그 자체가 좋은 테스트베드가 됨.
- [23:33] 결과 관찰: **네트워크 용량을 키울수록 배치 사이즈 스케일링이라는 추가 축도 함께 열린다**고 말함.
  - [23:36] 깊이/용량 증가가 배치 스케일링의 가능성을 함께 확장한다는 내러티브를 재강조.
- [23:38] 앞으로의 확장 가능성에 대한 문제의식/호기심을 제기함.
  - [23:42] “충분한 컴퓨트”가 있는 사람이 특정 환경들에서 배치를 키우고,
  - [23:47] 깊이를 가능한 한 최대로 키우고,
  - [23:50] 배치 사이즈도 동시에 키우면,
  - [23:53] 언어 분야처럼 여러 축을 동시에 스케일링하듯이 RL에서도 **새로운 스케일링 차원**을 열 수 있는지 궁금하다고 함.
  - [23:58] 그렇게 했을 때 어떤 역량이 생기고, RL 에이전트 학습의 **최전선을 어디까지 밀어붙일 수 있는지** 질문함.
- [24:05] (다음 질문으로 전환) “충분한 컴퓨트”를 말할 때 구체적으로 **어느 정도의 컴퓨트 예산**을 의미했는지 되묻는 질문이 이어짐.

> "[23:06] 배치 사이즈를 키우는 것이 가장 효과적인 경우는 충분히 큰 네트워크 용량이 있어야만, 그 증가한 배치 사이즈를 활용할 수 있을 때" — (화자 미상)

> "[23:15] 배치 스케일링이 그렇게 효과적이지 않았던 이유가 전통적인 강화학습에서는… 이런 작은 네트워크들이 그것을 포착하지 못했기 때문" — (화자 미상)

---

## Compute Requirements: State-of-the-Art on a Single GPU

- [24:09] 인터뷰어가 “어떤 자원을 쓰셨는지”를 구체적으로 묻고, 실험의 계산/자원 요구사항을 명확히 하고 싶다고 함.
- [24:13] 연구 의도 중 하나가 “접근하기 쉽게” 만드는 것이었다고 설명함.
  - [24:19] 핵심은 모든 실험이 단일 GPU에서 돌아가도록 설계했다는 점을 강조함.
- [24:20] 1,000층 네트워크조차 `80GB H100 GPU 1장`에서 실행 가능하다고 밝힘.
  - [24:24] 이 구성에서의 비용(리소스 비용)에 대한 확인이 오가며 동의함.
- [24:28] 이론적으로는 분산 학습을 구성해 연산을 더 투입하면, “최전선(state-of-the-art)”을 더 밀어붙였을 때 어떤 일이 일어날지 흥미롭다고 언급함.
  - [24:32] 분산 학습 셋업 + 더 많은 연산 투입 가능성을 열어둠.
- [24:39] 인터뷰어가 최근 `비전-언어-액션(VLA)` 모델을 뉴립스에서 적극적으로 배우는 중이라고 공유함.
  - [24:53] 특히 로보틱스에서 “표현(representation)이 어떻게 적용되는지”에 관심이 크다고 함.
  - [24:58] 문헌을 많이 읽고 많은 사람들과 대화하며 탐색 중이라고 함.
- [25:01] “제너럴 인튜이션(General Intuition)” 관련 에피소드를 공개했다고 언급됨.
  - [25:09] 회사의 역사: 원래 게임 클리핑 회사로 시작했다는 맥락을 소개함.
  - [25:13] 현재는 비전-언어-액션 모델이 있다는 설명이 이어짐.
- [25:16] 인터뷰어가 미리보기를 보고 인상적이었다고 말함.
  - [25:20] 다만 “얼마나 잘 체화된(embodied) 사용 사례로 전이될지”는 확신하지 못한다고 함.
  - [25:25] 그럼에도 꼭 체화일 필요는 없고 “화면 기반(screen-based)이면 충분”하다는 관점을 제시함.
- [25:29] 인터뷰어가 의견을 요청하며, 이를 흥미로운 연구 방향으로 평가함.
- [25:33] “액션을 출력(output)한다”는 개념이 산업계에서는 크게 인기 있지 않다고 말함.
  - [25:47] 지난 3년간은 텍스트가 지배적이었고, 툴 콜링도 결국 “구조화된 텍스트”의 한 형태라고 설명함.
  - [25:56] 액션 연구가 다음 단계로 열리려면 무엇이 필요할지 본인도 확실히 모르겠다고 솔직히 말하며, 흥미로운 사례가 있으면 공유해 달라고 요청함.
- [26:08] 액션/VLA 관련해서 “멋진 연구”들이 있다고 답변이 이어짐.
  - [26:09] 접근법 예시 1: 사전학습된 `VLM`을 활용하고, 이를 `동결(freeze)`한 뒤 적용함.
  - [26:15] 그 위에 “액션을 출력하는 전문가(expert)”를 얹는 형태를 소개함.
  - [26:19] 접근법 예시 2(시스템 관점): `계층적 계획(hierarchical planning)`을 세움.
    - [26:24] 상위 수준의 계획을 출력하는 더 큰 네트워크는 추론 시간이 오래 걸릴 수 있다고 설명함.
    - [26:30] 그래서 상위 계획은 낮은 빈도/청크 단위로 출력하고,
    - [26:36] 이후에는 두 번째 시스템이 더 빠르게 동작하는 구성(느린 플래너 + 빠른 컨트롤러)을 제시함.
  - [26:38] 이 방향으로 흥미로운 연구가 꽤 많다고 평가하며 기대감을 표함.
- [26:42] 인터뷰어가 마지막 질문으로, 포스터 세션에서의 어려운 질문/인상적 만남(유명인 포함) 경험을 물음.
- [26:48] 답변자는 아직 학회장을 많이 둘러볼 기회가 없었다고 말함.
  - [26:53] 현재 풀타임으로 일하고 있어 더 그랬다고 설명함.
  - [26:57] 세션 직전에 배지를 받았을 정도라, 그 질문에 답하기 적절하지 않을 것 같다고 함.
- [27:05] 인터뷰어가 “사람들이 뭘 묻는지/어떤 분위기인지” 같은 일반적인 관찰도 괜찮다고 재설명하려다 발화가 끊김.
  - [27:14] “사람들은…”에서 문장이 미완으로 끝남.

> "[24:19] 저희의 모든 실험이, 심지어 1,000층짜리 네트워크도 단 하나의 80GB H100 GPU 한 장에서 실행할 수 있다는 점입니다" — 발표자

---

## Closing Thoughts: Challenging Conventional Wisdom in RL Scaling

- [27:14] 전반적으로 사람들의 반응은 “눈이 확 트인다”는 쪽이 많았다고 언급함.
  - [27:21] 많은 분들이 이 논문이 인상적이라고 본 이유로, 목표 함수가 꽤 단순하면서도 우아하다는 점을 꼽음.
- [27:32] 강화학습은 “잘 스케일되지 않는다”는 통념에 대해, 완전히 뒤집었다고까진 말하진 않지만 어느 정도 도전/전환점을 만들었다고 설명함.
  - [27:38] 네트워크를 1,000층까지 밀어붙였고, 그럼에도 성능이 계속 개선되는 것을 관찰했다는 점이 핵심이라고 강조함.
- [27:44] 이 방향으로 계속 구축해 나갈 수 있다면, 매우 멋진 일이 될 수 있다고 전망함.
  - [27:52] 스케일링을 다양한 차원으로 확장할 수 있고,
  - [27:53] 강화학습 역량의 최전선을 더 밀어붙일 수 있을 것 같아 매우 궁금하다고 말함.
  - [27:57] 앞으로 어떻게 될지 지켜보겠다고 덧붙임.
- [27:58] 진행자가 방문에 감사 인사를 전하고, 논문 수상을 다시 축하함.
  - [28:01] 앞으로의 연구에도 행운을 빈다고 마무리함.
- [28:03] 발표자가 초대해줘서 감사하다고 답하며 마무리함.

> "[27:21] 많은 분들이 이 논문이 정말 눈이 확 트인다고 생각하셨는데" — 발표자  
> "[27:38] 1,000층까지 밀어붙였고, 계속 개선되는 성능을 봤다는 점입니다" — 발표자  
> "[27:58] 들러 주셔서 감사하고, 논문도 다시 축하드립니다." — 진행자  
> "[28:03] 감사합니다, 초대해 주셔서 감사합니다, 네." — 발표자
>>>
