WEBVTT

00:00:00.030 --> 00:00:01.620
만나 뵙게 되어 정말 반갑습니다. 저도 반갑습니다.

00:00:01.650 --> 00:00:03.020
샌프란시스코에서였죠. 네, 알고 있습니다.

00:00:03.030 --> 00:00:08.330
여기저기 많이 다니시는데요. 올해가 다르게 느껴지는지 궁금합니다.

00:00:08.330 --> 00:00:12.140
지난번 다보스에 오셨을 때와 비교해서요. 제미니 3가 출시됐습니다.

00:00:12.620 --> 00:00:16.219
그 소식은 들었습니다. 내부적으로는 '코드 레드'라고 불렀습니다.

00:00:16.550 --> 00:00:19.490
구글이 다시 활력을 되찾았다고 느끼십니까?

00:00:20.480 --> 00:00:24.230
글쎄요, 제가 그렇게 말할 처지는 아니지만, 우리는 매우 좋은

00:00:24.230 --> 00:00:27.320
한 해를 보냈다고 느낍니다. 그래서 정말 많은 노력과 고된 작업이 필요했습니다.

00:00:27.320 --> 00:00:32.840
우리의 기술과 모델을 다시 최첨단 수준으로 되돌리는 일이었죠.

00:00:32.840 --> 00:00:37.820
특히 제미니 3와 이미징 소프트웨어의 나노봇으로 그 일을 해냈다고 생각합니다.

00:00:37.820 --> 00:00:41.060
그리고 또, 매우 빠르게 출시하는 이 새로운 세상에

00:00:41.060 --> 00:00:44.390
정말 잘 적응했다고 생각합니다.

00:00:44.630 --> 00:00:46.970
우리가 하는 일에 일종의 스타트업 에너지를 불어넣은 셈입니다.

00:00:47.150 --> 00:00:50.300
사람들이 구글을 과소평가했거나 무엇인가를 잘못 이해했다고 보십니까?

00:00:50.330 --> 00:00:51.740
네, 어쩌면요. 확신은 없습니다.

00:00:51.770 --> 00:00:56.180
우리는 항상 최전선에 서기 위한 기반과 자원을 갖추고 있었다고 생각합니다.

00:00:56.180 --> 00:00:59.360
당연히 이 분야에서 오랜 역사를 갖고 있습니다.

00:00:59.360 --> 00:01:01.580
그리고 지난 10년을 돌아보면,

00:01:02.030 --> 00:01:05.750
구글과 딥마인드는 함께 현대 AI 산업이 의존하는

00:01:05.900 --> 00:01:07.610
대부분의 혁신을 만들어냈습니다.

00:01:07.610 --> 00:01:11.270
트랜스포머, 가장 유명한 알파고, 심층 강화학습 같은 것들이요.

00:01:11.270 --> 00:01:14.120
그리고 우리는 놀라운 제품군과

00:01:14.120 --> 00:01:18.590
수십억 사용자 기반의 서비스 접점을 갖고 있습니다. AI가 자연스럽게 스며들 곳은

00:01:18.620 --> 00:01:24.830
검색부터 이메일, 크롬까지 다양하지만, 결국 중요한 것은

00:01:24.830 --> 00:01:28.190
그 모든 것을 한데 모아 올바른 방식으로 정리하는 일이었습니다.

00:01:28.190 --> 00:01:31.040
그리고 지난 몇 년 동안 그 일을 해냈다고 생각합니다만

00:01:31.040 --> 00:01:33.380
아직도 할 일이 많습니다. 그래도 그 결실이 나타나기 시작했다고 봅니다.

00:01:33.440 --> 00:01:37.420
우리가 우위가 있다고 보신다면, 그 우위가 얼마나 크다고 생각하십니까?

00:01:37.490 --> 00:01:41.600
그 우위는 얼마나 지속됩니까? 저는 모든 것이

00:01:41.600 --> 00:01:45.710
연구에서 시작한다고 생각합니다. 그리고 특히 모델은

00:01:45.710 --> 00:01:49.670
다양한 벤치마크에서 최첨단 수준에 있어야 합니다.

00:01:49.910 --> 00:01:53.150
구글과 딥마인드를 통합했을 때 우리가 가장 먼저 집중한 것도 그것이었습니다.

00:01:53.930 --> 00:01:57.710
그리고 제미니 시리즈의 진행 상황에는 매우 만족하고 있습니다.

00:01:58.280 --> 00:02:02.510
그 부분에도 할 일이 더 많습니다. 하지만 우리는 유일하게

00:02:02.510 --> 00:02:07.010
TPU부터 하드웨어, 데이터

00:02:07.010 --> 00:02:11.330
센터, 클라우드 비즈니스, 프런티어 랩, 그리고 이 모든 놀라운

00:02:11.330 --> 00:02:14.030
제품까지 전체 스택을 가진 조직은 우리뿐이라고 생각합니다. 모두 AI에 자연스럽게 맞습니다.

00:02:14.210 --> 00:02:18.560
그래서 구조적으로, 근본 원칙부터 보자면 우리는 매우

00:02:18.560 --> 00:02:20.690
잘할 수 있어야 합니다. 그리고 사실 아직 더 많은

00:02:20.690 --> 00:02:24.320
성장 여력이 남아 있다고 봅니다.

00:02:24.320 --> 00:02:28.430
프런티어 모델을 이끄는 CEO의 하루는 어떤지 궁금합니다. 새벽 1시에서 4시 사이에

00:02:28.430 --> 00:02:31.310
주로 생각을 정리하신다고 들었습니다. 네, 맞습니다.

00:02:32.150 --> 00:02:37.730
내부에서 코드 레드가 아닌 때가 있나요? 편안하다고 느끼신 적이 있나요?

00:02:38.270 --> 00:02:39.920
아니요. 결코 편안하다고 느끼지 않습니다.

00:02:39.950 --> 00:02:43.730
물론 노력은 합니다만, '코드 레드'는 아주 특별한 상황에만 쓰려고 합니다.

00:02:43.730 --> 00:02:49.310
하지만 늘 그렇습니다. 지난 3~4년 동안은

00:02:49.490 --> 00:02:53.390
믿을 수 없을 만큼 강도 높았습니다. 주 100시간 근무에,

00:02:54.470 --> 00:02:57.530
1년에 50주를 그렇게 합니다. 그게 정상입니다.

00:02:57.800 --> 00:03:02.180
그리고 이렇게 믿을 수 없을 만큼 빠르게 움직이는 기술의 최전선에서는

00:03:02.180 --> 00:03:05.090
그렇게 할 수밖에 없습니다. 경쟁이 극도로 치열합니다.

00:03:05.900 --> 00:03:09.020
기술 분야 역사상 가장 치열한 경쟁일지도 모릅니다.

00:03:09.350 --> 00:03:15.440
그리고 판돈이 믿을 수 없을 만큼 큽니다. AGI 같은 것들까지,

00:03:15.440 --> 00:03:18.770
그것이 상업적으로도 과학적으로도 갖는 의미가 엄청나기 때문입니다.

00:03:19.310 --> 00:03:23.450
게다가 우리가 하는 일 자체의 흥분에 더해,

00:03:23.450 --> 00:03:28.430
아시다시피 제 열정은 AI로 과학적 문제를 탐구하고

00:03:28.430 --> 00:03:31.400
과학적 발견 자체를 가속하는 것입니다.

00:03:32.000 --> 00:03:34.660
이것이 제가 늘 꿈꿔 왔던 일이고, 저는 평생 AI에 매달려

00:03:34.670 --> 00:03:38.000
이 순간을 향해 왔습니다. 그래서 잠을 자기 어렵습니다. 왜냐하면

00:03:38.000 --> 00:03:41.270
할 일이 너무 많기 때문입니다. 하지만 동시에, 정말로

00:03:41.270 --> 00:03:44.120
살펴보고 더 밀어붙일 흥미로운 것들이 너무 많습니다.

00:03:44.370 --> 00:03:48.140
그러니까 과학적 진보를 이끄는 데 매우 집중하고 계시다는 건 압니다만,

00:03:48.440 --> 00:03:51.590
새로운 소재를 발견하는 것뿐 아니라, 이제 제미니가

00:03:51.590 --> 00:03:55.550
휴머노이드 로봇에 통합되는 것도 봤습니다. 물리 세계에서의

00:03:55.550 --> 00:04:00.500
물리 세계에서의 AI는 무엇이며, 그것은 어떤 모습입니까?

00:04:00.500 --> 00:04:02.540
그렇습니까? 네, 저는 지난 1년 동안 많은 시간을

00:04:02.540 --> 00:04:04.780
실제로 로보틱스를 매우 면밀히 들여다보는 데 썼습니다.

00:04:04.790 --> 00:04:10.070
저는 우리가 물리적 지능에서 일종의 돌파구가 열릴 문턱에 와 있다고 생각합니다.

00:04:10.070 --> 00:04:13.160
물리적 지능 분야에서요. 다만 저는 아직 대략 18개월에서 2년 정도는

00:04:13.160 --> 00:04:16.040
더 시간이 필요하다고 봅니다. 더 많은 연구가 필요합니다.

00:04:16.040 --> 00:04:18.890
하지만 저는 이런 기반 모델들이

00:04:18.890 --> 00:04:21.829
예를 들어 Gemini 같은 기반 모델이 앞으로의 길을 보여 준다고 생각합니다. 처음부터 저희는

00:04:21.829 --> 00:04:25.100
Gemini를 멀티모달로 만들어 물리 세계를 이해할 수 있게 했습니다.

00:04:25.100 --> 00:04:28.550
그 이유는 여러 가지였고, 그중 하나는 범용

00:04:28.550 --> 00:04:30.440
어시스턴트를 만들기 위해서였습니다. 안경에 있거나

00:04:30.440 --> 00:04:32.220
휴대폰에 탑재되어 주변 세계를 이해하는 형태일 수도 있습니다.

00:04:32.420 --> 00:04:35.120
하지만 물론 두 번째 용도는 로보틱스였습니다.

00:04:35.540 --> 00:04:38.780
그렇다면 물리 세계에서의 그 순간은 어떤 모습입니까?

00:04:39.380 --> 00:04:45.770
제 생각에는 로봇이 실제 세계에서 유용한 작업을

00:04:45.770 --> 00:04:47.870
신뢰성 있게 수행하는 것입니다. 그리고 이를 가로막는 요인이 몇 가지 있습니다.

00:04:47.870 --> 00:04:48.580
여전히 그것을 가로막고 있습니다.

00:04:48.590 --> 00:04:51.290
그중 일부는 알고리즘이 아직 충분히 성숙하지 않았다는 점입니다.

00:04:51.580 --> 00:04:57.260
조금 더 견고해야 하고, 연구실에서 얻는 것보다 더 적은 데이터로도

00:04:57.260 --> 00:05:00.350
순수 디지털 모델에서처럼 충분한 데이터를 얻지 못한 상태에서 작동해야 합니다.

00:05:00.350 --> 00:05:03.830
물론 합성 데이터를 어느 정도 만들 수는 있지만, 그런 종류의

00:05:03.830 --> 00:05:06.020
데이터를 만드는 것은 훨씬 더 어렵습니다. 그리고 여전히 몇 가지 문제가 있습니다.

00:05:06.020 --> 00:05:09.020
하드웨어 측면에서도 아직 해결되지 않은 문제들이 있는데, 특히 팔과

00:05:09.020 --> 00:05:11.210
손 같은 부분입니다. 사실 로봇공학을

00:05:11.270 --> 00:05:14.660
아주 자세히 들여다보면, 적어도 저는

00:05:14.660 --> 00:05:19.580
인간의 손에 대해 새삼 감탄하게 되었고, 진화의 설계가 얼마나 정교한지 정말 놀랍습니다. 그리고

00:05:19.580 --> 00:05:24.110
인간의 손이 가진 신뢰성, 힘, 그리고 정교함과 민첩성을

00:05:24.110 --> 00:05:27.320
맞먹게 구현하기가 매우 어렵습니다. 그래서 제 생각에는 아직도

00:05:27.320 --> 00:05:30.770
맞춰야 할 조각이 꽤 많습니다. 하지만 매우 흥미로운 일들도 있습니다.

00:05:30.770 --> 00:05:34.070
예를 들어, 저희는 방금 보스턴 다이내믹스와의 새로운 심층 협력을 발표했습니다.

00:05:34.310 --> 00:05:37.760
그들은 매우 흥미로운 로봇을 보유하고 있고, 실제로 이를

00:05:37.880 --> 00:05:41.120
자동차 제조에 적용하고 있습니다. 그리고 앞으로 1년 동안

00:05:41.120 --> 00:05:44.570
프로토타입 형태로 그것이 어떻게 진행되는지 보게 될 것이고, 그리고 1~2년 뒤에는

00:05:44.570 --> 00:05:46.940
확대 적용할 수 있는 정말 인상적인 시연을 보여줄 수도 있습니다.

00:05:47.600 --> 00:05:51.680
1년 전에는 딥시크가 서방에 재앙처럼 보였습니다.

00:05:53.030 --> 00:05:57.140
그런데 1년이 지난 지금은 조용합니다. 중국도 더 조용해 보입니다.

00:05:57.140 --> 00:05:59.540
네. 중국과의 경쟁에 대한

00:05:59.540 --> 00:06:01.490
의견이 바뀌었나요? 꼭 그렇지는 않습니다.

00:06:01.790 --> 00:06:04.070
애초에 저는 그것이 재앙적이라고 생각하지 않았습니다.

00:06:04.070 --> 00:06:05.960
서방의 반응이 과도했다고 봅니다.

00:06:05.960 --> 00:06:10.670
인상적이긴 했습니다. 그리고 그것은 중국이

00:06:11.060 --> 00:06:14.900
매우 유능하다는 것을 보여줬고, 선도 기업들, 이를테면

00:06:14.900 --> 00:06:20.540
바이트댄스 같은 회사들이 가장 역량이 뛰어나다고 말할 수 있으며, 아마도 겨우 6개월

00:06:20.540 --> 00:06:23.510
정도 뒤처져 있을 뿐, 최전선에서 1~2년 뒤처진 것은 아닙니다.

00:06:23.750 --> 00:06:27.380
그래서 저는 그들이 그런 점을 보여줬다고 생각합니다. 다만 일부 주장들은 과장되었습니다.

00:06:27.380 --> 00:06:30.800
예를 들어 사용한 연산 자원이 매우 적었다는 등의

00:06:30.800 --> 00:06:35.060
주장 말인데요, 서방의 일부 모델에 의존했고 또

00:06:35.060 --> 00:06:37.490
서방의 선도 모델들이 낸 출력으로 파인튜닝을 했기 때문입니다.

00:06:37.760 --> 00:06:41.670
그래서 완전히 근거 없는 얘기는 아니었습니다. 말보: 그리고 또 한 가지는

00:06:41.670 --> 00:06:46.770
아직 지켜봐야 하는데, 중국의 기업들이 실제로

00:06:46.770 --> 00:06:50.610
스스로 최전선을 넘어서는 혁신을 할 수 있느냐는 점입니다. 즉

00:06:50.610 --> 00:06:54.210
그들은 최전선이 있는 지점까지 따라잡는 데는 매우 능하고

00:06:54.210 --> 00:06:57.720
그 능력도 점점 커지고 있지만, 저는 그들이

00:06:57.720 --> 00:07:01.240
최전선을 넘어 혁신할 수 있다는 것을 아직 보여주지 못했다고 봅니다. AGI를 정의하는 데 도움을 주셨습니다.

00:07:01.770 --> 00:07:05.130
2030년까지 거기에 도달할 확률이 50%라고 말씀하셨습니다.

00:07:05.730 --> 00:07:08.010
그 일정이 아직도 유효합니까? 네, 그렇습니다.

00:07:08.490 --> 00:07:12.270
그리고 AGI는 여전히 유용한 목표입니까?

00:07:12.720 --> 00:07:15.240
그렇다고 생각합니다. 제

00:07:15.240 --> 00:07:19.110
일정에서는 매우 좋은 기준이고, 다른 사람들보다 조금 더 긴 편입니다만, 당신 말씀처럼

00:07:19.230 --> 00:07:21.520
그렇게 보일 수 있습니다. 하지만 제 기준은 꽤 높습니다.

00:07:21.630 --> 00:07:25.200
즉, 인간이 가진 모든

00:07:25.200 --> 00:07:28.620
인지 능력을 보여주는 시스템이라는 뜻입니다. 그리고 우리는 아직도 분명히

00:07:28.620 --> 00:07:32.490
거기서 꽤 멀리 떨어져 있다고 봅니다. 예를 들어

00:07:32.490 --> 00:07:36.360
과학적 창의성 같은 것들인데, 단지 어떤 추측을 풀거나 과학의 문제를

00:07:36.360 --> 00:07:39.960
푸는 것이 아니라, 처음부터 가설이나 문제 자체를

00:07:39.960 --> 00:07:42.750
만들어내는 것입니다. 그리고 어떤 과학자든 알듯이, 올바른

00:07:42.750 --> 00:07:46.590
질문을 찾는 것이 답을 찾는 것보다 실제로는 종종 훨씬 더 어렵습니다.

00:07:46.800 --> 00:07:50.910
그래서 이런 시스템들이 그런 역량을 갖고 있는지는 분명하지 않습니다.

00:07:51.210 --> 00:07:53.880
지금은 분명히 없다고 생각합니다. 언젠가는 갖게 되겠지만,

00:07:53.880 --> 00:07:57.010
아직 무엇이 필요한지는 분명하지 않습니다. 그리고 지속 학습 같은 것들,

00:07:57.010 --> 00:08:01.170
즉 온라인 학습처럼, 자신들이 훈련받은 범위를 넘어서는 것들이 계속되면

00:08:01.170 --> 00:08:04.530
세상에 나가서는 정적인 존재가 되는데, 그때는 즉석에서 학습해야 합니다.

00:08:04.710 --> 00:08:08.580
그래서 제 관점에서는, 꽤 여러 가지 빠져 있는 역량들이 있고 그것들이 상당히

00:08:08.580 --> 00:08:12.300
제가 AGI 시스템이라고 보려면 매우 중요합니다.

00:08:13.230 --> 00:08:16.740
구글은 주요 투자자이고, Anthropic Del Rio가 오늘 앞서 여기 있었습니다.

00:08:17.460 --> 00:08:22.650
그가 5년 안에 AI가

00:08:22.950 --> 00:08:25.770
초급 화이트칼라 일자리의 50%를 사라지게 할 것이라는 예측에 동의하십니까, 아니면 반대하십니까?

00:08:26.610 --> 00:08:31.230
제 생각에는, 제 시간표와 제 관점에서는 그보다 훨씬 더 오래 걸릴 것 같습니다.

00:08:31.440 --> 00:08:35.400
물론 올해 들어 어쩌면 그 시작을 보고 있는 것 같기도 합니다,

00:08:35.400 --> 00:08:39.840
초급 일자리나 인턴십 같은 것들에서 말입니다.

00:08:40.950 --> 00:08:45.630
하지만 저는, 이런 일관성 문제를 훨씬 더 많이 해결해야 한다고 생각합니다,

00:08:45.630 --> 00:08:48.120
현재는 그게 없습니다, 그렇죠. 저는 그것을 재킷 지능이라고 부르는데,

00:08:48.120 --> 00:08:50.730
우리는 어떤 것들은 매우 잘하지만 다른 것들은 매우 형편없습니다.

00:08:50.740 --> 00:08:54.090
현재 시스템들도 마찬가지로 어떤 것들은 그렇습니다. 그리고 오늘날처럼

00:08:54.090 --> 00:08:59.580
보조 프로그램 같은 것들이 아니라, 예를 들어 에이전트에게 전체 작업을

00:08:59.580 --> 00:09:02.340
넘기거나 위임하고 싶다면, 전반적으로 훨씬 더 많은 일관성이 필요합니다.

00:09:02.340 --> 00:09:05.940
그 작업의 95%만 잘하는 것은 소용이 없습니다.

00:09:05.940 --> 00:09:08.220
전체 작업을 잘해야 합니다.

00:09:08.910 --> 00:09:12.060
그래야 실제로 그냥 맡겨두고 잊어버리는 방식으로 사용할 수 있습니다.

00:09:12.690 --> 00:09:15.960
그래서 제 생각에는, 그런 수준의 혼란을 보게 되기 전까지는

00:09:15.960 --> 00:09:19.050
아직 해야 할 일이 꽤 많이 남아 있습니다.

00:09:19.470 --> 00:09:23.280
하지만 그런 혼란은 일어날 것입니다.

00:09:23.280 --> 00:09:26.040
결국에는요, 물론입니다. 한계적으로 AGI가 있다면 말이죠,

00:09:26.460 --> 00:09:30.060
그런 시스템이 있다면, 저는 그것이 경제 전체를 바꾼다고 생각합니다.

00:09:30.240 --> 00:09:34.530
하지만 일자리 문제를 훨씬 넘어, 잠재적으로 우리가 제대로 만든다면

00:09:34.740 --> 00:09:41.100
우리는 결핍 이후의 세계에 들어설 수 있고, 에너지원 같은 세계의 근본적인

00:09:41.100 --> 00:09:44.250
매듭들을 풀 수 있습니다, 새롭고 깨끗하며 재생 가능하고 사실상 무료인

00:09:44.670 --> 00:09:47.610
에너지원 말입니다. 예를 들어 핵융합 같은 것을 해결한다면,

00:09:47.610 --> 00:09:51.750
AI의 도움을 받아 새로운 소재까지 포함해서요, 그러면 우리는

00:09:51.750 --> 00:09:54.060
AGI 이후 5년, 10년쯤 지나면 급진적으로 풍요로운

00:09:54.060 --> 00:09:57.570
세계에 있을 것이라고 생각합니다. 그렇다면 경제는, 그리고 사회는

00:09:57.570 --> 00:10:00.990
사실, 우리가 탈희소성 세계에 도달하기 전에도, 물론 거기까지 간다면요,

00:10:00.990 --> 00:10:04.890
그 사이에 무슨 일이 벌어질지에 대한 불안이 정말 큽니다.

00:10:05.130 --> 00:10:07.200
아시다시피 저는 엄마이고, 당신도 아이가 있는 걸로 압니다.

00:10:07.230 --> 00:10:10.740
아이들에 대해 가장 두려운 점은 무엇인가요?

00:10:11.070 --> 00:10:14.760
아이들과는 무엇을 이야기하나요? 앞으로 올 일에 대해 무엇이라고 말해주나요?

00:10:14.760 --> 00:10:16.530
그러니까요, 저는 정말 많은 얘기를 들었어요, 세상에.

00:10:16.530 --> 00:10:19.860
대학 졸업생들이 정말 힘든 시간을 겪게 될 거라는 말이요.

00:10:20.100 --> 00:10:23.430
음, 그건 잘 모르겠습니다만, 보세요, 저는 그게, 그게, 그게 아마

00:10:23.450 --> 00:10:27.750
산업혁명처럼 격변의 시대가 될 거라고 생각합니다, 아마 그 열 배쯤

00:10:27.750 --> 00:10:30.990
일 텐데요, 제가 보통 말하는 것보다 열 배나 더 빠르다고 생각하면 믿기 어려울 정도입니다.

00:10:30.990 --> 00:10:33.120
제가 보통 말하는 것보다요; 규모는 열 배 더 크고

00:10:33.120 --> 00:10:35.090
속도도 열 배 더 빠를 겁니다. 산업혁명은 2년,

00:10:35.150 --> 00:10:37.830
100년이 걸렸다고 하면요, 그러면 100배인 셈입니다.

00:10:38.400 --> 00:10:41.430
아니요, 저는 이 얘길 누구에게나 하지만, 그와 함께

00:10:41.430 --> 00:10:44.550
엄청난 기회가 따라온다고 봅니다. 그리고 저는

00:10:44.550 --> 00:10:48.150
인간의 창의성과 기발함을 크게 믿습니다. 우리는

00:10:48.150 --> 00:10:50.820
마음이 매우 범용적이기 때문에 아주 적응력이 큽니다. 인간의 마음은

00:10:50.820 --> 00:10:52.830
정말 범용적이죠. 우리는 적응해왔고, 현대 세계를

00:10:52.830 --> 00:10:55.350
우리 주변에 만들어냈습니다. 아시다시피 우리의 수렵채집인 뇌가

00:10:55.350 --> 00:10:57.570
현대 문명을 만들어내는 데 성공했습니다.

00:10:57.780 --> 00:11:01.080
그래서 저는 우리가 또다시 적응할 거라고 봅니다. 다만 이건 다소 전례가 없습니다,

00:11:01.080 --> 00:11:04.200
속도 때문이죠. 보통은 이런 변화가 일어나려면 한 세대나 두 세대가

00:11:04.200 --> 00:11:08.150
걸리고, 이런 변혁의 규모도

00:11:08.160 --> 00:11:13.350
이 기술의 변혁적 힘이 엄청납니다. 하지만 저는 오늘날 아이들이,

00:11:13.350 --> 00:11:17.880
이 새로운 도구들을 놀라울 정도로 능숙하게 다루도록

00:11:17.880 --> 00:11:22.740
자연스럽게 익히도록 격려하겠습니다, 그건 거의 아이들에게

00:11:22.740 --> 00:11:26.610
초능력을 주는 것과 비슷합니다. 창작 예술 분야에서라면 아마

00:11:26.610 --> 00:11:29.820
원래 열 사람이 하던 일을 한 사람이 해낼 수도 있습니다.

00:11:29.820 --> 00:11:34.320
그리고 이는, 그러니까, 여러분이 기업가 정신이 있고, 창의적으로

00:11:34.650 --> 00:11:38.640
게임 디자인이나 영화 같은 프로젝트를 한다면, 훨씬 더 많은 일을 해내고

00:11:38.640 --> 00:11:41.110
그 산업들에 진입하는 것도

00:11:41.260 --> 00:11:44.770
과거보다 훨씬 더 쉬울 겁니다, 아시다시피요. 떠오르는 새로운 인재가요.

00:11:45.310 --> 00:11:50.590
일부 사람들은 규제가 따라잡을 시간을 주고

00:11:50.590 --> 00:11:54.490
사회가 이런 변화에 어느 정도 적응할 시간을 주기 위해 일시 중단을 주장해왔습니다.

00:11:55.000 --> 00:11:59.640
이상적인 세상이라면, 다른 모든 회사가 멈출 거라고 확신할 수 있겠죠.

00:11:59.650 --> 00:12:02.140
모든 나라가 잠시 멈춘다면요. 그렇게 하자고 주장하시겠습니까?

00:12:02.170 --> 00:12:04.240
그렇다고 생각합니다. 저는 공개적으로 제가 무엇을 바라는지 말해 왔고,

00:12:04.240 --> 00:12:08.230
어떤 일이 일어나길 바라는지요. 그것은 늘 제 꿈이었고,

00:12:08.350 --> 00:12:11.860
제가 15년 전 딥마인드를 시작했을 때 마음속에 그려 둔 로드맵이자,

00:12:11.860 --> 00:12:17.080
사실 지금으로부터 25년 전쯤 이 일을 시작했을 때의 생각은, 우리가 이 순간에 가까워질수록

00:12:17.080 --> 00:12:19.930
AGI가 도래하는 이 문턱의 순간, 이 임계의 순간에 가까워질수록요,

00:12:21.400 --> 00:12:25.590
우리가 과학적인 방식으로 서로 협력할 수도 있을 것이라고요.

00:12:25.600 --> 00:12:30.460
저는 때때로 AI를 위한 국제적 CERN에 해당하는 기관을 세우자고 이야기합니다.

00:12:30.610 --> 00:12:36.400
그곳에서 전 세계 최고의 인재들이 함께 협력해 함께 연구를 진행하는 것이지요.

00:12:36.400 --> 00:12:40.810
사회 전체가 참여하는 매우 엄격한 과학적 방식으로 마지막 단계를 밟고,

00:12:40.840 --> 00:12:44.980
기술자들뿐 아니라 철학자, 사회과학자, 경제학자 같은 분들도 참여하는 것입니다.

00:12:45.820 --> 00:12:51.130
이 기술에서 우리가 무엇을 원하는지, 그리고 이를 어떻게 활용할지 알아보기 위해서요.

00:12:51.130 --> 00:12:54.100
인류 전체에 이익이 되도록 말입니다.

00:12:54.280 --> 00:12:57.730
그리고 저는 그것이 바로 걸린 문제라고 생각합니다. 하지만 안타깝게도 이는

00:12:57.730 --> 00:13:01.690
국제적 협력이 필요합니다. 왜냐하면 한 회사나 심지어 한

00:13:01.690 --> 00:13:06.970
국가, 혹은 서방만 그렇게 하기로 결정하더라도, 전 세계가 동의하지 않으면

00:13:06.970 --> 00:13:10.090
최소한 어떤 최저 기준에라도 합의하지 않으면 의미가 없기 때문입니다.

00:13:10.300 --> 00:13:14.080
그리고 아시다시피 지금은 국제 협력이 다소 까다로운 상황입니다.

00:13:14.080 --> 00:13:19.090
그래서 우리가 AGI로 가는 마지막 단계에서 그런 종류의

00:13:19.660 --> 00:13:22.840
엄격한 과학적 접근을 하려면, 그 상황이 바뀌어야 합니다.

00:13:22.900 --> 00:13:27.400
그렇다면 AGI가 예를 들어 2030년에 온다고 했을 때, 아직 규제가 갖춰져 있지 않다면

00:13:27.400 --> 00:13:33.850
우리는 어려운 상황을 맞게 되는 건가요? 음, 그러면, 아시다시피, 저는, 저는

00:13:33.850 --> 00:13:38.170
저는 여전히 주요 주체들 가운데 충분히 많은 이들이

00:13:39.790 --> 00:13:43.000
서로 소통하고, 바라건대 적어도 몇 가지에서는 협력하리라고 낙관합니다.

00:13:43.000 --> 00:13:45.910
안전과 보안 프로토콜 같은 것들에서는요. 그런 일은 이미 많이 하고 있습니다.

00:13:45.910 --> 00:13:50.950
예를 들어 저희는 그런 것들에 대해 Anthropic과도 꽤 긴밀히 협력하고 있고,

00:13:50.950 --> 00:13:56.230
그런 협력은 필요할 것입니다. 그리고 국제적인 틀이 작동하지 않는다면, 아시다시피, 아마

00:13:56.230 --> 00:13:58.510
동료들 간 협력 같은, 보다 피어 기반의 협력이 필요할 수도 있습니다.

00:13:58.630 --> 00:14:02.230
하지만 저는, 아시다시피, 그러면 여러분과 샘이 협력하는 것에 더 가까운 모습일 겁니다.

00:14:03.130 --> 00:14:05.980
저는 아시다시피, 대체로는

00:14:05.980 --> 00:14:07.800
주요 연구소의 다른 리더들 대부분과 꽤 좋은 관계를 유지하고 있다고 생각합니다.

00:14:07.810 --> 00:14:11.290
그러니까 이해관계가 충분히 크다면, 그리고 그중 많은 부분은

00:14:11.590 --> 00:14:15.220
무엇이 걸려 있는지와 위험이 무엇인지 이해하는 것이라고 생각합니다.

00:14:15.460 --> 00:14:19.450
그리고 그 점은 앞으로 2~3년 안에 모두에게 더 분명해질 것이라고 생각합니다.

00:14:19.570 --> 00:14:22.390
그럼 기술과 다음 곡선에 대해 이야기해 보겠습니다.

00:14:22.930 --> 00:14:27.700
얀 르쿤은 트랜스포머와 LLM만으로는 우리를

00:14:27.700 --> 00:14:30.760
AGI로 데려가지 못한다고 말했습니다. 동의하십니까, 아니면 반대하십니까?

00:14:30.760 --> 00:14:34.220
그리고, 아시다시피, 그것들이 막다른 길이라면 우리는 무엇을 하고 있나요?

00:14:34.250 --> 00:14:37.120
네, 아니요, 저는 그것들이 막다른 길이라는 데에는 동의하지 않습니다.

00:14:37.120 --> 00:14:40.750
하지만 저는 그건 분명히 틀렸다고 생각합니다.

00:14:40.750 --> 00:14:43.750
그러니까 이미 정말 놀라울 정도로 유용합니다.

00:14:44.080 --> 00:14:46.990
다만 제가 말하고 싶은 것은, 이것이 경험적 질문이라는 겁니다.

00:14:46.990 --> 00:14:51.340
그것이 충분한지 여부는 과학적 질문입니다.

00:14:51.340 --> 00:14:54.760
그것들만으로요. 저는, 아시다시피, 그 가능성이 반반이라고 봅니다, 즉

00:14:54.760 --> 00:14:57.640
약간의 조정을 더해 기존 방법을 계속 스케일업하는 것만으로도 충분할지요.

00:14:57.640 --> 00:15:00.220
그럴 수도 있습니다. 그리고 그렇게 해봐야 합니다.

00:15:00.220 --> 00:15:04.600
그리고 제 생각엔 그건 유용한 작업인데, 최소한 제가 보기에는

00:15:04.600 --> 00:15:08.560
이런 LLM들은 최종 시스템의 한 구성 요소가 될 텐데, 매우 중요한 구성 요소입니다.

00:15:08.560 --> 00:15:11.860
제 머릿속에서 유일한 질문은, 그것이

00:15:12.070 --> 00:15:13.870
유일한 구성 요소인가 하는 점입니다. 맞습니다.

00:15:14.050 --> 00:15:18.040
그리고 저는 한두 번의 돌파구, 어쩌면 소수의 돌파구가 있을 수 있다고 상상할 수 있고,

00:15:18.280 --> 00:15:21.650
그러니까 다섯 개도 안 되는 것들이 여기서부터 아직 필요하다고요, 그렇지요?

00:15:21.940 --> 00:15:24.250
네. 그래서, 아시다시피, 그런 것들은

00:15:24.250 --> 00:15:27.130
세계 모델 같은 것일 수 있습니다. 제가 말해 왔고 우리가 작업하고 있는 분야입니다.

00:15:27.130 --> 00:15:29.080
사실 우리는 현재 최고의 세계 모델을 갖고 있는데,

00:15:29.080 --> 00:15:31.450
그것이 GENIE, 저희 GENIE 시스템입니다.

00:15:31.450 --> 00:15:33.670
그리고 저는 그걸 직접 연구하고 있고, 매우 중요하다고 생각합니다.

00:15:33.940 --> 00:15:39.130
하지만 지속 학습 같은 것과, 일관된 시스템을 갖추는 것도 중요합니다, 즉

00:15:39.430 --> 00:15:43.120
잘하는 것과 못하는 것이 이런 들쭉날쭉한 모서리로 남아 있지 않도록요.

00:15:43.120 --> 00:15:45.010
일반적인 시스템이라면 그런 것은 있어서는 안 됩니다.

00:15:45.250 --> 00:15:48.820
그래서 더 나은 추론, 더 장기적인 계획 수립 등, 아직 빠진 역량이 꽤 있습니다.

00:15:48.820 --> 00:15:53.950
그리고 새로운 아키텍처나 새로운 돌파구가 필요한지, 아니면

00:15:54.460 --> 00:15:57.520
같은 것을 더 하는 것으로 되는지는 미지수입니다.

00:15:57.700 --> 00:16:00.640
그리고 제 관점에서는, 구글과 딥마인드의 관점에서는,

00:16:00.640 --> 00:16:03.970
우리는 가능한 한 강하게 밀어붙이고 있습니다. 그리고 그 두 가지 모두, 즉 발명도 하고

00:16:03.970 --> 00:16:08.110
새로운 것을 만들고 기존 것을 확장하는 일 모두를 하고 있습니다. 그래서 약간 다르지만

00:16:08.110 --> 00:16:10.990
서로 연관된 일입니다. 엘리아스 에스코베도는 규모 확장의 시대, 즉

00:16:10.990 --> 00:16:15.280
더 큰 모델을 만들어 개선을 얻는 시대가 거의 끝났다고 말했습니다.

00:16:15.610 --> 00:16:17.770
이에 동의하십니까? 저는 동의하지 않습니다.

00:16:17.800 --> 00:16:20.530
제 기억에 그의 정확한 말은 이랬습니다. "우리는 연구의 시대로 돌아왔다."

00:16:20.530 --> 00:16:25.060
하지만 저는 엘리를 좋아하고 우리는 매우 친한 친구이며 많은 것에 동의합니다. 그러나 제 견해는

00:16:25.060 --> 00:16:27.430
우리는 연구의 시대를 떠난 적이 없다는 것입니다.

00:16:27.430 --> 00:16:29.290
적어도 제 관점에서는 그렇습니다.

00:16:29.470 --> 00:16:32.740
우리는 항상 투자해 왔습니다. 제 생각에는 우리는 항상

00:16:32.890 --> 00:16:38.600
가장 깊고 가장 넓은 인재층을 갖추고 있었습니다. 사실 구글과 딥마인드를 합치면,

00:16:38.840 --> 00:16:42.890
지난 10년 동안 현대 산업이 의존하는 돌파구의 약 90%를 발명했습니다.

00:16:42.890 --> 00:16:46.520
물론 트랜스포머가 가장 유명하지만,

00:16:46.880 --> 00:16:48.620
심층 강화학습도 있습니다.

00:16:48.620 --> 00:16:50.960
알파고 같은 강화학습 기법들입니다.

00:16:51.170 --> 00:16:54.200
우리가 그 모든 것을 개척했습니다. 그래서 미래에 새로운 돌파구가

00:16:54.200 --> 00:16:59.990
필요해진다면, 저는 과거처럼 앞으로도 우리가

00:16:59.990 --> 00:17:03.320
그 돌파구를 만들어낼 주체가 우리가 될 것이라고 믿습니다, 앞으로도 말입니다.

00:17:04.579 --> 00:17:08.720
마지막으로, 동의하십니까 아니면 동의하지 않으십니까? 일론은 우리가 특이점에 들어섰다고 말합니다.

00:17:09.569 --> 00:17:15.339
아니요, 저는 그것이 매우 성급하다고 생각합니다. 저는 특이점은

00:17:15.349 --> 00:17:18.020
완전한 AGI의 도래를 뜻하는 또 다른 말이라고 생각합니다.

00:17:18.230 --> 00:17:22.020
그리고 제가 앞서 설명했듯이, 우리는 아직, 그러니까,

00:17:22.130 --> 00:17:24.650
그것과는 전혀 거리가 멉니다. 하지만 저는 우리가 거기에 도달할 것이라고 생각합니다.

00:17:25.310 --> 00:17:28.460
그리고 5년, 5년조차도

00:17:28.460 --> 00:17:31.760
그것이 무엇을 의미하는지 생각해 보면 긴 시간이 아닙니다. 하지만 그전에 여전히 해야 할 일이 많다고 생각합니다.

00:17:31.760 --> 00:17:34.580
특이점처럼 보이는 어떤 것이라도 갖기 전까지는 말입니다.

00:17:34.640 --> 00:17:38.650
그렇다면 지금 구글 내부의 문화에 대해 조금 말씀해 주십시오, 그러니까,

00:17:38.650 --> 00:17:43.190
이 경쟁에서 이기되 제대로 해내기 위해서요. 리더십과 관련해서도요.

00:17:43.190 --> 00:17:45.660
현재 래리와 세르게이는 얼마나 관여하고 있습니까?

00:17:45.680 --> 00:17:48.230
그들과 얼마나 자주 이야기하시고, 그들의 우선순위는 무엇입니까?

00:17:48.620 --> 00:17:51.350
네, 두 분 다 매우 깊이 관여하고 계십니다. 그리고 래리는 좀 더

00:17:51.350 --> 00:17:53.600
전략적인 쪽에 더 집중하십니다. 이사회 회의에서 뵙기도 하고,

00:17:53.600 --> 00:17:57.590
제가 실리콘밸리에 방문할 때 다른 자리에서도 뵙습니다. 그래서 다시 말해, 더 직접적으로 참여하십니다.

00:17:57.590 --> 00:18:00.860
그러니까 제미니 팀과 관련해 코딩에도 관여하시고

00:18:00.860 --> 00:18:04.970
특히 알고리즘 세부 사항, 즉 알고리즘적 디테일에 더 집중하십니다.

00:18:05.690 --> 00:18:08.600
그리고 두 분 모두 우리가 있는 위치에 대해 열정을 갖고 계신 것이 정말 좋습니다. 그리고

00:18:08.600 --> 00:18:11.870
누가 그렇지 않겠습니까, 지금 이 순간은 정말 믿기 어려울 정도로 놀라운 순간입니다.

00:18:11.870 --> 00:18:14.330
컴퓨터 과학에 있어서요. 그래서 순수하게 과학적 관점에서만 보더라도,

00:18:14.330 --> 00:18:18.890
두 분 다 그런 분들이지만, 이는 역사상 믿기 어려울 만큼 흥미로운 순간입니다.

00:18:18.890 --> 00:18:22.700
정말로 인류 역사에서도 그렇습니다. 그래서 물론 모두가

00:18:23.300 --> 00:18:26.030
직접 손을 대고 깊이 관여하고 싶어 합니다. 그래서 그 점이 좋습니다.

00:18:26.310 --> 00:18:31.460
그리고 우리로서는, 하나의 조직으로서, 저는

00:18:31.460 --> 00:18:34.610
여러 세계의 장점을 결합하려고 합니다. 그러니까 스타트업의 에너지, 즉 빠르게 출시하고

00:18:34.610 --> 00:18:38.120
위험을 감수하며 일을 추진하는 에너지인데, 여러분께서도

00:18:38.120 --> 00:18:42.980
그 성과를 보고 계시다고 생각합니다. 그리고 대기업의 자원은 놀랍고

00:18:42.980 --> 00:18:47.510
유용하지만, 동시에 장기 연구를 위한 공간도 계속 보호해야 하고,

00:18:47.510 --> 00:18:51.860
탐색적 연구도 마찬가지입니다. 단지

00:18:51.860 --> 00:18:54.440
3개월 안에 제품에서 무엇을 내놓을지에만 맞춘 연구만 하는 것은 실수라고 생각합니다.

00:18:54.620 --> 00:18:57.680
그래서 저는 이런 여러 요인들을 함께 균형 있게 맞추려고 합니다.

00:18:57.680 --> 00:19:01.250
그리고 지난 1년 동안은 상황이 잘 흘러가고 있다고 생각하며, 또

00:19:01.250 --> 00:19:04.040
더 잘할 수도 있고 올해도 더 잘할 것이라고 생각합니다.

00:19:04.490 --> 00:19:08.810
하지만 우리의 궤적에는 매우 만족합니다. 실제로 업계에서 그 누구보다도

00:19:08.810 --> 00:19:11.300
개선과 진전의 기울기가 가장 가파르다고 생각합니다.

00:19:11.750 --> 00:19:17.000
당신은 노벨상 수상자이시며, AI가

00:19:17.000 --> 00:19:20.990
과학 연구를 추진하는 데 얼마나 집요하게 관심을 두고 계신지도 알고 있습니다. 만약 AI 자체가, 예컨대, 노벨상에

00:19:20.990 --> 00:19:22.890
걸맞은 발견을 한다면, 상은 당신께서 받으셔야 합니다.

00:19:23.030 --> 00:19:27.920
네, 저는 여전히 인간이 받는 것이 맞다고 말씀드리겠습니다. 왜냐하면

00:19:27.920 --> 00:19:31.250
완전히 스스로 했다는 것이 무엇을 의미하느냐에 달려 있기 때문입니다.

00:19:31.430 --> 00:19:37.100
그래서 현재로서는 이것들은 여전히 도구입니다. 저는 이것들을, 그러니까

00:19:37.550 --> 00:19:41.570
아마도 궁극적인 과학 도구일 수는 있지만, 망원경과

00:19:41.570 --> 00:19:43.730
현미경의 더 나은 버전 같은 것으로 봅니다. 우리는 항상 도구를 만들어서

00:19:43.730 --> 00:19:48.110
자연 세계를 더 잘 탐구해 왔습니다. 우리는 기본적으로 도구를 만드는 존재입니다.

00:19:48.110 --> 00:19:50.780
그것이 인간을 다른 동물들과 구별해 주는 점입니다.

00:19:51.050 --> 00:19:55.010
그리고 그것이 우리의 초능력이며, 물론 저는 컴퓨터도 그 안에 포함합니다.

00:19:55.220 --> 00:19:57.590
그리고 AI는 그 궁극적인 표현이라고 할 수 있습니다.

00:19:57.590 --> 00:20:00.680
어떤 면에서는 저는 AI를 늘 과학을 하기 위한 궁극의 도구라고 생각해 왔습니다.

00:20:00.680 --> 00:20:05.270
그리고 가까운 미래에도 저는 그렇게 될 것이라고 생각합니다.

00:20:05.270 --> 00:20:08.420
즉 최고의 과학자들과의 협업이 될 것이라고 생각합니다.

00:20:08.930 --> 00:20:12.980
과학자들이 창의적인 아이디어와 가설을 이 놀라운 도구들에 입력하며 협업하는 방식입니다.

00:20:12.980 --> 00:20:17.630
이 도구들은 데이터 처리와 패턴 매칭, 그리고 과학 탐구를 강화해 줍니다.

00:20:17.990 --> 00:20:20.600
이런 도구들 덕분입니다. 그리고 당신은 분명 딥마인드를 누구에게든 매각할 수도 있었을 것입니다.

00:20:20.690 --> 00:20:26.450
그리고 저는 이런 회사들이 우리에게 많은 신뢰를 요구하고 있다고 생각합니다.

00:20:26.450 --> 00:20:29.180
네.

00:20:29.420 --> 00:20:34.490
특히 규제가 기술을 따라가지 못한다면, 역사적으로 보아

00:20:34.490 --> 00:20:36.950
아마도 따라가지 못할 가능성이 큽니다. 왜 그렇습니까?

00:20:37.130 --> 00:20:40.700
왜 우리가 당신을 신뢰해야 합니까? 그리고 왜 당신은 구글이

00:20:40.700 --> 00:20:45.620
우리가 믿어도 되는 곳이라고 암묵적으로 믿는다고 생각하십니까?

00:20:46.340 --> 00:20:49.220
특히 가장 믿어야 할 곳이라고요? 이렇게

00:20:49.220 --> 00:20:50.180
위험하게 느껴지는 사안에서 말입니다. 네.

00:20:50.180 --> 00:20:54.770
저는 이런 회사들은 행동으로 판단해야 하고, 또한

00:20:54.770 --> 00:20:59.390
그 일을 이끄는 리더들의 동기 또한 들여다봐야 한다고 생각합니다.

00:20:59.390 --> 00:21:01.910
그런 시도들에서 말입니다. 그리고 저와 저희에게는

00:21:01.910 --> 00:21:05.810
제가 딥마인드의 적절한 터전으로 구글을 선택한 데에는 여러

00:21:05.810 --> 00:21:09.560
가지 이유가 있는데, 가장 큰 이유는 구글 창립자들과 구글이

00:21:09.560 --> 00:21:12.170
그들에 의해 과학 회사로 설립되었다는 점입니다.

00:21:12.290 --> 00:21:14.860
많은 분들이 잊지만, 구글 자체가 박사 과정

00:21:14.930 --> 00:21:17.570
프로젝트였습니다. 래리와 세르게이가 시작한 프로젝트였습니다.

00:21:17.750 --> 00:21:20.390
그래서 저는 그들과 즉각적인 친밀감을 느꼈습니다.

00:21:20.900 --> 00:21:24.560
또한 인수는 래리가 주도했지만, 그들이 구성한 이사회도

00:21:24.560 --> 00:21:26.150
눈여겨볼 만합니다. 예를 들어 존 헤네시 같은 분들이 있고,

00:21:26.150 --> 00:21:29.660
의장인 그는 튜링상 수상자이며 프랜시스 아널드 같은 또 다른

00:21:29.660 --> 00:21:32.090
노벨상 수상자도 있습니다. 이런 분들은

00:21:32.090 --> 00:21:36.290
기업 이사회에서는 매우 이례적인 구성입니다. 그래서 전체 환경이

00:21:36.290 --> 00:21:40.660
매우 과학적이며, 과학과 연구가 주도하고 공학이

00:21:40.660 --> 00:21:43.570
주도하는 문화가 그 조직에 깊이 뿌리내려 있습니다.

00:21:43.690 --> 00:21:47.170
이는 최고 수준의 과학을 한다는 것이 곧

00:21:47.170 --> 00:21:51.010
정말 엄격하고 사려 깊으며, 가능한 모든 곳에 과학적 방법을

00:21:51.010 --> 00:21:52.330
적용한다는 뜻입니다. 저는 이것이

00:21:52.330 --> 00:21:55.180
기술뿐 아니라 조직 운영 방식에도 해당된다고 생각합니다.

00:21:55.570 --> 00:22:01.150
그래서 저희는 가능한 한 매우 신중하고

00:22:01.150 --> 00:22:05.440
책임감 있게 행동하며, 세상에 내놓는 기술들에 대해 가능한 한

00:22:05.440 --> 00:22:07.600
큰 통제력을 가지려 노력해 왔다고 생각합니다. 그렇다고 해서

00:22:07.720 --> 00:22:11.800
모든 것을 항상 옳게 해낼 수 있다는 뜻은 아닙니다. 이 기술은

00:22:11.800 --> 00:22:14.200
너무 복잡하고 새로우며 초기 단계이고, 매우 변혁적이기 때문입니다. 하지만

00:22:14.200 --> 00:22:16.900
무언가가 잘못된다면 가능한 한 빨리 방향을 바로잡고자 합니다.

00:22:17.500 --> 00:22:21.940
그리고 마지막으로 말씀드리면, 저는 구글이 세상에서 하려는 일들의

00:22:21.940 --> 00:22:23.860
그런 성격 자체에 끌렸습니다.

00:22:24.040 --> 00:22:27.280
예를 들어 '세상의 정보를 정리한다'는 일은 매우 숭고한

00:22:28.030 --> 00:22:30.910
목표이며, 이는 분명 구글의 사명 선언문입니다.

00:22:31.000 --> 00:22:34.300
그리고 저는 그것이 딥마인드의 사명 선언문과도 매우 잘 맞는다고 생각합니다.

00:22:34.300 --> 00:22:37.690
즉 지능을 해결하고 그것으로 다른 모든 문제를 해결한다는 목표입니다.

00:22:37.690 --> 00:22:42.580
그리고 저는 이 두 사명 선언문이 자연스럽게 맞물린다고 생각합니다.

00:22:42.940 --> 00:22:46.210
AI와 세상의 정보를 정리하는 일은 본질적으로 함께 갑니다.

00:22:46.390 --> 00:22:49.840
또 구글은 그런 종류의 제품들로

00:22:49.930 --> 00:22:54.670
지도, Gmail, 그리고 당연히 검색 같은 것들로 잘 알려져 있습니다.

00:22:55.000 --> 00:22:58.600
저는 그것들이 세상에 정말 유용한 제품들이라고 생각하며, AI는

00:22:58.600 --> 00:23:01.330
그와 자연스럽게 결합할 수 있다고 봅니다. AI가 그런 제품들과

00:23:01.330 --> 00:23:04.330
어떻게 결합해 모두가 일상에서 쓰도록 더 향상시킬 수 있을지요.

00:23:04.330 --> 00:23:06.190
저는 그것이 세상에 매우 좋은 일이라고 생각합니다.

00:23:06.190 --> 00:23:09.010
그래서 그 일에 기여하게 되어 기쁘게 생각합니다.

00:23:09.220 --> 00:23:13.030
그렇다면 사람들이 더 이상 일자리를 갖지 않는 탈희소성 세계에서, 당신은

00:23:13.030 --> 00:23:18.220
모든 기술적 목표를 달성한 뒤에 개인적으로 시간을 어떻게 쓰실 계획입니까?

00:23:18.220 --> 00:23:18.940
목표를 말입니다. 네.

00:23:18.940 --> 00:23:21.460
연구는 그냥 스스로 자동화되는 것 아닙니까?

00:23:21.880 --> 00:23:25.180
저는 그것을 제가 할 일에 활용하고 싶습니다.

00:23:25.180 --> 00:23:29.650
싱귤래리티 이후에는 물리학의 한계를 탐구하는 데 쓰고 싶습니다.

00:23:29.860 --> 00:23:34.000
학교에서 제가 가장 좋아했던 주제는 '큰 질문'들이었습니다.

00:23:34.000 --> 00:23:37.150
예를 들면 현실의 구조는 무엇이며, 현실의 본질은 무엇입니까?

00:23:37.300 --> 00:23:42.250
의식의 본질은 무엇인지, 페르미 역설의 답은 무엇인지 같은

00:23:42.250 --> 00:23:43.450
질문들입니다. 시간은 무엇입니까?

00:23:43.450 --> 00:23:45.670
중력은 무엇입니까? 저는 더 많은 사람들이 이런 것들을

00:23:45.670 --> 00:23:49.330
일상에서 별로 생각하지 않으며 살아간다는 점이 놀랍습니다,

00:23:49.330 --> 00:23:54.340
저에게는 이런 거대한 질문들이 늘 '답이 무엇이냐'고

00:23:54.340 --> 00:23:56.350
마치 소리치는 것처럼 느껴지기 때문입니다.

00:23:56.590 --> 00:24:01.810
그리고 저는 AI를 활용해서 그런 모든 것들을 탐구해 보고 싶습니다, 어쩌면

00:24:01.810 --> 00:24:05.410
또한, 아시다시피, 새로운 에너지의 도움을 받아 별들로 여행하는 것까지도,

00:24:05.410 --> 00:24:08.830
에너지원과 재료 같은 것들, 그리고 AI가 열어 주는 다른 것들로 말입니다. 우리가 일을 하지 않으면

00:24:08.830 --> 00:24:10.270
우리 모두 의미와 목적을 가질 수 있겠습니까?

00:24:10.510 --> 00:24:13.630
음, 솔직히 말씀드리면 저는 그것이

00:24:13.630 --> 00:24:16.390
경제 문제보다 더 걱정되는 부분이라고 생각합니다. 경제는 거의

00:24:16.390 --> 00:24:19.690
정치적 질문에 가깝다고 생각합니다. 이런 추가적인 혜택과

00:24:19.690 --> 00:24:22.300
생산성을 얻게 될 때, 그것이 공유되도록

00:24:23.440 --> 00:24:26.110
모두에게 혜택이 돌아가게 할 수 있겠습니까? 그리고 저는 당연히 그것이

00:24:26.110 --> 00:24:28.540
제가 믿는 바입니다. 하지만 더 큰 질문은, 그리고 그것은

00:24:28.540 --> 00:24:32.260
우리 중 많은 사람들이 직장에서 얻는 목적과 의미는 어떻게 되느냐는 것입니다,

00:24:32.260 --> 00:24:36.070
과학적 노력에서 말입니다. 우리는 새로운 세계에서 그것을 어떻게 찾겠습니까?

00:24:36.640 --> 00:24:39.940
그리고 제 생각에는, 아시다시피, 새로운 위대한 철학자들이 필요할 것입니다,

00:24:40.120 --> 00:24:45.370
그 문제를 도와주고 그것을 끝까지 생각해 보도록 말입니다. 어쩌면, 아시다시피, 우리는

00:24:45.380 --> 00:24:50.080
예술과 탐구 같은 활동도 훨씬 더 세련되게 해 나가게 될 것이고, 그리고

00:24:50.080 --> 00:24:53.920
아시다시피 극한 스포츠처럼 오늘날 우리가 하는 많은 일들은 단지

00:24:53.920 --> 00:24:56.590
경제적 이익만을 위한 것은 아닙니다. 그리고 아마도 우리는 매우 난해한

00:24:56.590 --> 00:24:58.510
그런 것들의 미래형 버전도 갖게 될지도 모릅니다. 좋습니다.

00:24:58.510 --> 00:25:00.610
그래서 이 방에 있는 모두가 자신이 무엇을 해야 하는지 궁금해하고 있습니다.

00:25:00.610 --> 00:25:06.040
예컨대 10년 뒤 다보스에서 여기 앉아 있는 저는 무엇을 해야 합니까?

00:25:07.150 --> 00:25:10.600
이 방에 있는 사람들이 가장 크게 저지를 실수는 무엇이라고 생각하십니까

00:25:10.750 --> 00:25:13.600
AI에 관해서 말입니다. 자, 제 생각에는 두 가지가 있습니다.

00:25:13.600 --> 00:25:16.210
하나는 젊은 세대와

00:25:16.210 --> 00:25:19.630
우리 아이들 등에게 해당하는데, 우리가 확실히 말할 수 있는 유일한 것은 앞으로

00:25:19.630 --> 00:25:22.810
엄청난 변화가 일어날 것이라는 점입니다. 그래서 기술을 배우는 측면에서는,

00:25:23.080 --> 00:25:27.190
준비하셔야 합니다. 결국 배우는 법을 배우는 것이 가장 중요합니다.

00:25:27.310 --> 00:25:31.120
새로운 상황에 얼마나 빨리 적응하고, 새로운 정보를 흡수할 수 있느냐가 관건이며

00:25:31.120 --> 00:25:34.990
우리가 가진 도구를 활용하는 것입니다. 이 방의 CEO와 비즈니스 리더들에게는 어떻습니까?

00:25:35.170 --> 00:25:39.220
지금 가장 중요한 것은 선도 모델 제공업체가 여러 곳 있다는 점이며,

00:25:39.220 --> 00:25:43.150
선도 서비스 제공업체도 있고, 앞으로 이런 AI 모델의 제공업체는 더 늘 것이라는 점입니다.

00:25:43.150 --> 00:25:46.180
그러니, 아시다시피, 올바른 방식으로 접근하고 있다고

00:25:46.180 --> 00:25:49.540
여러분이 느끼는 파트너를 선택하십시오.

00:25:49.720 --> 00:25:54.040
그리고, 아시다시피, 변화를 만들어 가는 이들과

00:25:54.040 --> 00:25:57.040
세상에서 그렇게 되기를 바라는 방식으로 이 기술에 접근하는 이들과 협력하십시오.

00:25:57.250 --> 00:26:01.310
그러면 함께, 아시다시피, AI가 다가오면서 우리가 원하는 그 미래를

00:26:01.390 --> 00:26:02.950
만들어 갈 수 있다고 생각합니다.
