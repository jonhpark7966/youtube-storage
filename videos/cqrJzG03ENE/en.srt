1
00:00:00,000 --> 00:00:01,840
I think perhaps the thing that most surprised

2
00:00:01,840 --> 00:00:03,980
me is the extent to which I feel

3
00:00:03,980 --> 00:00:05,060
like the AI economy

4
00:00:05,060 --> 00:00:07,220
stabilized. We have like the model layer companies

5
00:00:07,220 --> 00:00:09,280
and the application layer companies and the

6
00:00:09,280 --> 00:00:10,960
infrastructure layer companies. It seems like everyone is

7
00:00:10,960 --> 00:00:12,740
going to make a lot of money and

8
00:00:12,740 --> 00:00:14,720
there's kind of like a relative playbook for

9
00:00:14,720 --> 00:00:16,320
how to build an AI native company on

10
00:00:16,320 --> 00:00:16,940
top of the models.

11
00:00:17,120 --> 00:00:18,800
Many episodes ago we talked about how it

12
00:00:18,800 --> 00:00:20,820
was felt easier than ever to peer and

13
00:00:20,820 --> 00:00:21,820
find a startup idea

14
00:00:21,820 --> 00:00:23,840
because if you could just survive, if you

15
00:00:23,840 --> 00:00:25,460
just wait a few months, there was likely

16
00:00:25,460 --> 00:00:25,780
going to be

17
00:00:25,780 --> 00:00:28,640
some like big announcement that would completely make

18
00:00:28,640 --> 00:00:30,420
a new set of ideas possible. And so

19
00:00:30,420 --> 00:00:30,620
like

20
00:00:30,620 --> 00:00:32,720
finding ideas is sort of returning to sort

21
00:00:32,720 --> 00:00:34,560
of normal levels of difficulty.

22
00:00:42,020 --> 00:00:44,300
Welcome back to another episode of The Light

23
00:00:44,300 --> 00:00:46,540
Cone. Today we're talking about the most surprising

24
00:00:46,540 --> 00:00:49,220
things that we saw this year in 2025.

25
00:00:50,060 --> 00:00:53,400
Diana, you found a pretty crazy one. It's

26
00:00:53,400 --> 00:00:53,960
sort of a changing

27
00:00:53,960 --> 00:00:56,460
of the guard almost in who is the

28
00:00:56,460 --> 00:00:59,860
preferred LLM at YC during the YC batch.

29
00:01:00,120 --> 00:01:02,440
Yes. In fact, we just wrapped up the

30
00:01:02,440 --> 00:01:06,660
winter 26 selection cycle for companies. And one

31
00:01:06,660 --> 00:01:06,880
of the

32
00:01:06,880 --> 00:01:09,240
questions we asked to all the founders that

33
00:01:09,240 --> 00:01:11,720
apply to YC is what is your tech

34
00:01:11,720 --> 00:01:13,640
stack and model of choice?

35
00:01:14,000 --> 00:01:16,040
And one of the shocking things is that

36
00:01:16,040 --> 00:01:18,920
for the longest time, OpenAI was the clear

37
00:01:18,920 --> 00:01:19,540
winner

38
00:01:19,540 --> 00:01:22,920
for all of last year, last couple of

39
00:01:22,920 --> 00:01:25,460
batches, though that number has been coming down.

40
00:01:26,380 --> 00:01:30,520
And shockingly, in this batch, the number one

41
00:01:30,520 --> 00:01:34,920
API is actually Anthropic. It came out a

42
00:01:34,920 --> 00:01:35,540
bit more than

43
00:01:35,540 --> 00:01:37,800
OpenAI, which who would have thought? I think

44
00:01:37,800 --> 00:01:40,620
when we started this podcast series back then,

45
00:01:40,620 --> 00:01:44,180
OpenAI was like 90 plus percent. And now,

46
00:01:44,940 --> 00:01:46,620
Anthropic, who would have thought?

47
00:01:46,860 --> 00:01:48,620
Yeah. And you know, they'd been hovering around

48
00:01:48,620 --> 00:01:52,060
like 20, 25% for most of like

49
00:01:52,060 --> 00:01:55,980
2024 and early 2025. And then

50
00:01:55,980 --> 00:01:58,280
only even in the last three to six

51
00:01:58,280 --> 00:02:00,740
months, did this sort of changing of the

52
00:02:00,740 --> 00:02:01,640
guard actually happen?

53
00:02:01,640 --> 00:02:03,860
They had this hockey stick with the with

54
00:02:03,860 --> 00:02:06,060
the growth are over 52%.

55
00:02:06,060 --> 00:02:07,420
Why do you think that is?

56
00:02:07,660 --> 00:02:09,380
I think there's a couple of things in

57
00:02:09,380 --> 00:02:11,440
terms of the tech stack selection. I think

58
00:02:11,440 --> 00:02:12,760
as we've seen this year,

59
00:02:13,280 --> 00:02:16,000
there's been a lot of wins in terms

60
00:02:16,000 --> 00:02:18,220
of vibe coding tools that are getting built

61
00:02:18,220 --> 00:02:19,680
out out there. And

62
00:02:19,680 --> 00:02:22,700
coding agents are so many categories that this

63
00:02:22,700 --> 00:02:25,740
ended up being a bigger problem space that

64
00:02:25,740 --> 00:02:26,240
actually is

65
00:02:26,240 --> 00:02:28,140
creating a lot of value. And it turns

66
00:02:28,140 --> 00:02:30,760
out the model that performs the best at

67
00:02:30,760 --> 00:02:31,580
it is actually

68
00:02:32,580 --> 00:02:35,800
models from Anthropic. And I think that's not

69
00:02:35,800 --> 00:02:38,080
by accident. I think from hearing the conversation

70
00:02:38,080 --> 00:02:39,780
we had with Tom Brown, not too long

71
00:02:39,780 --> 00:02:41,500
ago, he came and spoke is that was

72
00:02:41,500 --> 00:02:43,180
one of their internal evals.

73
00:02:43,360 --> 00:02:45,940
They, on purpose, made them their North Star.

74
00:02:46,120 --> 00:02:47,840
And you can see it in the model

75
00:02:47,840 --> 00:02:49,220
taste as a result of

76
00:02:49,740 --> 00:02:53,060
what's the best choice of model for a

77
00:02:53,060 --> 00:02:55,580
lot of founders building products is Anthropic.

78
00:02:55,840 --> 00:02:58,140
The vast majority of the use cases people

79
00:02:58,140 --> 00:03:00,060
are using it for, though, is not coding.

80
00:03:00,240 --> 00:03:01,000
So I wonder if there's

81
00:03:01,000 --> 00:03:02,320
like a bleed through effect, where people are

82
00:03:02,320 --> 00:03:05,420
using Claude for their personal coding. And then

83
00:03:05,420 --> 00:03:07,680
as a result, they're more likely to choose

84
00:03:07,680 --> 00:03:09,260
it for their application, even if their application

85
00:03:09,260 --> 00:03:09,460
is

86
00:03:09,460 --> 00:03:10,300
not doing coding at all.

87
00:03:10,300 --> 00:03:12,940
Because you'd be very familiar with like the

88
00:03:12,940 --> 00:03:15,940
personality of Claude Opus or whatever they're

89
00:03:15,940 --> 00:03:17,720
choosing. Sonnet, I suppose.

90
00:03:18,140 --> 00:03:20,440
How about Gemini? How's Gemini doing in those

91
00:03:20,440 --> 00:03:20,720
rankings?

92
00:03:21,060 --> 00:03:23,440
Gemini has also pretty much has been climbing

93
00:03:23,440 --> 00:03:25,780
up pretty, pretty high. I think last year

94
00:03:25,780 --> 00:03:26,000
was

95
00:03:26,000 --> 00:03:28,640
probably single digit percent or even like two,

96
00:03:28,800 --> 00:03:32,480
three percent. And now for winter 26, it's

97
00:03:32,480 --> 00:03:32,660
about

98
00:03:32,660 --> 00:03:36,320
23 percent. And we've personally been using also

99
00:03:36,320 --> 00:03:38,320
a lot of Gemini 3.0 and we've

100
00:03:38,320 --> 00:03:39,140
been impressed with

101
00:03:39,140 --> 00:03:42,200
the quality of it. I think it's really,

102
00:03:42,420 --> 00:03:43,300
really working.

103
00:03:43,540 --> 00:03:45,800
I mean, they have all different personalities, don't

104
00:03:45,800 --> 00:03:45,980
they?

105
00:03:46,160 --> 00:03:46,280
That too.

106
00:03:46,560 --> 00:03:46,760
Yeah.

107
00:03:47,300 --> 00:03:49,940
It's kind of the classic where OpenAI sort

108
00:03:49,940 --> 00:03:52,740
of has the black cat energy. And almost

109
00:03:52,740 --> 00:03:53,020
like

110
00:03:54,340 --> 00:03:55,780
Anthropic is kind of more the happy-go

111
00:03:55,780 --> 00:03:58,660
-lucky, a bit more very helpful golden retriever.

112
00:03:58,840 --> 00:03:59,240
At least

113
00:03:59,240 --> 00:04:00,360
that's what I feel when I talk to

114
00:04:00,360 --> 00:04:00,520
them.

115
00:04:00,940 --> 00:04:01,920
And how about Gemini?

116
00:04:02,140 --> 00:04:03,220
It's kind of like in between.

117
00:04:03,780 --> 00:04:05,480
Harj, you prefer Gemini, actually.

118
00:04:05,860 --> 00:04:08,900
Yeah, I've switched to Gemini this year as

119
00:04:08,900 --> 00:04:11,340
my just go-to model. I think even

120
00:04:11,340 --> 00:04:13,360
before 2.5 Pro came

121
00:04:13,360 --> 00:04:16,920
out and just seemed better at reasoning for

122
00:04:16,920 --> 00:04:19,400
me, it was just like the increasingly I

123
00:04:19,400 --> 00:04:20,120
replaced my

124
00:04:20,120 --> 00:04:22,420
Google searches with Gemini. And I just sort

125
00:04:22,420 --> 00:04:24,960
of trusted that Google's, I think like the

126
00:04:24,960 --> 00:04:25,320
groundings

127
00:04:25,320 --> 00:04:27,340
API and its ability to actually like use

128
00:04:27,340 --> 00:04:29,740
the Google index to give you like real

129
00:04:29,740 --> 00:04:30,820
-time information

130
00:04:30,820 --> 00:04:32,900
correctly. I just found it was better than,

131
00:04:33,100 --> 00:04:34,660
personally I found it was better than all

132
00:04:34,660 --> 00:04:34,760
the

133
00:04:34,760 --> 00:04:35,840
other tools for that. And it was better

134
00:04:35,840 --> 00:04:37,780
than Perplexity on it too. Like Perplexity would

135
00:04:37,780 --> 00:04:37,940
be

136
00:04:37,940 --> 00:04:40,300
fast, but not always accurate. And Gemini was

137
00:04:40,300 --> 00:04:42,040
not quite as fast as Perplexity, but it

138
00:04:42,040 --> 00:04:42,400
was always

139
00:04:42,400 --> 00:04:44,760
pretty accurate if I asked it about something

140
00:04:44,760 --> 00:04:45,940
that happened today, for example.

141
00:04:45,940 --> 00:04:47,960
Even if you use Gemini as the reasoning

142
00:04:47,960 --> 00:04:49,420
engine in Perplexity?

143
00:04:50,140 --> 00:04:51,200
I have not done that.

144
00:04:51,440 --> 00:04:53,640
Interesting. Yeah. So it's hard to know like

145
00:04:53,640 --> 00:04:55,020
how much of it is the tooling and

146
00:04:55,020 --> 00:04:55,800
how much of it is

147
00:04:55,800 --> 00:04:57,100
like the base LLM.

148
00:04:57,220 --> 00:04:57,640
That's fair.

149
00:04:57,880 --> 00:04:59,900
Yeah. I mean, what are your guys' tools

150
00:04:59,900 --> 00:05:01,860
of choice? I haven't switched off of ChatGPT.

151
00:05:02,000 --> 00:05:02,140
I mean,

152
00:05:02,260 --> 00:05:04,440
I find the memory very sticky. It knows

153
00:05:04,440 --> 00:05:06,400
me, it knows my personality, it knows the

154
00:05:06,400 --> 00:05:06,800
things that

155
00:05:06,800 --> 00:05:09,620
I think about. And so I'll use Perplexity

156
00:05:09,620 --> 00:05:13,580
for fast web searches or things that I

157
00:05:13,580 --> 00:05:14,200
know is like

158
00:05:14,200 --> 00:05:16,680
a research task because I think ChatGPT is

159
00:05:16,680 --> 00:05:17,960
still like a little bit of a step

160
00:05:17,960 --> 00:05:19,320
behind for searching

161
00:05:19,320 --> 00:05:20,940
the web. I don't know. I think memory

162
00:05:20,940 --> 00:05:25,540
is turning into an actual moat for like

163
00:05:25,540 --> 00:05:26,740
that consumer experience.

164
00:05:26,740 --> 00:05:29,340
And I don't expect Gemini to ever have

165
00:05:29,340 --> 00:05:32,400
the personality that I would expect from ChatGPT.

166
00:05:32,560 --> 00:05:34,600
It just feels like a different like entity,

167
00:05:34,920 --> 00:05:35,140
you know?

168
00:05:35,480 --> 00:05:37,340
The thing I'm still surprised about is why

169
00:05:37,340 --> 00:05:42,380
there just aren't more consumer apps around like

170
00:05:42,380 --> 00:05:42,780
all the

171
00:05:42,780 --> 00:05:44,960
various things we do. Like if I think

172
00:05:44,960 --> 00:05:46,400
back, one of the big changes for me

173
00:05:46,400 --> 00:05:47,960
this year is just the amount of

174
00:05:48,840 --> 00:05:51,220
prompting and context engineering I do for like

175
00:05:51,220 --> 00:05:53,620
my life. Like we bought a house recently

176
00:05:53,620 --> 00:05:54,020
and like

177
00:05:54,020 --> 00:05:55,700
the whole thing, like I just had like

178
00:05:55,700 --> 00:05:59,220
a really long running ChatGPT conversation stuffing it

179
00:05:59,220 --> 00:05:59,440
full of

180
00:05:59,440 --> 00:06:01,980
context of like every inspection report or like

181
00:06:01,980 --> 00:06:04,400
wanting it to be like level the playing

182
00:06:04,400 --> 00:06:04,880
field between

183
00:06:04,880 --> 00:06:06,720
me and like the realtor to understand kind

184
00:06:06,720 --> 00:06:08,420
of all the dynamics and things that are

185
00:06:08,420 --> 00:06:08,920
going on.

186
00:06:09,120 --> 00:06:11,240
And it just feels like there should be

187
00:06:11,240 --> 00:06:12,180
an app for that.

188
00:06:12,180 --> 00:06:15,800
But simultaneously, I'm sure you took the PDFs

189
00:06:15,800 --> 00:06:18,080
and just like dropped them into Gemini and

190
00:06:18,080 --> 00:06:18,640
said

191
00:06:18,640 --> 00:06:20,900
like, well, summarize and tell me what's important

192
00:06:20,900 --> 00:06:21,280
for me.

193
00:06:21,360 --> 00:06:23,180
I guess I worry about, I worried about,

194
00:06:23,220 --> 00:06:25,260
I still don't trust the models enough to

195
00:06:25,260 --> 00:06:25,460
be

196
00:06:25,460 --> 00:06:28,160
accurate without lots of prompting. And it's a

197
00:06:28,160 --> 00:06:29,820
high value transaction. So you don't want to

198
00:06:29,820 --> 00:06:30,100
like

199
00:06:30,100 --> 00:06:32,360
get incorrect data out of it. So I

200
00:06:32,360 --> 00:06:33,580
still feel like you need to put in

201
00:06:33,580 --> 00:06:34,760
the work and it feels like

202
00:06:34,760 --> 00:06:37,360
there should still be apps that just do

203
00:06:37,360 --> 00:06:38,260
all the work for you.

204
00:06:38,380 --> 00:06:40,080
Did you see Karpathy release like sort of

205
00:06:40,080 --> 00:06:42,920
a LLM arena of a sort, which I

206
00:06:42,920 --> 00:06:44,320
mean, I do by like hand

207
00:06:44,320 --> 00:06:46,480
right now using tabs. It's like you have

208
00:06:46,480 --> 00:06:49,260
Claude open, you have Gemini open, you have

209
00:06:49,260 --> 00:06:50,020
ChatGPT open,

210
00:06:50,020 --> 00:06:52,400
and you give it the same task. And

211
00:06:52,400 --> 00:06:54,240
then you take the output from each. And

212
00:06:54,240 --> 00:06:55,440
then I usually go to Claude

213
00:06:55,440 --> 00:06:56,900
at that point. And I'm like, all right,

214
00:06:57,040 --> 00:06:58,380
Claude, this is what the other one said.

215
00:06:58,480 --> 00:06:59,020
What do you think?

216
00:06:59,020 --> 00:07:01,060
And check each other's work. I actually think

217
00:07:01,060 --> 00:07:02,860
that that particular behavior at the consumer,

218
00:07:03,000 --> 00:07:06,360
that level that we're doing, startups are doing

219
00:07:06,360 --> 00:07:08,360
as well. They are actually arbitraging

220
00:07:08,360 --> 00:07:09,660
a lot of the models. I had some

221
00:07:09,660 --> 00:07:12,320
conversations with a number of founders where before

222
00:07:12,320 --> 00:07:12,620
they

223
00:07:12,620 --> 00:07:15,180
might've been loyalists to, let's say, open AI

224
00:07:15,180 --> 00:07:18,580
models or Anthropic. And I just had some

225
00:07:18,580 --> 00:07:20,760
conversations recently with them. And these are founders

226
00:07:20,760 --> 00:07:23,380
that are running larger companies like

227
00:07:23,380 --> 00:07:26,420
series B level type of companies. With AI,

228
00:07:26,680 --> 00:07:29,600
they're actually abstracting all that away

229
00:07:29,600 --> 00:07:33,580
and building this orchestration layer where perhaps as

230
00:07:33,580 --> 00:07:35,620
each new model release comes out,

231
00:07:35,760 --> 00:07:37,980
they can swap them in and out, or

232
00:07:37,980 --> 00:07:40,360
they can use specific models that are better

233
00:07:40,360 --> 00:07:41,440
at certain things

234
00:07:41,440 --> 00:07:43,700
for just that. For example, I heard from

235
00:07:43,700 --> 00:07:47,000
the startup, they use Gemini 3 to do

236
00:07:47,000 --> 00:07:47,600
the context

237
00:07:47,600 --> 00:07:50,740
engineering, which they actually then fed into OpenAI

238
00:07:50,740 --> 00:07:52,540
to execute it. And they keep swapping it

239
00:07:52,540 --> 00:07:52,960
as

240
00:07:52,960 --> 00:07:55,000
new models come up and the winner for

241
00:07:55,000 --> 00:07:58,600
each category or type of agent work is

242
00:07:58,600 --> 00:07:58,960
different.

243
00:07:59,600 --> 00:08:02,680
And ultimately they can do this because it

244
00:08:02,680 --> 00:08:04,840
is all grounded based on the evals. And

245
00:08:04,840 --> 00:08:05,580
the evals are all

246
00:08:05,580 --> 00:08:07,820
proprietary to them because they're a vertical AI

247
00:08:07,820 --> 00:08:09,540
agent and they just work in a very

248
00:08:09,540 --> 00:08:10,420
regulated industry

249
00:08:10,420 --> 00:08:12,300
and they have this data set that just

250
00:08:12,300 --> 00:08:14,020
works the best for them. I think this

251
00:08:14,020 --> 00:08:15,300
is the new normal right

252
00:08:15,300 --> 00:08:17,600
now where people are expecting, yeah, that it's

253
00:08:17,600 --> 00:08:20,260
cool that the model companies, they're spending all

254
00:08:20,260 --> 00:08:20,400
this

255
00:08:20,400 --> 00:08:23,960
money and making intelligence faster and better and

256
00:08:23,960 --> 00:08:25,240
we can all benefit. Let's just do the

257
00:08:25,240 --> 00:08:25,580
best.

258
00:08:26,020 --> 00:08:28,700
It's almost like the era of Intel and

259
00:08:28,700 --> 00:08:31,240
AMD with new architecture would come up. People

260
00:08:31,240 --> 00:08:31,800
could just swap

261
00:08:31,800 --> 00:08:33,380
them, right? Yeah, if it was at the

262
00:08:33,380 --> 00:08:35,960
highest level, that angst around where's the value

263
00:08:35,960 --> 00:08:36,660
going to accrue?

264
00:08:36,800 --> 00:08:37,560
Is it going to go to the model

265
00:08:37,560 --> 00:08:39,660
companies or like the application layer, i.e.

266
00:08:39,660 --> 00:08:40,900
the startups, feels like

267
00:08:40,900 --> 00:08:43,760
that ebbs and flows in either direction a

268
00:08:43,760 --> 00:08:45,200
little bit throughout the year to me. Like

269
00:08:45,200 --> 00:08:46,020
I feel there are moments

270
00:08:46,020 --> 00:08:49,080
where Claude Code, amazing launch and it was

271
00:08:49,080 --> 00:08:50,540
like, oh, okay, like the model companies are

272
00:08:50,540 --> 00:08:50,700
actually

273
00:08:50,700 --> 00:08:52,380
going to play out the application layer. But

274
00:08:52,380 --> 00:08:53,760
then to me at least it's all vibes

275
00:08:53,760 --> 00:08:54,420
based. Like the

276
00:08:54,420 --> 00:08:56,400
Gemini surge, especially over the last few months,

277
00:08:56,480 --> 00:08:57,900
just feels like it returns us to a

278
00:08:57,900 --> 00:08:58,460
world of where

279
00:08:58,460 --> 00:09:00,800
exactly that. Like the models are all essentially

280
00:09:00,800 --> 00:09:02,800
commoditizing each other and it's just like the

281
00:09:03,520 --> 00:09:05,240
application layer and the startups are going to

282
00:09:05,240 --> 00:09:07,540
set up to have another fantastic year if

283
00:09:07,540 --> 00:09:08,040
that continues.

284
00:09:08,040 --> 00:09:10,840
I'm curious what you think, Jared, with some,

285
00:09:10,840 --> 00:09:15,700
a lot of perhaps the negative comments on

286
00:09:15,700 --> 00:09:15,940
Twitter

287
00:09:15,940 --> 00:09:17,560
around, is this a bit of a bubble,

288
00:09:17,860 --> 00:09:22,020
an AI bubble? Yeah. When I talk to

289
00:09:22,020 --> 00:09:23,140
undergrads, this is like a

290
00:09:23,140 --> 00:09:25,080
common question that I get is like, oh,

291
00:09:25,220 --> 00:09:27,800
like I heard it's a big AI bubble

292
00:09:27,800 --> 00:09:29,180
because like there's all

293
00:09:29,180 --> 00:09:31,320
this like crazy round tripping going between NVIDIA

294
00:09:31,320 --> 00:09:35,400
and OpenAI and like, is it all fake?

295
00:09:35,600 --> 00:09:35,760
Yeah.

296
00:09:35,760 --> 00:09:38,420
No, this is fantastic, right? Like people look

297
00:09:38,420 --> 00:09:39,700
at the telecom bubble and it's like there's

298
00:09:39,700 --> 00:09:39,880
just,

299
00:09:39,920 --> 00:09:41,840
you know, billions of dollars, like tens of

300
00:09:41,840 --> 00:09:43,400
billions, hundreds of billions, just like

301
00:09:43,400 --> 00:09:45,960
sort of sitting in a bunch of telecom

302
00:09:45,960 --> 00:09:48,620
back in like the, you know, 90s. Actually,

303
00:09:48,680 --> 00:09:49,640
that's why YouTube was

304
00:09:49,640 --> 00:09:52,020
able to exist, right? Like if you just

305
00:09:52,020 --> 00:09:53,940
have a whole bunch of extra bandwidth that

306
00:09:53,940 --> 00:09:54,780
isn't being used and

307
00:09:54,780 --> 00:09:56,680
is relatively cheap, the cost is low enough

308
00:09:56,680 --> 00:09:58,660
for like something like YouTube to exist. Like

309
00:09:58,660 --> 00:09:58,920
if there

310
00:09:58,920 --> 00:10:02,060
wasn't a glut of telecom, then like maybe

311
00:10:02,060 --> 00:10:03,300
YouTube would have happened and just would have

312
00:10:03,300 --> 00:10:03,560
happened

313
00:10:03,560 --> 00:10:05,560
later. And then that, isn't that like sort

314
00:10:05,560 --> 00:10:07,560
of what we're talking about here? Like how

315
00:10:07,560 --> 00:10:07,860
do we,

316
00:10:07,980 --> 00:10:10,280
we have to accelerate, right? We have the

317
00:10:10,280 --> 00:10:12,120
age of intelligence, the rocks can talk, they

318
00:10:12,120 --> 00:10:12,580
can think,

319
00:10:12,700 --> 00:10:13,880
and they can do work and you just

320
00:10:13,880 --> 00:10:16,100
have to zap them more and you get

321
00:10:16,100 --> 00:10:17,820
like smarter and smarter stuff at

322
00:10:17,820 --> 00:10:20,060
this point. I think the argument to college

323
00:10:20,060 --> 00:10:22,500
students is actually like, because there will be

324
00:10:22,500 --> 00:10:22,880
a glut,

325
00:10:23,480 --> 00:10:25,460
there is an opportunity for you. And if

326
00:10:25,460 --> 00:10:27,780
there was not a glut, then there wouldn't

327
00:10:27,780 --> 00:10:28,160
be as much

328
00:10:28,160 --> 00:10:31,900
competition. The prices would be higher. The margins

329
00:10:31,900 --> 00:10:34,640
lower in the stack would be higher, right?

330
00:10:35,020 --> 00:10:36,300
And then, you know, what's one of the

331
00:10:36,300 --> 00:10:39,140
big stories this year? Like Nvidia suddenly is

332
00:10:39,140 --> 00:10:40,240
on the outs. Like

333
00:10:40,240 --> 00:10:41,940
I think their stock is today is like

334
00:10:41,940 --> 00:10:44,380
around 170s or something. You know, I think

335
00:10:44,380 --> 00:10:45,540
I'm still a long-term

336
00:10:45,540 --> 00:10:47,780
buy and hold, honestly. But for the moment,

337
00:10:48,000 --> 00:10:49,500
people are like, oh, well, Gemini is so

338
00:10:49,500 --> 00:10:49,960
good.

339
00:10:49,960 --> 00:10:52,140
And all the, you know, nobody seems to

340
00:10:52,140 --> 00:10:55,260
be Nvidia only now and everyone's buying AMD

341
00:10:55,260 --> 00:10:56,040
and everyone's,

342
00:10:56,180 --> 00:10:59,160
you know, and TPUs are working. So, you

343
00:10:59,160 --> 00:11:01,200
know, at the moment it looks like there's,

344
00:11:01,360 --> 00:11:01,580
you know,

345
00:11:01,700 --> 00:11:04,680
what does that mean? Like there's competition and

346
00:11:04,680 --> 00:11:07,920
it means that there will be more compute,

347
00:11:08,080 --> 00:11:11,200
not less. And then that means that probably

348
00:11:11,200 --> 00:11:13,880
a little bit better things for all of

349
00:11:13,880 --> 00:11:14,900
the big LLM

350
00:11:14,900 --> 00:11:16,800
companies like sort of the, you know, the

351
00:11:16,800 --> 00:11:19,180
AI labs, they get a little bit of

352
00:11:19,180 --> 00:11:19,900
power, but,

353
00:11:20,020 --> 00:11:22,440
you know, they too are in competition with

354
00:11:22,440 --> 00:11:24,200
one another. So then what does that mean?

355
00:11:24,320 --> 00:11:24,380
Well,

356
00:11:24,460 --> 00:11:26,260
then it's, you know, go up another level

357
00:11:26,260 --> 00:11:28,200
in this stack, right? Like as long as

358
00:11:28,200 --> 00:11:29,240
there are a great many

359
00:11:29,700 --> 00:11:32,160
AI labs that are in a deep competition

360
00:11:32,160 --> 00:11:35,680
with one another, then that's even better for

361
00:11:35,680 --> 00:11:36,640
that college

362
00:11:36,640 --> 00:11:38,820
student who's about to start a company at

363
00:11:38,820 --> 00:11:41,160
the application level. Yeah, I think that's exactly

364
00:11:41,160 --> 00:11:41,400
right.

365
00:11:41,400 --> 00:11:43,560
It's like people are asking this question, like,

366
00:11:43,700 --> 00:11:45,840
is it a bubble? That's maybe a question

367
00:11:45,840 --> 00:11:46,060
that's

368
00:11:46,060 --> 00:11:47,580
really relevant. If you're like the equivalent of

369
00:11:47,580 --> 00:11:49,860
like Comcast, like if you're Nvidia, that's a

370
00:11:49,860 --> 00:11:50,020
very

371
00:11:50,020 --> 00:11:52,320
relevant question. Like, oh, are people overbuilding GPU

372
00:11:52,320 --> 00:11:54,660
capacity, but like the college students, they're

373
00:11:54,660 --> 00:11:57,100
not Comcast. They're actually like YouTube. If you're

374
00:11:57,100 --> 00:11:58,480
doing a startup in your dorm room, it's

375
00:11:58,480 --> 00:11:58,580
like

376
00:11:58,580 --> 00:12:00,220
the AI equivalent of like YouTube and like

377
00:12:00,220 --> 00:12:02,380
kind of doesn't really matter that much. Maybe

378
00:12:02,380 --> 00:12:02,800
Nvidia's

379
00:12:02,800 --> 00:12:04,700
talk will go down next year. I don't

380
00:12:04,700 --> 00:12:07,060
know. But like, even if it does, that

381
00:12:07,060 --> 00:12:07,800
doesn't actually mean

382
00:12:07,800 --> 00:12:08,900
that it's like a bad time to be

383
00:12:08,900 --> 00:12:10,540
working on an AI startup. Yeah, it's what

384
00:12:10,540 --> 00:12:11,220
Zuck said in a

385
00:12:11,220 --> 00:12:12,720
podcast early this year, I think, right? It's

386
00:12:12,720 --> 00:12:15,420
like Meta may end up over investing like

387
00:12:15,420 --> 00:12:16,000
a significant

388
00:12:16,000 --> 00:12:19,620
amount in like the capex and infrastructure, but

389
00:12:19,620 --> 00:12:21,360
like they essentially have to, the big companies

390
00:12:21,360 --> 00:12:22,720
have to do it because they can't just

391
00:12:22,720 --> 00:12:24,860
like sit on the sidelines. And in the

392
00:12:24,860 --> 00:12:27,200
case, like demand falls

393
00:12:27,200 --> 00:12:29,300
off a cliff for some reason, it's their

394
00:12:29,300 --> 00:12:31,380
capex, not the startup's capex. And there's still

395
00:12:31,380 --> 00:12:31,780
going to be

396
00:12:31,780 --> 00:12:34,120
tons of infrastructure and ideas to still continue

397
00:12:34,120 --> 00:12:36,820
building. There was this book written by this

398
00:12:36,820 --> 00:12:40,380
economist called Carlota Perez, who studied a lot

399
00:12:40,380 --> 00:12:43,140
of tech trends, and it studies a lot

400
00:12:43,140 --> 00:12:43,340
of

401
00:12:44,220 --> 00:12:47,200
technology revolutions. And it summarizes that there's really

402
00:12:47,200 --> 00:12:49,840
two phases. There's the phase of

403
00:12:49,840 --> 00:12:52,100
installation, which is where a lot of the

404
00:12:52,100 --> 00:12:56,180
very heavy capex investment come in. And then

405
00:12:56,180 --> 00:12:56,580
there's the

406
00:12:56,580 --> 00:13:00,520
deployment phase where really ripples, where it rips,

407
00:13:00,600 --> 00:13:03,000
and then everything explodes in terms of abundance.

408
00:13:03,000 --> 00:13:06,840
And during the initial phase of installation is

409
00:13:06,840 --> 00:13:08,340
where it feels like a bubble. There's a

410
00:13:08,340 --> 00:13:08,580
bit of a

411
00:13:08,580 --> 00:13:10,540
frenzy because it starts first with a, there's

412
00:13:10,540 --> 00:13:13,040
this new technology that's amazing, which happened

413
00:13:13,040 --> 00:13:16,080
with the ChatGPT moment in 2023. Everyone got

414
00:13:16,080 --> 00:13:18,420
super excited about the tech, and then everyone's

415
00:13:18,420 --> 00:13:18,640
got

416
00:13:18,640 --> 00:13:21,800
super hyped and got into investing into a

417
00:13:21,800 --> 00:13:23,440
lot of the infrastructure with buying a lot

418
00:13:23,440 --> 00:13:24,240
of GPUs and

419
00:13:24,240 --> 00:13:27,040
all the giant gigawatt data center build out.

420
00:13:27,160 --> 00:13:28,680
And then people say, but what is the

421
00:13:28,680 --> 00:13:29,340
demand? What are

422
00:13:29,340 --> 00:13:30,760
going to be all the applications to be

423
00:13:30,760 --> 00:13:32,300
built out? I think right now we're in

424
00:13:32,300 --> 00:13:32,860
that transition,

425
00:13:32,880 --> 00:13:36,140
which is actually really good news for startup

426
00:13:36,140 --> 00:13:38,980
founders because they are not involved into the

427
00:13:38,980 --> 00:13:41,280
building the data centers, but they're going to

428
00:13:41,280 --> 00:13:44,600
build the next generation of applications in the

429
00:13:44,600 --> 00:13:48,180
deployment phase when it really proliferates. And what

430
00:13:48,180 --> 00:13:50,320
happened, just going back to the analogy with

431
00:13:50,320 --> 00:13:53,800
with the era of the internet, before the

432
00:13:53,800 --> 00:13:55,660
2000, there was a lot of heavy capex

433
00:13:55,660 --> 00:13:56,740
investment into

434
00:13:56,740 --> 00:14:00,480
the telcos, right? Those were giant projects that

435
00:14:00,480 --> 00:14:02,800
college students wouldn't be involved,

436
00:14:03,000 --> 00:14:05,660
but they were very heavily invested. And in

437
00:14:05,660 --> 00:14:07,740
some cases were over invested. I mean,

438
00:14:07,820 --> 00:14:09,660
there's a whole thing with dark fiber and

439
00:14:09,660 --> 00:14:11,760
some pipes that are not used and that's

440
00:14:11,760 --> 00:14:12,020
fine.

441
00:14:12,260 --> 00:14:15,280
The internet ended up being still a giant

442
00:14:15,280 --> 00:14:18,720
economic driver. And what that means is startups

443
00:14:18,720 --> 00:14:19,340
like the

444
00:14:19,340 --> 00:14:21,400
future Facebook or the future Google are yet

445
00:14:21,400 --> 00:14:23,160
to be started because those come in in

446
00:14:23,160 --> 00:14:23,780
the deployment

447
00:14:23,780 --> 00:14:26,440
phase. Because right now I think things are

448
00:14:26,440 --> 00:14:28,440
still getting built up. I do think the

449
00:14:28,440 --> 00:14:29,100
foundation lab

450
00:14:29,100 --> 00:14:31,820
companies and GPUs very much are falling into

451
00:14:31,820 --> 00:14:33,060
the bucket of infrastructure.

452
00:14:33,500 --> 00:14:35,460
Yeah. I mean, it's interesting to watch how

453
00:14:35,460 --> 00:14:37,160
this stuff is evolving a little bit. So

454
00:14:37,160 --> 00:14:37,540
do you remember

455
00:14:37,540 --> 00:14:40,340
summer 24, there was a company called StarCloud

456
00:14:40,340 --> 00:14:42,440
that came out and was one of the

457
00:14:42,440 --> 00:14:43,300
first to come out and

458
00:14:43,300 --> 00:14:45,840
say, we're going to make data centers in

459
00:14:45,840 --> 00:14:48,560
space. And what was the reaction when, you

460
00:14:48,560 --> 00:14:48,660
know,

461
00:14:48,660 --> 00:14:50,980
people laugh at them on the internet. Yeah.

462
00:14:51,380 --> 00:14:52,800
They said, that's the stupidest idea ever.

463
00:14:53,160 --> 00:14:55,760
You know, I guess 18 months later, suddenly

464
00:14:55,760 --> 00:14:57,900
Google's doing it. Elon's doing it.

465
00:14:57,900 --> 00:14:59,460
So now it mentions it in every interview

466
00:14:59,460 --> 00:15:00,040
now, apparently.

467
00:15:00,220 --> 00:15:00,460
Is that right?

468
00:15:00,520 --> 00:15:02,160
It seems to be like his top talking

469
00:15:02,160 --> 00:15:02,400
point.

470
00:15:02,620 --> 00:15:04,500
Yeah. And so, I mean, why is that?

471
00:15:04,700 --> 00:15:06,180
Like, I feel like one of the aspects

472
00:15:06,180 --> 00:15:06,860
is that like,

473
00:15:07,360 --> 00:15:10,200
part of the infrastructure build out right now

474
00:15:10,200 --> 00:15:12,880
that's so intense is like, we literally don't

475
00:15:12,880 --> 00:15:13,300
have

476
00:15:13,860 --> 00:15:17,620
power generation. Boom Supersonic, instead of making supersonic

477
00:15:17,620 --> 00:15:18,400
jets right now,

478
00:15:18,600 --> 00:15:21,220
is on this good quest to create enough

479
00:15:21,220 --> 00:15:24,260
power for a bunch of these AI data

480
00:15:24,260 --> 00:15:25,060
centers that are being

481
00:15:25,060 --> 00:15:28,140
built right now. They use jet engines. And

482
00:15:28,140 --> 00:15:30,240
even those like are so bad, you know,

483
00:15:30,280 --> 00:15:30,880
the supply chain

484
00:15:30,880 --> 00:15:32,880
for jet engines to generate power for data

485
00:15:32,880 --> 00:15:35,100
centers is so backed up that, you know,

486
00:15:35,220 --> 00:15:35,840
you would have had

487
00:15:35,840 --> 00:15:38,040
to have ordered these things, you know, two

488
00:15:38,040 --> 00:15:40,420
or three years ago just to even have

489
00:15:40,420 --> 00:15:41,660
it in two or three

490
00:15:41,660 --> 00:15:45,180
years from now. You know, these constraints, uh,

491
00:15:45,420 --> 00:15:48,640
end up like influencing like fairly directly

492
00:15:48,640 --> 00:15:51,800
what the giant tech companies need to do

493
00:15:51,800 --> 00:15:53,520
to win the game three or five years

494
00:15:53,520 --> 00:15:53,920
out.

495
00:15:54,700 --> 00:15:57,940
Like suddenly there's not enough land, you know,

496
00:15:58,020 --> 00:16:00,380
in America, we can't build. The regulations are

497
00:16:00,380 --> 00:16:00,520
too

498
00:16:00,520 --> 00:16:02,860
high. In California, we have CEQA, which is

499
00:16:02,860 --> 00:16:06,140
totally abused by the environmental lobby to stop

500
00:16:06,140 --> 00:16:06,460
all

501
00:16:06,460 --> 00:16:09,880
innovation and building housing. By the way, we

502
00:16:09,880 --> 00:16:13,640
just don't have enough terrestrially like to just

503
00:16:13,640 --> 00:16:15,820
do the things that society sort of needs

504
00:16:15,820 --> 00:16:18,160
right now. So, you know, the escape valve

505
00:16:18,160 --> 00:16:18,520
is like,

506
00:16:18,660 --> 00:16:20,840
actually, let's just do it in space. Yeah.

507
00:16:20,900 --> 00:16:22,000
Come to think about, we, we kind of

508
00:16:22,000 --> 00:16:22,120
have

509
00:16:22,120 --> 00:16:24,220
the trifecta of YC companies that are solving

510
00:16:24,220 --> 00:16:26,320
the data center buildup problem. Well, you need

511
00:16:26,320 --> 00:16:26,580
fusion

512
00:16:26,580 --> 00:16:29,640
energy. Yeah. Yeah. Well, we have the company

513
00:16:29,640 --> 00:16:32,620
that's solving the no land problem by building

514
00:16:32,620 --> 00:16:34,920
the data centers in space. We have boom

515
00:16:34,920 --> 00:16:36,940
and helium, which are solving that we don't

516
00:16:36,940 --> 00:16:37,560
have an energy

517
00:16:37,560 --> 00:16:40,160
problem. I just found today, uh, a space

518
00:16:40,160 --> 00:16:43,240
fusion company that just graduated called Zephyr fusion.

519
00:16:43,460 --> 00:16:44,900
That's a cool one. And they actually had

520
00:16:44,900 --> 00:16:46,600
a great seed round out of demo day.

521
00:16:46,780 --> 00:16:47,640
They're in their forties,

522
00:16:47,680 --> 00:16:51,140
they're national lab engineers who their entire careers,

523
00:16:51,140 --> 00:16:53,620
they were building, you know, tokamaks and

524
00:16:53,620 --> 00:16:56,580
fusion energy. And they came into the lab

525
00:16:56,580 --> 00:16:58,960
one day, they looked at the physics, they,

526
00:16:59,100 --> 00:17:00,120
you know, looked at the

527
00:17:00,120 --> 00:17:03,020
math and the models and they said, you

528
00:17:03,020 --> 00:17:05,040
know what, if we did this in space,

529
00:17:05,340 --> 00:17:06,700
it would actually pencil.

530
00:17:07,040 --> 00:17:08,660
And so that's, they're on like this sort

531
00:17:08,660 --> 00:17:11,500
of grand next five, 10 year quest to

532
00:17:11,500 --> 00:17:13,220
actually manifest it,

533
00:17:13,300 --> 00:17:15,980
to actually create it in space, uh, because

534
00:17:15,980 --> 00:17:18,980
the equations say that it is possible. And,

535
00:17:18,980 --> 00:17:19,100
uh,

536
00:17:19,180 --> 00:17:20,560
if they do it, it's actually the only

537
00:17:20,560 --> 00:17:23,280
path to gigawatts of energy, uh, up there

538
00:17:23,280 --> 00:17:24,560
in space. So,

539
00:17:24,700 --> 00:17:26,300
you know, it might be, you know, an

540
00:17:26,300 --> 00:17:28,500
even more perfect trifecta, uh, shortly.

541
00:17:28,500 --> 00:17:30,280
Yeah. Something else I feel like happened over

542
00:17:30,280 --> 00:17:32,300
the course of this year is the, um,

543
00:17:32,660 --> 00:17:35,740
interest in starting model companies. Like, I guess

544
00:17:35,740 --> 00:17:37,200
that maybe at both ends, there's like

545
00:17:37,200 --> 00:17:39,180
the people who can raise the capital to

546
00:17:39,180 --> 00:17:41,040
go and actually try and build a head

547
00:17:41,040 --> 00:17:41,800
on competitor to

548
00:17:41,800 --> 00:17:43,240
open AI, which there are very, very few,

549
00:17:43,320 --> 00:17:46,040
like maybe have ILIO with SSI, but then

550
00:17:46,040 --> 00:17:47,360
more so within YC,

551
00:17:47,680 --> 00:17:49,800
people trying to build like smaller models. Um,

552
00:17:50,060 --> 00:17:51,360
I've certainly had more of those in the

553
00:17:51,360 --> 00:17:51,740
last few

554
00:17:51,740 --> 00:17:53,720
batches than before, like whether it's sort of

555
00:17:53,720 --> 00:17:55,800
like a models run on edge devices or

556
00:17:55,800 --> 00:17:56,460
maybe like a voice

557
00:17:56,460 --> 00:17:59,520
model specific to a particular language. And I'm

558
00:17:59,520 --> 00:18:01,640
curious to see if that trend continues going

559
00:18:01,640 --> 00:18:01,860
back

560
00:18:01,860 --> 00:18:04,140
to this early era of YC. Actually, we

561
00:18:04,140 --> 00:18:06,040
sort of saw the explosion of just startups

562
00:18:06,040 --> 00:18:06,700
being created

563
00:18:06,700 --> 00:18:10,280
and maybe especially SaaS startups. Partly what, what,

564
00:18:10,400 --> 00:18:13,080
um, fed that was just knowledge about startups

565
00:18:13,080 --> 00:18:15,500
became more dispersed. There wasn't like a canon

566
00:18:15,500 --> 00:18:17,520
of library information on the internet about like

567
00:18:17,520 --> 00:18:18,900
how to start a startup, how to build

568
00:18:18,900 --> 00:18:21,320
software. And then over like a decade that

569
00:18:21,320 --> 00:18:21,920
just became more

570
00:18:21,920 --> 00:18:25,640
commonplace and that just exploded like society's knowledge

571
00:18:25,640 --> 00:18:27,100
of startups and how to build things.

572
00:18:27,440 --> 00:18:29,540
And it's maybe feels like maybe we're going

573
00:18:29,540 --> 00:18:31,300
through that moment in sort of the AI

574
00:18:31,300 --> 00:18:32,580
research

575
00:18:32,580 --> 00:18:35,680
meets like actually building things with, with training

576
00:18:35,680 --> 00:18:37,160
models. I think we are absolutely going

577
00:18:37,160 --> 00:18:39,080
through that right now. Yes. Like where it's

578
00:18:39,080 --> 00:18:41,340
going from being a very rare skill set

579
00:18:41,340 --> 00:18:42,220
to a more common

580
00:18:42,220 --> 00:18:44,320
one. Cause like open AI a decade ago,

581
00:18:44,500 --> 00:18:45,760
it was like a rare, like you needed,

582
00:18:45,900 --> 00:18:46,700
you need like a,

583
00:18:46,700 --> 00:18:49,400
a unique combination of skills, right? You need

584
00:18:49,400 --> 00:18:50,800
like your researcher brain,

585
00:18:50,800 --> 00:18:53,340
your sort of like engineering brain, maybe like

586
00:18:53,340 --> 00:18:55,560
your sort of finance business brain.

587
00:18:55,880 --> 00:18:57,960
Wait, wait, wait. So did you just describe

588
00:18:57,960 --> 00:19:00,580
Ilya, Greg, and Sam?

589
00:19:00,640 --> 00:19:01,060
You got it.

590
00:19:03,100 --> 00:19:04,960
That was like a rare team, right? There

591
00:19:04,960 --> 00:19:07,080
was, wasn't that configuration of team around very

592
00:19:07,080 --> 00:19:07,940
much. And now

593
00:19:07,940 --> 00:19:11,820
a decade later, there's like a plethora of

594
00:19:11,820 --> 00:19:14,260
people who have like the research background,

595
00:19:14,480 --> 00:19:17,940
the engineering background, um, the startup capital raising,

596
00:19:17,940 --> 00:19:19,660
um, background, or at least can be taught

597
00:19:19,660 --> 00:19:20,600
how to do all of that kind of

598
00:19:20,600 --> 00:19:22,700
stuff. And I'm curious if that would just

599
00:19:22,700 --> 00:19:23,820
mean we'll just see more

600
00:19:24,960 --> 00:19:28,180
applied AI company starting, and maybe there'll be

601
00:19:28,180 --> 00:19:30,440
like even more models to choose from for

602
00:19:30,440 --> 00:19:30,540
all

603
00:19:30,540 --> 00:19:32,560
the various specific tasks. I think so. I

604
00:19:32,560 --> 00:19:34,880
think the other thing that's even contributing and

605
00:19:34,880 --> 00:19:35,480
making

606
00:19:35,480 --> 00:19:39,260
this an even bigger snowball is because of

607
00:19:39,260 --> 00:19:41,620
RL. I think there's all these new open

608
00:19:41,620 --> 00:19:43,120
source models that

609
00:19:43,120 --> 00:19:46,060
people are doing the fine tune on top

610
00:19:46,060 --> 00:19:48,980
of it with a particular RL environment and

611
00:19:48,980 --> 00:19:51,060
task. So it is very

612
00:19:51,060 --> 00:19:55,280
possible that you can create the best domain

613
00:19:55,280 --> 00:19:59,220
specific, let's say, healthcare model trained on a

614
00:19:59,220 --> 00:20:02,020
generic open source model by just doing fine

615
00:20:02,020 --> 00:20:04,960
tuning on it and doing RL. It beats

616
00:20:04,960 --> 00:20:06,260
the regular big model.

617
00:20:06,260 --> 00:20:08,380
Actually, I've heard and seen a number of

618
00:20:08,380 --> 00:20:11,900
startups where their domain specific model beats, uh,

619
00:20:12,300 --> 00:20:12,500
open

620
00:20:12,500 --> 00:20:14,540
AI, let's say on healthcare. There's this particular

621
00:20:14,540 --> 00:20:16,740
YC startup that told me that they collected

622
00:20:16,740 --> 00:20:17,180
the best

623
00:20:17,180 --> 00:20:20,520
data set for, for healthcare and they ended

624
00:20:20,520 --> 00:20:22,620
up performing better than open AI and a

625
00:20:22,620 --> 00:20:22,840
lot of the

626
00:20:22,840 --> 00:20:25,580
benchmarks for, for healthcare with only 8 billion

627
00:20:25,580 --> 00:20:27,620
parameters. I guess what's funny is, uh, you

628
00:20:27,620 --> 00:20:27,920
do need

629
00:20:27,920 --> 00:20:30,360
to have a post-training infrastructure. You know,

630
00:20:30,400 --> 00:20:32,940
we've also had YC companies where, uh, they

631
00:20:32,940 --> 00:20:33,560
had something that

632
00:20:33,560 --> 00:20:36,060
beat open AI, uh, you know, GPT 3

633
00:20:36,060 --> 00:20:38,500
.5 and they were doing fine tuning with

634
00:20:38,500 --> 00:20:42,360
RL, but then, uh, yeah, GPT 4.5

635
00:20:42,360 --> 00:20:42,600
and

636
00:20:42,600 --> 00:20:45,920
then 5.1 came out and, uh, you

637
00:20:45,920 --> 00:20:48,000
know, basically blew their fine tuning out of

638
00:20:48,000 --> 00:20:49,240
the water. You have to keep

639
00:20:49,240 --> 00:20:51,160
going. Yeah. Yeah. You gotta keep going. Yeah.

640
00:20:51,240 --> 00:20:53,260
I mean, you actually have to continue to,

641
00:20:53,260 --> 00:20:54,560
uh, get to the

642
00:20:54,560 --> 00:20:57,260
edge. Anything else, uh, that really sort of

643
00:20:57,260 --> 00:21:00,060
stood out from this past year that jumps

644
00:21:00,060 --> 00:21:00,500
out to you?

645
00:21:00,500 --> 00:21:02,640
Uh, it's funny. We started the year with

646
00:21:02,640 --> 00:21:04,640
one of our episodes that got a lot

647
00:21:04,640 --> 00:21:05,460
of views around

648
00:21:05,460 --> 00:21:07,760
vibe coding. I think we were talking about

649
00:21:07,760 --> 00:21:10,060
it more as observing a behavior that was

650
00:21:10,060 --> 00:21:10,400
happening

651
00:21:10,400 --> 00:21:13,560
from our founders. And I was surprised to

652
00:21:13,560 --> 00:21:16,220
see that this became like a giant category.

653
00:21:16,400 --> 00:21:16,560
There's

654
00:21:16,560 --> 00:21:19,460
lots of companies that are winning. I mean,

655
00:21:19,540 --> 00:21:22,600
we have Replit, there's Emergence, there's a bunch

656
00:21:22,600 --> 00:21:22,900
of them.

657
00:21:22,900 --> 00:21:25,540
Varun Mohan had gone over to Google. He

658
00:21:25,540 --> 00:21:28,340
released anti-gravity. And, uh, did you guys

659
00:21:28,340 --> 00:21:28,960
see the video?

660
00:21:29,200 --> 00:21:30,640
I actually, I'm sort of curious whether they

661
00:21:30,640 --> 00:21:33,300
actually used nano banana or any of these

662
00:21:33,300 --> 00:21:33,860
video

663
00:21:33,860 --> 00:21:36,160
gen things. Cause it's like a little too

664
00:21:36,160 --> 00:21:38,180
perfect, but Google has the budget to do

665
00:21:38,180 --> 00:21:38,640
the high

666
00:21:38,640 --> 00:21:41,380
production value video, but it's, you know, Varun

667
00:21:41,380 --> 00:21:43,460
at the keyboard and then, you know,

668
00:21:43,940 --> 00:21:46,120
Sergey is like right behind him. So I

669
00:21:46,120 --> 00:21:48,120
was like, it was very cinematic. Anyway, I

670
00:21:48,120 --> 00:21:49,280
think Sundar was, uh,

671
00:21:49,280 --> 00:21:51,800
you know, also not only talking about, um,

672
00:21:52,600 --> 00:21:55,680
space data centers, uh, he was also talking

673
00:21:55,680 --> 00:21:56,180
about vibe

674
00:21:56,180 --> 00:21:58,280
coding and I knew that I was a

675
00:21:58,280 --> 00:22:00,980
little bit trolling back, but knowing what we

676
00:22:00,980 --> 00:22:02,600
know, I mean, yes, vibe

677
00:22:02,600 --> 00:22:06,620
coding is not, you know, completely usable and

678
00:22:06,620 --> 00:22:09,700
trustable for, uh, you know, a hundred percent

679
00:22:09,700 --> 00:22:10,380
of your

680
00:22:10,380 --> 00:22:13,520
coding period. Like this, you know, it is

681
00:22:13,520 --> 00:22:16,560
not true that you can like ship a

682
00:22:16,560 --> 00:22:17,780
hundred, a hundred percent

683
00:22:17,780 --> 00:22:21,600
solid production code today as of 2020 and

684
00:22:21,600 --> 00:22:22,540
the end of 2025.

685
00:22:23,120 --> 00:22:24,580
Yeah. I was thinking about things that surprised

686
00:22:24,580 --> 00:22:26,960
me in 2025. And I think perhaps the

687
00:22:26,960 --> 00:22:27,440
thing that most

688
00:22:27,440 --> 00:22:29,680
surprised me is the extent to which I

689
00:22:29,680 --> 00:22:32,480
feel like the AI economy stabilized. Like I

690
00:22:32,480 --> 00:22:33,120
feel like when we did

691
00:22:33,120 --> 00:22:35,300
this episode at the end of 2024, it

692
00:22:35,300 --> 00:22:36,600
felt like we were still in the middle

693
00:22:36,600 --> 00:22:37,520
of a period of incredibly

694
00:22:37,520 --> 00:22:39,060
rapid change where the ground was shifting under

695
00:22:39,060 --> 00:22:41,300
our feet and like nobody knew when the

696
00:22:41,300 --> 00:22:41,660
other shoe

697
00:22:41,660 --> 00:22:43,640
might drop and like what exactly was going

698
00:22:43,640 --> 00:22:45,140
to happen with startups and AI and the

699
00:22:45,140 --> 00:22:46,140
economy. Now I feel like

700
00:22:46,140 --> 00:22:48,240
we've kind of settled into like a fairly

701
00:22:48,240 --> 00:22:50,380
stable AI economy where we have like the

702
00:22:50,380 --> 00:22:50,760
model layer

703
00:22:50,760 --> 00:22:52,980
companies and the application layer companies and seem,

704
00:22:53,140 --> 00:22:54,420
and the infrastructure layer companies,

705
00:22:54,580 --> 00:22:55,600
it seems like everyone is going to make

706
00:22:55,600 --> 00:22:57,540
a lot of money and there's kind of

707
00:22:57,540 --> 00:22:58,860
like a relative playbook

708
00:22:58,860 --> 00:23:00,540
for how to build an AI native company

709
00:23:00,540 --> 00:23:02,100
on top of the models. I feel like

710
00:23:02,100 --> 00:23:03,440
things really kind of matured

711
00:23:03,440 --> 00:23:05,080
in that way. Which feels it's all downstream

712
00:23:05,080 --> 00:23:07,560
of like the models themselves have incrementally

713
00:23:07,560 --> 00:23:09,740
improved this year, but there haven't been like

714
00:23:09,740 --> 00:23:12,040
major steps forward that have shaken everything up,

715
00:23:12,040 --> 00:23:13,820
which is, has a knock on effect. Many

716
00:23:13,820 --> 00:23:15,420
episodes ago, we talked about how it was

717
00:23:15,420 --> 00:23:16,600
felt easier than

718
00:23:16,600 --> 00:23:18,280
ever to pivot and find a startup idea.

719
00:23:18,480 --> 00:23:20,300
Because if you could just survive, if you

720
00:23:20,300 --> 00:23:20,980
just wait a few

721
00:23:20,980 --> 00:23:22,520
months, there was likely going to be some

722
00:23:22,520 --> 00:23:25,180
like big announcement that would completely make a

723
00:23:25,180 --> 00:23:25,440
new set

724
00:23:25,440 --> 00:23:28,020
of ideas possible and create more opportunities to

725
00:23:28,020 --> 00:23:30,540
build things. It certainly feels like that has

726
00:23:30,540 --> 00:23:32,900
slowed down. And so like finding ideas is

727
00:23:32,900 --> 00:23:35,380
sort of returning to sort of normal levels

728
00:23:35,380 --> 00:23:35,920
of difficulty,

729
00:23:35,920 --> 00:23:37,680
in my experience, not for hours. I agree.

730
00:23:37,880 --> 00:23:40,180
I'll tell you what's not a surprise. Do

731
00:23:40,180 --> 00:23:40,520
you remember

732
00:23:40,520 --> 00:23:43,580
that report, AI 2027, where it was just

733
00:23:43,580 --> 00:23:45,460
sort of like this doomer piece that said

734
00:23:45,460 --> 00:23:46,220
like, oh, well,

735
00:23:46,380 --> 00:23:47,960
society is going to start falling apart in

736
00:23:47,960 --> 00:23:50,300
2027. But, you know, at some point, they

737
00:23:50,300 --> 00:23:51,500
quietly revised it

738
00:23:51,500 --> 00:23:53,820
to say that it wasn't 2027, but they

739
00:23:53,820 --> 00:23:55,760
kept the title. Maybe it's not a surprise.

740
00:23:56,020 --> 00:23:57,000
Like I was always a

741
00:23:57,000 --> 00:23:59,740
little bit of a skeptic of like this

742
00:23:59,740 --> 00:24:03,360
fast takeoff argument, because even with the scaling

743
00:24:03,360 --> 00:24:04,340
law, it is

744
00:24:04,340 --> 00:24:09,000
log linear. So it is slower, it requires

745
00:24:09,000 --> 00:24:12,140
like 10x more compute, and it's still sort

746
00:24:12,140 --> 00:24:13,040
of, you know,

747
00:24:13,180 --> 00:24:16,040
topping out, right. And that's one form of

748
00:24:16,040 --> 00:24:18,520
good news. Another form of it's weird to

749
00:24:18,520 --> 00:24:19,100
call this good

750
00:24:19,100 --> 00:24:23,280
news, but human beings don't like change. In

751
00:24:23,280 --> 00:24:25,220
our previous episode, where we sort of blew

752
00:24:25,220 --> 00:24:25,740
up that

753
00:24:26,340 --> 00:24:28,620
MIT report that said that, you know, 98

754
00:24:28,620 --> 00:24:32,520
% or 90% of enterprise AI projects

755
00:24:32,520 --> 00:24:33,880
fail. Well, it turns out that

756
00:24:33,880 --> 00:24:37,400
90% of enterprises don't know how to

757
00:24:37,400 --> 00:24:40,540
do, you know, IT, let alone AI. It's

758
00:24:40,540 --> 00:24:41,280
weird to say that

759
00:24:41,280 --> 00:24:42,780
that's a good thing. But in the context

760
00:24:42,780 --> 00:24:44,780
of fast takeoff, like, that is a real

761
00:24:44,780 --> 00:24:46,940
break on the ability

762
00:24:46,940 --> 00:24:50,480
of this new, really insane technology from actually

763
00:24:50,480 --> 00:24:53,300
permeating society. I love to accelerate, but like,

764
00:24:53,400 --> 00:24:55,580
it's weird to say, like, oh, well, actually,

765
00:24:55,660 --> 00:24:57,080
in this case, maybe that's a good thing,

766
00:24:57,220 --> 00:24:57,640
right? Like,

767
00:24:57,640 --> 00:25:01,680
it is a shockingly powerful technology. But, you

768
00:25:01,680 --> 00:25:03,760
know, between being log linear scaling,

769
00:25:04,020 --> 00:25:07,620
and human beings really don't like change, like

770
00:25:07,620 --> 00:25:10,980
organizationally speaking, society will absorb

771
00:25:10,980 --> 00:25:14,160
this technology, everyone will have enough time to

772
00:25:14,160 --> 00:25:16,880
sort of process it, like culture will catch

773
00:25:16,880 --> 00:25:17,240
up,

774
00:25:17,420 --> 00:25:19,620
governments will be able to respond to it,

775
00:25:19,860 --> 00:25:23,120
not in like a frantic SB 1047 sort

776
00:25:23,120 --> 00:25:24,320
of like, you know,

777
00:25:24,320 --> 00:25:26,220
let's stop all the compute past 10 to

778
00:25:26,220 --> 00:25:28,700
the 26, right? Like just these knee jerk

779
00:25:28,700 --> 00:25:29,400
responses

780
00:25:29,980 --> 00:25:33,860
to technology. We're excited about the Arc AGI

781
00:25:33,860 --> 00:25:36,340
prize is, you know, going to come in

782
00:25:36,340 --> 00:25:36,760
and do the

783
00:25:36,760 --> 00:25:39,580
winter 26 batch as a nonprofit. The funny

784
00:25:39,580 --> 00:25:41,620
thing about that is like, yeah, maybe there's

785
00:25:42,160 --> 00:25:45,060
a team right now that is climbing the

786
00:25:45,060 --> 00:25:47,680
leaderboard of Arc AGI, and they're going to

787
00:25:47,680 --> 00:25:48,400
accelerate this

788
00:25:48,400 --> 00:25:48,880
thing again.

789
00:25:48,880 --> 00:25:50,840
Something that surprised me to relate to that

790
00:25:50,840 --> 00:25:52,300
with the startups is I remember around this

791
00:25:52,300 --> 00:25:52,580
time last

792
00:25:52,580 --> 00:25:54,580
year, we were talking about how companies are

793
00:25:54,580 --> 00:25:56,240
getting to a million dollars ARR and raising

794
00:25:56,240 --> 00:25:56,560
series

795
00:25:56,560 --> 00:25:58,720
A's without hiring, like some cases, not hiring

796
00:25:58,720 --> 00:26:00,980
anyone, just the founders, maybe hiring one person,

797
00:26:00,980 --> 00:26:04,040
which just felt very unusual. I feel like

798
00:26:04,040 --> 00:26:08,560
a year on, that hasn't translated into, okay,

799
00:26:08,640 --> 00:26:10,100
and then they went and hit like 10

800
00:26:10,100 --> 00:26:13,320
million ARR, or they scaled without adding any

801
00:26:13,320 --> 00:26:14,120
more people to.

802
00:26:14,120 --> 00:26:16,980
No, they turned around and started hiring like

803
00:26:16,980 --> 00:26:17,500
actual teams.

804
00:26:17,720 --> 00:26:20,540
Yeah. Like post series A, it actually largely

805
00:26:20,540 --> 00:26:23,680
feels like the playbook is the same and

806
00:26:23,680 --> 00:26:24,640
the companies

807
00:26:24,640 --> 00:26:26,760
might be smaller for the same amount of

808
00:26:26,760 --> 00:26:29,560
revenue, but it feels it's entirely because they

809
00:26:29,560 --> 00:26:29,880
hit the

810
00:26:29,880 --> 00:26:32,060
revenue so fast and there's still just bottlenecked

811
00:26:32,060 --> 00:26:33,480
on how long it takes to hire people

812
00:26:33,480 --> 00:26:34,400
versus they

813
00:26:34,400 --> 00:26:35,600
have demand for less people.

814
00:26:35,800 --> 00:26:37,500
I still think there is like, you know,

815
00:26:37,580 --> 00:26:39,920
some effect, but it is not like open

816
00:26:39,920 --> 00:26:41,540
and shut. It is not like you

817
00:26:41,540 --> 00:26:43,840
don't have to hire executives anymore. I think

818
00:26:43,840 --> 00:26:45,720
they're like, there might be a case of

819
00:26:45,720 --> 00:26:46,780
two foie

820
00:26:46,780 --> 00:26:49,720
gras startups, like one being Harvey and the

821
00:26:49,720 --> 00:26:52,020
other one being open evidence, right? Harvey,

822
00:26:52,300 --> 00:26:55,200
the founders are incredible. They were very early.

823
00:26:55,200 --> 00:26:57,480
And then there's this sort of idea of

824
00:26:57,480 --> 00:26:57,740
like,

825
00:26:58,000 --> 00:26:59,920
for VCs, you could just go down Sand

826
00:26:59,920 --> 00:27:02,100
Hill Road and like the fixes in, like

827
00:27:02,100 --> 00:27:03,260
you just sort of block out

828
00:27:03,260 --> 00:27:04,800
all of them. And then all the people,

829
00:27:05,000 --> 00:27:07,420
you know, maybe 30 people who could write

830
00:27:07,420 --> 00:27:08,860
checks of like 10 to

831
00:27:08,860 --> 00:27:10,480
a hundred million dollars. And if you just

832
00:27:10,480 --> 00:27:12,260
sort of get all of their money, like

833
00:27:12,260 --> 00:27:12,980
there's sort of no

834
00:27:12,980 --> 00:27:15,260
one who can actually come in and do

835
00:27:15,260 --> 00:27:17,440
the next series A, and then basically you're

836
00:27:17,440 --> 00:27:17,980
safe. Like

837
00:27:17,980 --> 00:27:20,060
you have capital as a bludgeon is capital

838
00:27:20,060 --> 00:27:21,980
as a moat in that case, right? So

839
00:27:21,980 --> 00:27:22,960
yeah, Harvey is interesting

840
00:27:22,960 --> 00:27:25,380
because, you know, uh, Lagora is coming fast

841
00:27:25,380 --> 00:27:27,900
for them. And obviously we have some skin

842
00:27:27,900 --> 00:27:28,280
in the game

843
00:27:28,280 --> 00:27:30,000
on Lagora, but we think that they have,

844
00:27:30,100 --> 00:27:32,120
uh, as good a shot at any, I

845
00:27:32,120 --> 00:27:33,160
guess that's one trend that we saw

846
00:27:33,160 --> 00:27:34,720
in 2025 is that there was like a

847
00:27:34,720 --> 00:27:37,000
first wave of like AI hate of companies

848
00:27:37,000 --> 00:27:37,840
like Harvey.

849
00:27:38,160 --> 00:27:39,480
Who might've wasted a lot of money on

850
00:27:39,480 --> 00:27:40,420
fine tuning actually.

851
00:27:40,980 --> 00:27:42,960
Totally. That like broke out really big in

852
00:27:42,960 --> 00:27:45,240
2023 and kind of did a victory lap

853
00:27:45,240 --> 00:27:45,680
that, you know,

854
00:27:45,780 --> 00:27:47,640
oh, we've won the space. And now we're

855
00:27:47,640 --> 00:27:49,660
seeing a second wave of companies like Lagora

856
00:27:49,660 --> 00:27:50,300
and Giga.

857
00:27:50,400 --> 00:27:51,780
And it turns out that like, oh, actually

858
00:27:51,780 --> 00:27:53,940
like it isn't so simple.

859
00:27:54,180 --> 00:27:56,860
Yeah. The wood beneficiary of, um, you know,

860
00:27:57,100 --> 00:28:00,720
burning some non-trivial double-digit percentage of

861
00:28:00,720 --> 00:28:03,640
your capital stack on fine tuning that buys,

862
00:28:03,640 --> 00:28:06,220
you know, advantage is like basically the investors

863
00:28:06,220 --> 00:28:07,840
are the only winners there because they just

864
00:28:07,840 --> 00:28:09,140
own more of your company, you know?

865
00:28:09,600 --> 00:28:10,740
Yeah. So at least it relates to like

866
00:28:10,740 --> 00:28:12,760
the, the hiring and team size. I feel

867
00:28:12,760 --> 00:28:13,160
like

868
00:28:13,160 --> 00:28:15,960
of the two camps, one being the AI

869
00:28:15,960 --> 00:28:17,100
is going to make everything more efficient. You

870
00:28:17,100 --> 00:28:17,500
will need less

871
00:28:17,500 --> 00:28:19,840
people and the other AI is going to

872
00:28:19,840 --> 00:28:21,960
reduce the cost of like producing the time

873
00:28:21,960 --> 00:28:22,580
to produce things.

874
00:28:22,740 --> 00:28:25,040
And so then the expectations from your users

875
00:28:25,040 --> 00:28:26,560
and customers will just go up and you'll

876
00:28:26,560 --> 00:28:26,760
need to

877
00:28:26,760 --> 00:28:28,680
keep hiring more people to satisfy like the

878
00:28:28,680 --> 00:28:29,560
growing expectations.

879
00:28:29,560 --> 00:28:31,640
I feel like this year has been more

880
00:28:31,640 --> 00:28:33,740
in that second camp. And I think that

881
00:28:33,740 --> 00:28:34,880
is what's driving the fact

882
00:28:34,880 --> 00:28:36,380
that the companies are still just hiring as

883
00:28:36,380 --> 00:28:38,840
many people as they were pre AI. It's

884
00:28:38,840 --> 00:28:39,480
just like the bar

885
00:28:39,480 --> 00:28:41,320
for what the software, what their customers expect.

886
00:28:41,760 --> 00:28:42,900
And they're all in the, you know, like

887
00:28:42,900 --> 00:28:43,660
Lagora's

888
00:28:43,660 --> 00:28:46,620
racing with Harvey, Giga's racing with Sierra. Like

889
00:28:46,620 --> 00:28:49,140
they're all still competing for the same set

890
00:28:49,140 --> 00:28:49,240
of

891
00:28:49,240 --> 00:28:51,720
customers and they still ultimately are bottlenecked on

892
00:28:51,720 --> 00:28:54,940
like people and like, I don't think anyone's

893
00:28:54,940 --> 00:28:56,700
bottlenecked on ideas, but they're bottlenecked on like

894
00:28:56,700 --> 00:28:58,140
people who can execute really well.

895
00:28:58,140 --> 00:28:59,560
I don't know. I think that's like, so

896
00:28:59,560 --> 00:29:01,260
it's exciting. It feels like an exciting phase.

897
00:29:01,420 --> 00:29:04,240
I agree with you that like the era

898
00:29:04,240 --> 00:29:07,340
of the one person running a trillion dollar

899
00:29:07,340 --> 00:29:08,340
company is not

900
00:29:08,340 --> 00:29:09,200
here. Not yet.

901
00:29:09,460 --> 00:29:10,980
Yeah. But I think it's going to trend

902
00:29:10,980 --> 00:29:13,320
that way eventually. That'll be a wild time.

903
00:29:13,640 --> 00:29:14,040
Maybe that's

904
00:29:14,040 --> 00:29:15,600
a prediction for... For next year?

905
00:29:15,680 --> 00:29:16,600
...2026. Yeah.

906
00:29:16,720 --> 00:29:17,300
Do you think it's coming?

907
00:29:17,300 --> 00:29:18,660
I mean, I don't think it'll happen in

908
00:29:18,660 --> 00:29:20,900
2026 either, honestly. I mean, I think you

909
00:29:20,900 --> 00:29:21,400
will have

910
00:29:21,400 --> 00:29:25,260
many stories of companies run by, you know,

911
00:29:25,420 --> 00:29:27,360
under a hundred people that are making hundreds

912
00:29:27,360 --> 00:29:27,560
of

913
00:29:27,560 --> 00:29:28,600
millions of dollars. Yeah.

914
00:29:28,600 --> 00:29:31,060
So, I mean, Gamma was interesting to see.

915
00:29:31,200 --> 00:29:33,200
Like one of the biggest things that they

916
00:29:33,200 --> 00:29:34,060
said in their launch

917
00:29:34,060 --> 00:29:35,680
that I think is a very good trend

918
00:29:35,680 --> 00:29:37,300
is they said they got to a hundred

919
00:29:37,300 --> 00:29:40,120
million dollars in ARR with

920
00:29:40,120 --> 00:29:42,660
only 50 employees. So, which is very different.

921
00:29:42,940 --> 00:29:44,800
It's, you know, such an inversion, right? Like

922
00:29:44,800 --> 00:29:47,400
normally you have the big banner and the

923
00:29:47,400 --> 00:29:49,840
like little X thing, you know, image. And

924
00:29:49,840 --> 00:29:50,660
it's like, oh yeah,

925
00:29:50,800 --> 00:29:52,700
like we raised all this money and look

926
00:29:52,700 --> 00:29:54,240
at all the people who work for us.

927
00:29:54,240 --> 00:29:56,500
It's a good trend to have the reverse

928
00:29:56,500 --> 00:29:58,220
flex, which is like, look at all this

929
00:29:58,220 --> 00:29:59,360
revenue and look how few

930
00:29:59,360 --> 00:30:01,340
people work for us. Well, that's all we

931
00:30:01,340 --> 00:30:03,520
have time for this time. We just wanted

932
00:30:03,520 --> 00:30:04,060
to wish you

933
00:30:04,060 --> 00:30:06,220
a really happy holidays and happy new year

934
00:30:06,220 --> 00:30:08,180
from all of us to you and yours.

935
00:30:08,460 --> 00:30:09,260
See you next time.

